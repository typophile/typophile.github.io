<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>United States navy ditches ALL CAPS message format | Typophile Archive</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.91.2" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <link href="/style.css" rel="stylesheet">

    <meta property="og:title" content="United States navy ditches ALL CAPS message format" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://typophile.github.io/node/103822/" /><meta property="article:section" content="node" />
<meta property="article:published_time" content="2013-06-13T12:57:26+00:00" />
<meta property="article:modified_time" content="2013-06-13T12:57:26+00:00" />

<meta itemprop="name" content="United States navy ditches ALL CAPS message format">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2013-06-13T12:57:26+00:00" />
<meta itemprop="dateModified" content="2013-06-13T12:57:26+00:00" />
<meta itemprop="wordCount" content="0">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="United States navy ditches ALL CAPS message format"/>
<meta name="twitter:description" content=""/>

  </head>

  <body class="ma0">

      <header>
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container">
      <a class="navbar-brand" href="/">Typophile Archive</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
    </div>
  </nav>
</header>
 
    <main role="main">
      
<article class="container">
  <header class="post-header py-3">
    <div class="float-right">
 
    <a class="text-decoration-none share facebook" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2ftypophile.github.io%2fnode%2f103822%2f"><i class="bi bi-facebook"></i></a> 
    
    <a class="text-decoration-none share twitter" href="https://twitter.com/intent/tweet?text=title&amp;url=http%3a%2f%2ftypophile.github.io%2fnode%2f103822%2f&amp;"><i class="bi bi-twitter"></i></a> 
    
    <a class="text-decoration-none share linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2ftypophile.github.io%2fnode%2f103822%2f&amp;source=http%3a%2f%2ftypophile.github.io%2fnode%2f103822%2f/"><i class="bi bi-linkedin"></i></a>
 </a>
</div>

    <h1 class="post-title p-name" itemprop="name headline">United States navy ditches ALL CAPS message format</h1>
    <p class="text-muted">
              
      <time class="f6 mv4 dib tracked" datetime="2013-06-13T12:57:26Z">June 13, 2013</time>
      
        
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
        </span></span>
      

    </p>
  </header>
  <div class="post-content e-content" itemprop="articleBody"><p><a href="http://www.cnn.com/2013/06/13/us/navy-all-caps/index.html?hpt=hp_t2">United States navy ditches ALL CAPS message format</a>. (From CNN, 2013, 06, 13)</p>
<p>Score one for <a href="http://en.wikipedia.org/wiki/Bouma">Bouma</a>.</p>
</div>

  
   <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">So…internet “etiquette” trumps over a century of tradition. Marshall McLuhan was right…</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 13:13:15"
			itemprop="datePublished"
		>
			2013-06-13 13:13:15
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:oldnick">
            oldnick
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>&ldquo;we have torpedos ready stop surrender your vessel stop so please stop stop lol stop&rdquo;</p>
<p>But does morse code does not support lowercase. How are they going to do that thing with the flashing lights, then? Use Unicode?</p>
<p>&ldquo;All your (pizza emoji) are belong to us stop&rdquo;</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 13:30:00"
			itemprop="datePublished"
		>
			2013-06-13 13:30:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Theunis&#43;de&#43;Jong">
            Theunis de Jong
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>“<em>In it, the Navy said it is ditching its in-house Defense Message System in favor of e-mail. One with a very apt acronym: NICE (Navy Interface for Command Email)</em>.”</p>
<p>Beautiful irony. Capital letters for an acronym to describe a system which no longer uses ALL CAPS settings.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 14:09:00"
			itemprop="datePublished"
		>
			2013-06-13 14:09:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p><cite>&hellip; a century of tradition. </cite></p>
<p>First it was the rum rations. Then it was Morse Code&hellip; Now this.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 15:53:58"
			itemprop="datePublished"
		>
			2013-06-13 15:53:58
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:russellm">
            russellm
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">My uncle died in WWII when his ship was torpedoed and sank, so anything to improve naval communications is fine with me. But with that said, given that orders are usually read very carefully, I&rsquo;m not sure how big a difference this will make, but hopefully it&rsquo;ll help.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 17:02:01"
			itemprop="datePublished"
		>
			2013-06-13 17:02:01
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:JamesM">
            JamesM
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Chris, do you actually believe in this bouma nonsense? There is an awful lot of research in this area. The reasons that lowercase is more legible than uppercase have nothing to do with word shape and everything to do with making each letter more distinctive from others.</p>
<p>That&rsquo;s not to say there is no interplay between letters, or that we don&rsquo;t need to recognize words. But we don&rsquo;t do it by the shape of the word as a whole.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 20:38:34"
			itemprop="datePublished"
		>
			2013-06-13 20:38:34
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Thomas&#43;Phinney">
            Thomas Phinney
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Casual joke.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 21:49:04"
			itemprop="datePublished"
		>
			2013-06-13 21:49:04
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>A bouma isn&rsquo;t always a whole word; sometimes it&rsquo;s a single letter (and maybe even less often than I suspect).</p>
<p>The &ldquo;nonsense&rdquo; is thinking that the brain would simply ignore the information-rich clustering of letters (and often their silhouettes).</p>
<p>hhp</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-13 22:19:46"
			itemprop="datePublished"
		>
			2013-06-13 22:19:46
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:hrant">
            hrant
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I was amused that the article noted that back in the 1850s, teletypewriters with upper-case only character sets and three-bank keyboards were in use by the Navy.</p>
<p>In fact, 5-level code was still very much alive for TELEX messages even in the 1960s (and, in fact, I&rsquo;ve heard that it isn&rsquo;t quite dead even yet).</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-14 02:37:51"
			itemprop="datePublished"
		>
			2013-06-14 02:37:51
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:quadibloc">
            quadibloc
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John,</p>
<p>Ditto for Weather Service and AP wires in the 1980s, sent to television stations&hellip;</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-14 13:44:20"
			itemprop="datePublished"
		>
			2013-06-14 13:44:20
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:oldnick">
            oldnick
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">our work purchasing system works in all caps.  I write my requisitions like this: &ldquo;I would like to purchase 100 stop signs please. Thank you very much and have a good day.&rdquo; and they come out as, :&quot; HEY YOU! I NEED 100 STOP SIGNS! NOW!!!</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-14 14:50:24"
			itemprop="datePublished"
		>
			2013-06-14 14:50:24
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:russellm">
            russellm
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>[Thomas] <em>… do you actually believe in this bouma nonsense?</em></p>
<p>Thomas, for the record, the term Bouma Shape was introduced by Insup Taylor and M. Martin Taylor in their 1983 book <em>The Psychology of Reading</em>. From the information below, you will see that the construct includes a whole lot more than “the shape of the word as a whole.” The term is a result of Taylor and Taylor’s efforts to factor “interior features” into the “shape definition” of words.</p>
<p>Taylor &amp; Taylor list “the seven groups of mutually confusable lowercase letters found by Bouma (1971)”. They mean the S1, S2, S3, S4, A1, A2 &amp; D grouping of lowercase letters Bouma introduced in “Visual Recognition of Isolated Lower-case Letters”, which they number from 1 through 7, as follows [the descriptions are Bouma’s; the groupings are a result of confusion frequencies culled from recognition tests]:
1 = a s z x : [HB’s S1] inner parts and rectangular envelope
2 = e o c : [HB’s S2] almost round envelope
3 = r v w : [HB’s S4] oblique outer parts
4 = n m u : [HB’s S3] rectangular envelope with well-expressed vertical outer parts
5 = d h k b : [HB’s A1] ascending extensions protruding from a well-expressed body
6 = t i l f : [HB’s A2] slenderness
7 = g j p q y : [HB’s D] no description
1, 2, 3, &amp; 4 are categorized by Bouma and the Taylors as short (S); 5 &amp; 6 as tall (A); 7 as projecting (D)</p>
<p>Taylor and Taylor: “The “Bouma shape” of a word can be defined by listing the group numbers of its letters: “at” has the shape 16 (short-filled, tall-thin), and “dog” is 527 (tall-fat, short-round, projecting).</p>
<p>Taylor and Taylor found that by their Bouma Shapes — defined in this way — something like 87% of the words in the English language corpus they used unique. If I&rsquo;m not mistaken, Taylor and Taylor used this to elucidate the contribution made to reading by parafoveal pre-processing and give an account of skipping.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-16 14:00:33"
			itemprop="datePublished"
		>
			2013-06-16 14:00:33
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>The term &ldquo;Bouma shape&rdquo; has also been used by P. Saenger. I just simplified it (being a big believer in two-syllable labels).</p>
<p>BTW, I would contend that:</p>
<ul>
<li>It&rsquo;s not about bouma uniqueness, but the degrees of bouma confusability.</li>
<li>The silhouettes are much more significant than the interiors, so that grouping is over-simple; the 1/2/3/4 and 5/6 groupings need to be on a joined and on a lower level.</li>
<li>It&rsquo;s all processing; no pre-processing.</li>
<li>I think &ldquo;skipping&rdquo; might be misleading, since no content is skipped (in &ldquo;real&rdquo; reading).</li>
</ul>
<p>hhp</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-16 15:02:35"
			itemprop="datePublished"
		>
			2013-06-16 15:02:35
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:hrant">
            hrant
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>The term Bouma shape as used by Paul Saenger in <em>Space Between Words: The Origins of Silent Reading</em> is clearly taken from Taylor and Taylor. But here is how Saenger defines the term in his glossary : “The shape of a word when written in upper- or lower case letters and delimited by space, as defined by the Dutch psychologist Herman Bouma.”</p>
<p>Saenger’s argument in the book is. “While the paleographer’s principal focus has been on the classification of individual letter forms, the student of the history of reading in the medieval West is primarily concerned with the evolution of word shape, and letter forms are important only to the degree that they play a role in determining that shape. Thus the adoption of the miniscule, that is, lower case letters, as a book script is significant for the historian of reading insofar as it contributed, in conjunction with word separation, to giving each word a distinct image.”</p>
<p>I originally glommed on to the “each word a distinct image” idea because of the content the Taylors gave to it, and because of Gerrit Noordzij’s account of the ‘consolidation’ of the word [Chapter 6 of <em>The stroke: theory of writing</em>]. Now I believe that reading starts with a low-resolution indexing of word items in parafoveal preview and proceeds to a feature-analytically based processing routine that occurs at and upon fixation. The low resolution indexing provides informative “ensemble statistics” that go beyond the envelope shape of the word or the simple pattern of ascending, descending and neutral characters, and provides a reference frame for the feature-analytic processing. The feature-analytic processing yields information about words at the level of their role-units (prototypical structures, like stems and counters): the identity of the role units, their relative positions, local combinations (in letters), and their across-the-word distributions.</p>
<p>So where the the Taylors’s notions might apply most directly, as far as I can see, is in relation to the kind of information the “ensemble statistics” contain. There is currently a growing body of research in this area.</p>
<p>But I just wanted to provide a little push-back on Thomas’s “bouma nonsense” comment.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-16 16:53:25"
			itemprop="datePublished"
		>
			2013-06-16 16:53:25
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">By the way, Thomas, I am not seeing that upper case letters are less differentiated from one another in terms of shape. Are H and N less different than h and n? Are B and D less different than b and d? What study are you alluding to? I thought the usual explanation for slower reading of all caps, for the letter ID and look-up view, is that lower case is more familiar—which I also don&rsquo;t buy as an explanation, but that is a different story.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-16 18:11:19"
			itemprop="datePublished"
		>
			2013-06-16 18:11:19
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Thomas, these matters are by no means settled. The &ldquo;word superiority effect&rdquo;—letters within words are more identifiable in a short time than letters within non-words—prima facie supports the idea of whole word pattern. Word pattern does not necessarily mean the oversimplified version of word envelope, as Peter says. Peter&rsquo;s idea is that for skilled readers and familiar words, the visual pattern of the whole word, like a Chinese Character, is read, rather than by going the route of first identifying letters and then doing a look-up—which we can also do. (And yes, I know there are root characters in Chinese.)</p>
<p>Supposedly the word superiority effect is made compatible with the letter identification and look-up view by &lsquo;interactive activation&rsquo;, but that this operates routinely for skilled readers has not been actually tested, only simulated in computer models. It is a model that can simulate the process, but whether it actually works that way in the brain is not known. Peter and I are quite doubtful that it operates routinely as claimed, because we think it would take too much time in the brain for the rapidity of skilled, normal reading. A more more one-way link between visual pattern and sound and semantics (rather than cycling up and down) would be quicker.</p>
<p>My understanding is that a difference between Peter&rsquo;s and Hrant&rsquo;s views is that Hrant thinks Boumas operate mainly in the parafovea, whereas Peter thinks this is an important but limited influence, and the main impact is on letters seen in the fovea.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-16 21:21:38"
			itemprop="datePublished"
		>
			2013-06-16 21:21:38
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Although h and n in one way, and b and d in another, are good examples, a more typical case would be to compare a and t to A and T. The presence of ascenders and descenders does make both words and letters easier to differentiate; all-caps text is clearly significantly less readable than lower-case text. Many studies have confirmed that, although it should be obvious for almost any typeface.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-17 03:59:40"
			itemprop="datePublished"
		>
			2013-06-17 03:59:40
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:quadibloc">
            quadibloc
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I agree that descenders and ascenders help. My comment was to rebut the explanation that Thomas reports: that they differentiate letters more. As far as topology the caps don&rsquo;t seem any less differentiated. The eye may pick out ascenders more easily, but then you are getting to the added effects of letters being visually within words, which the letter ID and look-up theory deny.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-17 16:42:38"
			itemprop="datePublished"
		>
			2013-06-17 16:42:38
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Letter differentiation and even the parallel processing of letters is taking place within the context of overall size, as denoted by the other neighboring letters. I won&rsquo;t suggest for a moment that such does not matter. Without that there would be no meaning of x-height, ascender and descender. Topology alone isn&rsquo;t everything, clearly.</p>
<p>I had always heard tell of &ldquo;bouma&rdquo; as the &ldquo;shape of the word&rdquo; in terms of its outer silhouette. If the bouma includes the topology of indivdual letters, then&hellip; well, it seems kind of like a two-party political system where both parties are moving toward the middle. Instead of a clear dichotomy between two theories, it seems more like a spectrum. I will be curious to hear what folks think the practical differences are between a theory driven by bouma shape (that includes full topology of individual letters) and one driven by individual letters that allows for interactions and effects of neighboring letters. Really, &ldquo;bouma shape&rdquo; doesn&rsquo;t feel so top-down in this version&hellip;.</p>
<p>Cheers,</p>
<p>T</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-19 21:22:23"
			itemprop="datePublished"
		>
			2013-06-19 21:22:23
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Thomas&#43;Phinney">
            Thomas Phinney
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Rather than rely on theoretical arguments, there is experimental data from traffic signs that mixed case signs are better than all caps for comprehension.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-19 21:34:24"
			itemprop="datePublished"
		>
			2013-06-19 21:34:24
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Delete">
            Delete
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p><em>there is experimental data from traffic signs</em></p>
<p>Yes, but all this data confirms is that mixed case text is better for traffic signs. The data doesn&rsquo;t indicate <em>why</em> mixed case is better, which is what the theoretical arguments are about.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-20 00:14:36"
			itemprop="datePublished"
		>
			2013-06-20 00:14:36
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Thomas: <em> I will be curious to hear what folks think the practical differences are between a theory driven by bouma shape (that includes full topology of individual letters) and one driven by individual letters that allows for interactions and effects of neighboring letters.</em></p>
<p>I think part of the attraction of Peter&rsquo;s proposed feature role model is that it suggests ways in which we might bypass the problems of interactions and effects of neighbouring letters, specifically the problem of crowding, which demonstrably makes individual letter recognition difficult. If we don&rsquo;t need to recognise individual letters in order to make a first pass at word recognition, then that would explain why crowding doesn&rsquo;t seem to be a major impediment to rapid and accurate reading. If, instead of recognising individual letters <em>as such</em> we are taking in information from multiple letters in the foveal fixation (I&rsquo;m with Peter on this, not Hrant) and recognising patterns that resolve to letter sequences, then the closeness and visual interaction of those letters that inhibit individual letter recognition actually become useful. This is how I understand boumas: a perceptual unit of recognition.</p>
<p>The latter point is important, I think, because it avoids having to commit to defining bouma as any specific graphical phenomenon &ndash; such as the &lsquo;word shape&rsquo; &ndash; and, indeed, indicates why such definitions are unhelpful. A bouma is a thing in the perception, not a thing on the page. This also means, of course, that it is liable to individual variation, i.e. you and I might perceive different boumas when reading the same piece of typography.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-20 00:31:44"
			itemprop="datePublished"
		>
			2013-06-20 00:31:44
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Thanks to all who elaborated on the topic.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-20 20:45:23"
			itemprop="datePublished"
		>
			2013-06-20 20:45:23
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Thomas&#43;Phinney">
            Thomas Phinney
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Incidentally, on the subject of how old 5-level code is: just today, I came across an ad for a 3M Whisper Writer 1000, a teletypewriter that used a thermal printer, in a 1983 issue of <em>Datamation</em>.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 01:47:32"
			itemprop="datePublished"
		>
			2013-06-21 01:47:32
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:quadibloc">
            quadibloc
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>@IsleofGough  “<em>…there is experimental data from traffic signs that mixed case signs are better than all caps for comprehension</em>.”</p>
<p>Reference?</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 03:42:00"
			itemprop="datePublished"
		>
			2013-06-21 03:42:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">my eyes.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 04:49:47"
			itemprop="datePublished"
		>
			2013-06-21 04:49:47
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:russellm">
            russellm
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>if you are looking at a sign from such a distance that you can distinguish the general shape of the words but not the letters, you will be able to understand the message - especially on a traffic or regulatory sign where there is a limited number of possible messages with less difficulty if the message is set in mixed case than if it is set in all caps. This is hardly even worth discussing.</p>
<p>No 
Trespassing</p>
<p>on an 18&quot; square sign is easier to read than</p>
<p>NO
TRESPASSING</p>
<p>from 100 meters. Try it.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 05:04:45"
			itemprop="datePublished"
		>
			2013-06-21 05:04:45
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:russellm">
            russellm
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Russell, IoG referred to &lsquo;experimental data&rsquo;, not anecdotal evidence, so Chris is quite justified in asking for a reference.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 06:30:15"
			itemprop="datePublished"
		>
			2013-06-21 06:30:15
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>[Thomas Phinney] <em>I will be curious to hear what folks think the practical differences are between a theory driven by bouma shape (that includes full topology of individual letters) and one driven by individual letters that allows for interactions and effects of neighboring letters. </em></p>
<p>Thomas, for me it’s mainly a matter of <em>compatibility</em> with the express and defining attunements of professional type-involved practitioners. And it’s a matter of providing perceptual-processing <em>touchstones</em>  for typography and type design.</p>
<p>If a feature-analytic processing of the S1 to S4 / A1 &amp; A2 / D shapes (see above) in <em> bounded maps</em> of visual information is seen as a foundational dynamic at the “front end” of reading, factors like distinctive cue-value, clear delineation, proper salience, relative location and some kind of cohesive equilibrium at the elemental level of shape primitives (letter details) are easily seen to be matters of intrinsic and densely interacting importance. A rhythmic spacing, a consistent contrast-styling scheme and strategic construction here, mindful of these factors, produce a gestalt integrity at the level of the whole word. So this provides a natural fit with the express attunements of experienced type designers and typographers to matters of spacing or fit, consistent contrast styling, and strategic construction.</p>
<p>The feature analytic processing that leverages bouma-shape particulars isn’t the whole story in identifying words though. Feature-analytic processing is generally thought to lead to a kind of parallel letter recognition as a next step which then underpins the orthographic processing necessary to get to words. In current models the feature-analytic processing is often underspecified, but your sense of moving toward the middle is apt. The two processing routines are compatible: they can be seen as different phases or sub-routines of a single hierarchical process.</p>
<p>I’m exploring the equally compatible — and neuro-mechanically plausible — idea that in the normal reading of extended texts by skilled readers feature-analytic processing yields — as a result of “unsupervised” perceptual learning — a more elemental decomposition than the decomposition into individual letters. My more elemental decomposition is a decomposition into what I’ve been calling role units. This then underpins a higher level parsing of the word into what can be called role-unit string kernels.* By leveraging the overlap in these string kernels in familiar words the visual word-form resolution system gets to words and a word superiority effect.</p>
<p>I’m generally hesitant to use the thread-space of a topic with another focus to summarize my perspective, but I feel it might be necessary to show that what’s involved in addressing your follow-up Thomas is not just a moving to the middle.</p>
<p>In the context of this thread, and its bouma-shape by-way, I suspect that the lower case construction is a construction that more effectively leverages for the purposes of individuating gestalt-level units, the dimensionality of the western alphabet’s cartesian feature-space — with it’s more diversified or informationally rich implementation of the compact base-line to x-height zone and more strategic use of the ascender and descender zones.</p>
<hr>
<ul>
<li>string kernels in this scheme are units with the structure x+(y+z)+a (where x and y and z and a are role-units, and the “+” sign inside the bracket indicates a local or contiguous combination implemented by a letter junction or a common edge, and the “+” sign outside the brackets indicates an “open” or non-contiguous combination constrained by immediate adjacency.</li>
</ul>
<p><img src="/images/old-images/string_kernels_72ppi_5970.jpg" alt=""></p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 12:55:13"
			itemprop="datePublished"
		>
			2013-06-21 12:55:13
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">No doubt he is, but I did suggest trying it :o)</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 15:40:35"
			itemprop="datePublished"
		>
			2013-06-21 15:40:35
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:russellm">
            russellm
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">[[http://clearviewhwy.com/ResearchAndDesign/legibilityStudies.php|Here are the experimental tests on Clearview Highway]], which found that U &amp; lc was more legible—readable quickly at a greater distance—within a given sized sign, than an all caps font. This was just one comparison, though.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 15:56:41"
			itemprop="datePublished"
		>
			2013-06-21 15:56:41
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Peter, would you predict that a lowercase string kernel nucleus is easier to recognize in the presence of a string kernel than on its own: in your example, the <em>a</em> is easier to recognize between <em>h</em> and <em>t</em> than a lowercase <em>a</em> on its own? Is this also true when the letters aren’t part of a word: would it be easier to recognize the lowercase <em>a</em> when it is between <em>j</em> and <em>t</em> than a lowercase <em>a</em> on its own? Does this effect go away for uppercase letters, or is it just diminished?</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 17:17:55"
			itemprop="datePublished"
		>
			2013-06-21 17:17:55
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Kevin&#43;Larson">
            Kevin Larson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I see a written word as an interaction of shapes as well as agreed upon letter forms.  Some type faces do a better job of integrating both.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-21 19:28:07"
			itemprop="datePublished"
		>
			2013-06-21 19:28:07
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:dezcom">
            dezcom
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Kevin, good questions. I’m thinking it through — jat is a pronounceable non-word and might partially activate two string kernels, and a letter is a string-kernel too, but with a lower dimensionality, so it gets complicated. I&rsquo;ll respond more fully later, but it sounds like something worth testing.</p>
<p>You might be interested to know that one of Jonathan Grainger’s former students Thomas Hannagan adapted the notion of string kernels to build on Grainger’s “open-bigram” model of how the brain encodes orthographic information during reading. My scheme uses a somewhat different application of the string-kernel concept than Thomas Hannagan introduced by transposing it from the level of letters to the role-unit level. My application and extension of the idea can probably account better for the speed and automaticity of lexical decision, and the speed and automaticity of “visual word-form resolution” in the “immersive” reading of skilled readers (which are probably related), while Thomas’s can probably account better for transposition effects such as those that occur in that “Cambridge” scrambled letter text. These predictions might be able to be quantified, implemented in a model and lead to a test.</p>
<p>Thomas’s string kernel paper — [2012] titled “Protein analysis meets visual word recognition: a case for String kernels in the brain” — is listed here: 
<a href="http://www.neuraldiaries.com/ThomasHannagan/publications.html">http://www.neuraldiaries.com/ThomasHannagan/publications.html</a></p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-22 03:22:40"
			itemprop="datePublished"
		>
			2013-06-22 03:22:40
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Point of information: jat is a word, referring to a member of a Punjabi peasant caste.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-22 07:06:14"
			itemprop="datePublished"
		>
			2013-06-22 07:06:14
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">@enne_son: Following Larson’s remarks, you may also wish to consider the difference between what happens with <a href="http://en.wikipedia.org/wiki/Pseudoword">pseudowords</a>, <a href="http://www.talktalk.co.uk/reference/encyclopaedia/hutchinson/m0038925.html">letter strings</a>, and random letter combinations.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-22 13:17:56"
			itemprop="datePublished"
		>
			2013-06-22 13:17:56
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Chris, Kevin: in the scheme I&rsquo;m proposing string kernels are “units” in a single “hidden” layer of a 3 to 4 deck hierarchical network. In its default operational mode the network doesn’t do “explicit labelling”  at the letter level. So I’m not sure I can make direct predictions that presume explicit labeling of the elements in the string kernel nucleus when the string kernel is presented in isolation — that is, outside of the context of the whole word or the full orthographic sequence, and abstracted from the full hierarchical network.</p>
<p>My first impulse was, however, to say that, as a consequence of the scheme, the &gt;a&lt; probably is easier to recognize between &gt;h&lt; and &gt;t&lt; than when it is between &gt;j&lt; and &gt;t&lt;. Even with John Hudson’s caveat, because for most jat is unfamiliar. The other cases seem less straightforward. But now I&rsquo;m not so sure there would be an effect.</p>
<p>In my scheme the detection of a string kernel by a string kernel detection neuron builds on activity at an earlier level, which involves what I&rsquo;ve been calling role-unit level “quantization,” local combination detection, and across-the-word pair-wise distribution mapping, which occur simultaneously and in concert with each other at a single level. So another way of asking the question is: is local combination detection easier when it is in the context of a word than when it operates without flankers, even if the flankers are part of an actual word.</p>
<p>That’s as far as I’ve gotten so far with an answer.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-24 03:56:03"
			itemprop="datePublished"
		>
			2013-06-24 03:56:03
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>After I read that the story about the US Navy ‘banning ALL CAPS’ on the CNN site, I commented that the next thing I’d see happen is the mixed writing of names on credit cards. Someone commented that this would be ‘impossible’ because of legal restrictions. Unfortunately I couldn’t comment on that anymore, because either Disquss or CNN prevents me from tracking and commenting that discussion (is this what Snowden is talking about? :–). Being halfway able to track what I wrote on Typophile, my question for today is wether anyone here knows about such (US?) legislation?</p>
<p>Btw, in Germany, the mixed writing of names on bank cards does not seem to be a problem. On some of my german EC bank cards, my name is written mixed. Also, the new DIN 1450 on legibility states that text has to be written mixed. All caps is to be used for emphasis only. At some point DIN 1450 might collide with some US credit card legislation …</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-24 12:59:28"
			itemprop="datePublished"
		>
			2013-06-24 12:59:28
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Albert&#43;Jan&#43;Pool">
            Albert Jan Pool
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Peter, the Hannagan and Grainger paper seems to associate string kernels strongly with the open bigram coding model. Is this also a factor of your proposal?</p>
<p>In a recent paper, Kinoshita and Norris present experimental results that question the open bigram model. 
<a href="http://www.sciencedirect.com/science/article/pii/S0749596X13000223">http://www.sciencedirect.com/science/article/pii/S0749596X13000223</a></p>
<p>Thoughts?</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-24 20:08:46"
			itemprop="datePublished"
		>
			2013-06-24 20:08:46
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>BTW, have you read the Norris and Kinoshita &lsquo;noisy channel&rsquo; paper?</p>
<p>&lsquo;Reading through a noisy channel: why there&rsquo;s nothing special about the perception of orthography.&rsquo;
<a href="http://psycnet.apa.org/?&amp;fa=main.doiLanding&amp;doi=10.1037/a0028450">http://psycnet.apa.org/?&amp;fa=main.doiLanding&amp;doi=10.1037/a0028450</a></p>
<p>I&rsquo;ve only seen the abstract, but it sounds lively and contentious:</p>
<blockquote>The goal of research on how letter identity and order are perceived during reading is often characterized as one of “cracking the orthographic code.” Here, we suggest that there is no orthographic code to crack: Words are perceived and represented as sequences of letters, just as in a dictionary. Indeed, words are perceived and represented in exactly the same way as other visual objects. The phenomena that have been taken as evidence for specialized orthographic representations can be explained by assuming that perception involves recovering information that has passed through a noisy channel: the early stages of visual perception. The noisy channel introduces uncertainty into letter identity, letter order, and even whether letters are present or absent. We develop a computational model based on this simple principle and show that it can accurately simulate lexical decision data from the lexicon projects in English, French, and Dutch, along with masked priming data that have been taken as evidence for specialized orthographic representations.</blockquote></div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-25 00:19:53"
			itemprop="datePublished"
		>
			2013-06-25 00:19:53
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, my default scheme doesn&rsquo;t have a bigram-coding “deck.” I suspect Thomas Hannagan will find a way to adapt his string kernels idea to accommodate the new data presented by Kinoshita and Norris. Whether what emerges will still be able to bear the name &ldquo;open-bigram&rdquo; model, I don’t know.</p>
<p>The assumption underlying most current computational models of orthographic processing is that “the game of visual word recognition is played in the orthographic court of constituent letter recovery.” This is a quotation from Ram Frost in a 2011 <em>Behavioral and Brain Sciences</em> target paper that tries to plot the path towards a “universal” model of reading. Thomas Hannagan applies the string kernel idea to orthography. I apply it to role-units.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-25 18:46:17"
			itemprop="datePublished"
		>
			2013-06-25 18:46:17
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, I&rsquo;ve been following the Norris and Kinoshita work for several years and have read the paper you referenced. The contention that there is no orthographic code to crack is a bit of a red herring, because, in the stimulus sampling approach used by Norris and his associates, a representation of letter order is assumed. On their own account “the Bayesian Reader (Norris, 2006) assumes that word recognition involves a representation of input string consisting of sequentially ordered letter identities.” These form the “priors” in a process that decides if the pattern of letters in the stimulus can be fitted to the coded representation. The newer work proposes “an extension to the Bayesian Reader that incorporates letter position noise during sampling from perceptual input.”</p>
<p>My scheme doesn’t do everyday recognition on the basis of a representation of letter order, but on the basis of a relational filtering (see Bill’s post just above) at the level of role units, that detects string kernels before getting to words.</p>
<p>In 2012 Jeffrey S. Bowers and Colin J. Davis wrote a paper highly critical of the Bayesian approach with the provocative title “Bayesian Just-So Stories in Psychology and Neuroscience.” 
See the downloadable pdf at the top of the list here:
<a href="http://scholar.google.ca/scholar?cluster=5694260091742085801&amp;hl=en&amp;as_sdt=0,5">http://scholar.google.ca/scholar?cluster=5694260091742085801&amp;hl=en&amp;as_sdt=0,5</a>
Griffiths, Chater, Norris and Pouget replied to this here:
<a href="http://www.yangzhiping.com/files/pubs/415.pdf">http://www.yangzhiping.com/files/pubs/415.pdf</a>
Bowers and Davis replied to the reply here:
<a href="http://scholar.google.ca/scholar?cluster=17829249214662681864&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2012">http://scholar.google.ca/scholar?cluster=17829249214662681864&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2012</a></p>
<p>So it gets complicated and contentuous, and we might have to bring in the US Navy to settle accounts using lower case so we can read it comfortably.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-25 18:55:58"
			itemprop="datePublished"
		>
			2013-06-25 18:55:58
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Kevin, John, I&rsquo;ve been batting back and forth with Peter some ideas about a brain processing model that would give a more detailed causal account of how the brain would work to identify words in the manner Peter has hypothesized. This model building is with an eye to identifying a crucial experiment to test the usual &lsquo;slot processing&rsquo; of letters against Peter&rsquo;s whole word model.</p>
<p>First, Peter warns me that the phrase &ldquo;word specific visual pattern&rdquo; is often used in the literature to refer to all aspects of the word—the type style, weight, the slant, etc. So above I should have more accurately said that Peter&rsquo;s theory claims that immersive reading of skilled readers involves identifying words by the <em>relational structure</em> of letter features across the whole word. The key features he thinks are &lsquo;role units&rsquo; such as ascenders, bowls, stems.</p>
<p>The model I&rsquo;ve developed does give a prediction in answer to your insightful questions, Kevin. I call the model &ldquo;recursive  relationship filtering,&rdquo; using &ldquo;matrix resonance.&rdquo;</p>
<p>Before I describe it, let me give some background that will make it more clear where I am coming from. My impression—and I only know the literature second hand in discussing with Peter—is that most of the modeling is too heavily influenced by the Anglo-American tradition of associationism, beginning with Locke, Berkeley, and Hume. The idea is that the whole story of learning is variations on the theme of repeated association of events. Kant thought that instead we have an inborn framework that we use to interpret perceptual information, and without that framework we are &ldquo;blind.&rdquo; So association plays a role, but inner frameworks are equally important.</p>
<p>A key development beyond Kant came with Darwin&rsquo;s survival of the fittest. Psychologists and philosophers took Darwin as a model for learning, and said that Kant&rsquo;s idea of inner frameworks was correct, but that we actively adapt and change them, using trial and error to better and more accurately capture meaningful information from the welter of perceptual data that comes into our brains. This was the idea of Külpe in Germany, and is followers in the Gestalt and Würtzburg schools of psychology, and by the American pragmatist Charles Saunders Peirce. My teacher Popper was influenced by teachers from the Würtzburg school, as I explained in my book with John Wettersten, <em>Learning from Error</em> (in the German edition <em>Lehrnen aus dem Irrtum</em>).</p>
<p>The relevance of this long-standing dispute to reading models is that I don&rsquo;t assume that skilled readers simply use the same process as those learning to read, but just do it faster. —The processing patterns of learning and skilled readers are not isomorphic. Instead, I assume that in learning to read fluently, the brain sets up, by trial and error, a framework and pattern of connections that will very quickly and efficiently decode the structural pattern of words.</p>
<p>This is related to Elkhonon Goldberg&rsquo;s theory (See <em>The Executive Brain</em>) that one side of the brain deals more with novelty, and the other with learned, more efficient processing structures. In effect, as we learn, a much more efficient structure is set up on one side of the brain only. This idea is corroborated, Peter tells me, by the discovery that skilled readers have more &lsquo;lateralization&rsquo;—processing only in one side—than learners, who use both sides more.</p>
<p>I&rsquo;ll post this prelim now, and then explain the model later today.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-25 23:53:36"
			itemprop="datePublished"
		>
			2013-06-25 23:53:36
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>So here&rsquo;s the processing model of &lsquo;relational filtering&rsquo; to identify words, using Peter&rsquo;s concepts.</p>
<p>First, it is a decoding model, encoding structural features, and then matching them to words. I don&rsquo;t know whether it operates at early visual processing levels, or only after the visual pattern is already processed to the point that we would recognize any script that we can&rsquo;t decipher. Second, this basic model doesn&rsquo;t assume that Peter is right about reading whole word structural patterns, though I think he is.</p>
<p>The motto is Michelangelo&rsquo;s &ldquo;the more the marble wastes, the more the statue grows.&rdquo; Its central feature is a filtering model, rather than a pure building-up model. It starts with a huge amount of data, and ends up with a single word through relationship filtering which arrives at a brief code.</p>
<p>Each filter layer I will describe in terms of functionality, as I don&rsquo;t know exactly how this is executed by one or masses of neurons.</p>
<ol>
<li>
<p>The bottom layer or deck is a layer in which we throw a framework of an inner matrix (inborn or learned) over the incoming data.</p>
</li>
<li>
<p>All the information on the bottom layer is sent to layer two, which establishes a &lsquo;set&rsquo; as to what kind of pattern it is: whether the pattern is, for example: 1. a face, 2. a three-dimensional object, or 3. writing to be decoded. If the criteria in this second layer in the &lsquo;face&rsquo; area aren&rsquo;t met, the signal is stopped, and not sent on from this area. Since it is a word the &lsquo;word&rsquo; filter will detect a match, and all of the information will be passed on to the third level.</p>
</li>
<li>
<p>On the third level, letter features are coded. The cells in the matrix in each small area are sent forward to the fourth level, in which a group of say four vertical cells is coded to represent location.</p>
</li>
<li>
<p>The information on the third level is mapped to arrays of role unit detectors on the fourth level; each detector in an array can detect one of the possible combinations of the four states of the cells. All detectors receive the information of all four cells, but only where there is a match to the code of a particular role unit is the information sent on to the next level. All the alternative ID&rsquo;s for a particular role unit are passed on to the next level preserving information on what is adjacent.</p>
</li>
<li>
<p>The fifth level puts together the role units in Peter&rsquo;s &lsquo;string kernels&rsquo;, as described above, and passes them directly to &lsquo;word codes&rsquo;, which code words as an assembly of string kernels.</p>
</li>
<li>
<p>On the sixth and final level are all words represented in whole word structural relationship codes, codes of adjacent string kernels. Each complete word code assembled on level 5 is &lsquo;painted&rsquo; or sent across the entire vocabulary, or at least common vocabulary, and whatever remembered word code matches that input, responds and links to a meaning and sound. The word is detected consciously at this point.</p>
</li>
</ol>
<p>Now this sixth level is the final decoding level only for skilled readers reading familiar words. If it is an unfamiliar name, for example, or a confusion typo, the information will be sent on to an orthographic level, where letters are identified and look-ups done on higher levels again.</p>
<p>In other words, reading by visual pattern, without any orthographic processing, is the preferred route, as it is quickest. But we can also do quite quickly—but a bit longer—the orthographic route. All of these routes have been built up in the learning process, and unfamiliar names etc. regularly draw on these skills. The idea is that processing by whole word structural pattern is the last learned, but the first used.</p>
<p>When the words are well done visually, and the memory of the code is there, this is experienced as instant reading of words: we don&rsquo;t see letters, we experience meaningful words. In something like the scrambled letter tests, we are conscious of some time taken to stare at a word to decipher it.</p>
<p>The main advantage of of model like this is that it accounts for why we read AlTeRnAtiNg CaSe more slowly, which the interactive activation model just puts down to lack of familiarity. But this explanation is not plausible because the interactive activation says we first identify separate letters as abstract entities, and we are in fact extremely familiar with both lower case and caps.</p>
<p>Next, I will post answering Kevin&rsquo;s questions with this model, and discussing all caps verse lower case. Sorry for the lack of an illustrative diagram. I don&rsquo;t have time to do one now.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 00:00:17"
			itemprop="datePublished"
		>
			2013-06-26 00:00:17
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I&rsquo;m sympathetic to both the matrix and role unit ideas but, other than a presumption of efficiency, is there a reason to favour a model that builds up from these to whole word structure recognition, rather than considering that they might contribute to letter recognition and hence to orthographic processing?</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 00:27:21"
			itemprop="datePublished"
		>
			2013-06-26 00:27:21
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I think it&rsquo;s only efficiency, in terms of being quicker. But this is a big deal in brain processing, I think. Matthew Luckiesh compared reading speed to blood pressure. The body will do almost anything to keep up blood pressure, because if it is too low you can pass out and die. Similarly, the brain wants to perceive meaning, and perceive it now. It is a matter of survival, which carries over to reading speed.</p>
<p>The brain is massively connected—more synapses than stars in the milky way!—and does parallel processing, but it is not that quick in cycling through a reflective process. (Reflection undoubtedly involves interaction of levels as envisaged in interactive activation.) Compare the speed of recognizing a face and adding 436 + 784. Also I don&rsquo;t see how going to a look-up of abstract letter identities isn&rsquo;t an additional step and time. You already have the letter features, and relationships in order to identify letters. If you just squeeze the ID from that, without going to abstract letter identities, you are leaving out a step, it seems to me, so that it is quicker. That&rsquo;s why I think we learn the whole word structural pattern—unconsciously, just as we learn grammar in our mother tongue.</p>
<p>Whether we need to identify the flanking role units in the manner Peter suggests is less clear to me. We might have identified the structural pattern of the letters, and of the relational pattern of the role units in whole word in another way, maybe. But I don&rsquo;t see why a look-up of orthography of abstract letter units is needed routinely.</p>
<p>An important point to note is that the word superiority effect is a time-based effect. It is only when you flash words for very short times followed by a mask that you get it. It you look longer at words and non-words, it goes away. That to me is an indication of the fact that the race to word ID is won by whole word relational pattern, before it goes to orthography.</p>
<p>The race is &lsquo;won&rsquo; when you get a meaningful word into working memory, as that is key to consciousness. To me you get the word superiority effect because word ID by the whole word structural pattern gets into working memory before the orthographic processing route. We are also very good at the orthographic route, but it isn&rsquo;t as fast.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 00:58:36"
			itemprop="datePublished"
		>
			2013-06-26 00:58:36
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I&rsquo;m not convinced by the efficiency argument, per se, because evolution favours good-enough solutions over optimal solutions. Your comments about the word superiority effect gave me pause, but that is, after all, a <em>letter recognition test</em>. What I have posited is that narrow matrix alignment of role units perceived as boumas aid letter recognition in a crowded context, and the so-called &lsquo;word superiority effect&rsquo; is exactly what one would anticipate in that model, because real words are made up of role units arranged in ways that produce familiar bouma shapes, and non-words are not.</p>
<p>I&rsquo;m really looking forward to your experiment design ideas!</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 02:30:59"
			itemprop="datePublished"
		>
			2013-06-26 02:30:59
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>It sounds as though Norris and Kinoshita are presenting an interesting idea: that people read as a naive person would assume, by recognizing the letters one by one, but the fact that they do so under adverse conditions, and with the aid (or otherwise) of a visual pre-processing system developed for other purposes makes it appear that &lsquo;bouma&rsquo; and word shape play a direct role in reading.</p>
<p>After all, it&rsquo;s only occasionally in reading that one has to guess which letter is in any given position. A lot of people even move their lips when they read. Those who read more quickly are more likely to be recognizing a word at a time, but even they consciously think of themselves as reading individual letters.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 02:31:11"
			itemprop="datePublished"
		>
			2013-06-26 02:31:11
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:quadibloc">
            quadibloc
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John asked: is there a reason to favour a model that builds up from role-units to whole word structure recognition, rather than considering that role units might contribute to letter recognition and hence to orthographic processing?</p>
<p>I think it can be argued that in the part of the visual cortex where letters are recognized, the neurons’ preferred receptive field is larger than the single letter, and includes parts of adjacent letters in optimally spaced text typographically speaking. In a relational filtering environment, this mismatch makes the local combination detection required to get to letters subject to crowding, unless the role-units that are adjacent to the string-kernel nucleus, are part of an already learned and synaptically supported code-item.</p>
<p>According to Denis Pelli and others crowding is negligible in foveal vision. However, in a poster presentation at this year’s Vision Sciences Society Annual Meeting last month, Mara Lev, Oren Yehezkel, and Uri Polat found that when foveal processing of letter targets is interrupted by backward masking, the spatial crowding is revealed and that a release from crowding in the fovea is achieved by allowing an increase in reaction time.</p>
<p>So there is a psychophysical reason to favour a model that goes from role-units to string kernels instead of letters. Getting to independent letters would require squelching of elements in the area surrounding the string kernel nucleus.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 02:54:26"
			itemprop="datePublished"
		>
			2013-06-26 02:54:26
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, following the above model, the word superiority effect comes from the word getting into working memory before the letters are identified as such. We don&rsquo;t identify the middle letter or whatever but deduce it from knowing the word. So it&rsquo;s not really &lsquo;letter recognition&rsquo; in my model and Peter&rsquo;s theory, it&rsquo;s word recognition and letter deduction.</p>
<p>About what evolution favors, I think being able to read &lsquo;meaning&rsquo; in scenes and situations, very rapidly, is highly rewarded both for survival and reproduction. Language in particular is essential to social interaction, which is the big thing that gives us our advantage. Also it is important to winning and keeping a mate, so good language skills give a competitive advantage, pushing beyond &lsquo;good enough.&rsquo;</p>
<p>The guy who can understand ladies very well is way ahead of the game. Of course I&rsquo;ve never met one who could :)</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 02:56:43"
			itemprop="datePublished"
		>
			2013-06-26 02:56:43
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Very interesting analysis, Peter.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 03:00:48"
			itemprop="datePublished"
		>
			2013-06-26 03:00:48
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Remind me, does the word superiority effect apply only to real words, or also in any degree to word-like sequences, i.e. non-words that correspond to orthographic patterns of the given language.</p>
<p>Or, put the question another way, is the word superiority over non-words the same as the word superiority over pseudo-words? Is there a superiority of pseudo-words over non-words?</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 04:13:28"
			itemprop="datePublished"
		>
			2013-06-26 04:13:28
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">John, it’s the strongest when real words are used, but superiority effects also occur for pseudo-words that follow the orthographic patterns that occur in real words. Consonant strings perform the worst.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 04:26:33"
			itemprop="datePublished"
		>
			2013-06-26 04:26:33
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Thanks, Peter. Superiority effects for pseudo-words suggest something other than &ndash; or in addition to &ndash; &lsquo;knowing the word&rsquo; is at work.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 05:28:01"
			itemprop="datePublished"
		>
			2013-06-26 05:28:01
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>@William Berkson:
The human brain has high requirements for oxygen, calories, and protein. Thus, how can we explain the extent to which it is developed, considering that only long after the brain was developed did intelligence bring a big evolutionary payoff, by letting humans invent things like the bow and arrow - never mind rifles and shotguns and antibiotics?</p>
<p>In nature, some animals have structures that are developed out of all proportion to their survival needs. The antlers of the Irish Elk. The tail of the peacock. And these were due to sexual selection.</p>
<p>Thus, it is very likely that sexual selection played a significant role in the development of human intelligence.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 13:42:58"
			itemprop="datePublished"
		>
			2013-06-26 13:42:58
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:quadibloc">
            quadibloc
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, one of the key weaknesses with the research on the word superiority effect that Peter has pointed me to is that the time between the onset of the stimulus and the mask, the so-called SOA (Stimulus Onset Asychrony), has not been well studied and controlled. They tended to just adjust it until they got results. But how long an SOA gets what results isn&rsquo;t clear.</p>
<p>What the masking seems to do is cut off the processing—by feeding in a new image—and the exact time interval matters, and could be very revealing of what is going on.</p>
<p>So to evaluate the relationship between word and pseudo word and non word effects, you&rsquo;d need to be more systematic about controlling the SOAs.</p>
<p>My suspicion is that when the whole word relational pattern fails, the signals with information about the role units are immediately fed forward for orthographic and sound of syllables processing. Some of that must be accessible to memory immediate after the mask. But Peter&rsquo;s note that the superiority of pseudo words is less; so this indicates some kind of hierarchy in time on the processing, with in my view the whole words pattern first.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 15:33:19"
			itemprop="datePublished"
		>
			2013-06-26 15:33:19
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, by definition a pronounceable pseudo-word contains several identifiable string kernels.</p>
<p>Superiority effects are about the facilitating and /or inhibiting effect of context. Interactive Activation schemes see the facilitation and inhibition as a consequence of the two-way communication between decks and as requiring a series of feed-forward and feed-backward <em>epochs</em> to sort themselves out.</p>
<p>In a relational filtering environment using role-units, string kernels and an intrinsic integration interface, local combination detection is already inhibited in non-words and facilitated in familiar words by the time we get to the string-kernel level in the initial feed-forward sweep. It is inhibited in non-words by the presence of unfamiliar flanking role units, and facilitated in words and pronounceable pseudo-words by familiar flanking role-units.</p>
<p>Local combination detection is foundational to letter recognition, but the post-mask letter recognition called for in the Richer-Wheeler versions of the tests — tests that are biased toward encouraging rapid automatic word-form resolution at the front end — requires explicit labeling at the post-mask end. It can be argued then, that the post-mask letter identification benefits from the early low-level facilitation of local combination detection in the string-kernel context.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 15:39:28"
			itemprop="datePublished"
		>
			2013-06-26 15:39:28
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Bill, I don&rsquo;t think longer SOAs were needed to get pseudoword superiority effects. I think they were needed to get word superiority effects with alternating case stimuli and words with increased spacing between the letters.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 16:21:25"
			itemprop="datePublished"
		>
			2013-06-26 16:21:25
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Peter, ok, thanks. Maybe the pronunciation of syllables in the language is coded at the same level as whole words? That might explain it. Then the pronunciation clues would be fed forward to the next processing level if the whole word structural pattern doesn&rsquo;t ID a specific meaningful word. But otherwise stopped.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-26 17:31:06"
			itemprop="datePublished"
		>
			2013-06-26 17:31:06
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>As a result of viewing the Word Superiority Demo <a href="http://www.psiexp.ss.uci.edu/research/teachingP140C/demos/demo_wordsuperiorityeffect.ppt">here</a>, I think I need to extend my scheme and revise the analysis I gave in the last sentence of the last paragraph of my 26 Jun 2013 — 7:39am post.</p>
<p>But I&rsquo;ll wait until readers still following this topic have had a chance to view the demo. Go through it several times, then try some introspection.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 16:30:01"
			itemprop="datePublished"
		>
			2013-06-27 16:30:01
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Best to use informative hyperlinks instead of “here.” Especially when they initiate downloads.</p>
<p>How about “download a Word Superiority demonstration (ppt, 159k).”</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 20:09:09"
			itemprop="datePublished"
		>
			2013-06-27 20:09:09
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Wow. I don&rsquo;t think I was expecting the word superiority effect to be quite so extreme. For me, it was the difference between 100% success with words and 100% failure with non-words. The non-words were all orthographically impossible, though, and I&rsquo;d love to see a demo that included pseudo-words.</p>
<p>I&rsquo;m still not sure what the word superiority effect indicates, though, in terms of how we read. It demonstrates that we proceed very quickly to word recognition, and that our brains are telling us what word we&rsquo;re looking at (if it is a word) more quickly than we consciously identify individual letters in specific <em>unannounced</em> positions. But does that actually tell us anything about <em>how</em> we recognise the word, or only that consciously identifying letters in unannounced positions is something we are not practiced at doing?</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 20:16:21"
			itemprop="datePublished"
		>
			2013-06-27 20:16:21
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Chris, I’ll bear your suggestion in mind.</p>
<p>Chris or Kevin, does the demo give an accurate impression of the paradigm in use?</p>
<p>I don’t know what exposure times and SOAs were used in the demo. I do know that in the early stages of the history of testing with the Reicher-Wheeler task, trigrams and quadrigrams were often used.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 20:59:05"
			itemprop="datePublished"
		>
			2013-06-27 20:59:05
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, I think the subjective experience supports the basic idea that Peter has been urging for a long time, and that I have tried to model a bit above. Skilled readers can recognize familiar words by the relational pattern of letter features across the whole word. We can also do orthographic look-ups, probably in some manner that the interactive activation folks have modeled. But these are not the quickest or the routine route for skilled readers.</p>
<p>We recognize the word as a whole, and then if we are asked about letters, we can draw on our memory of correct spelling. But just as we can read words we can&rsquo;t spell correctly—if you&rsquo;re like me anyway!—we routinely read words without having to process spelling at all.</p>
<p>Sorry for the delay on further posting on Kevin&rsquo;s questions. I&rsquo;ll do that soon.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 22:11:27"
			itemprop="datePublished"
		>
			2013-06-27 22:11:27
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Peter, in the demo what are the letters above and below XXXXXX for?</p>
<p>**</p>
<p>As with John, IMO the word superiority effect reveals little about the reading process, other than the fact you can’t recognize something that shouldn’t exist.</p>
<p><img src="/images/old-images/fp_06_5701.jpg" alt=""></p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 22:44:02"
			itemprop="datePublished"
		>
			2013-06-27 22:44:02
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Nick&#43;Shinn">
            Nick Shinn
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Nick, you have to run the demo in the slide show mode, clicking your way through. The XXXXXXs are a masking device that is intended to arrest the further coding of the visual information after the stimulus disappears. When the arrangement of XXXXXXs appear with a letter above and below one item in the series, you have to decide which one you saw in the stumulus word or non-word.</p>
<p>You can check up on your the accuracy of your selection by toggling back using the backward / forward / up / down keys on your keyboard.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 23:07:40"
			itemprop="datePublished"
		>
			2013-06-27 23:07:40
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Bill: <em>Skilled readers can recognize familiar words by the relational pattern of letter features across the whole word. We can also do orthographic look-ups, probably in some manner that the interactive activation folks have modeled. But these are not the quickest or the routine route for skilled readers.</em></p>
<p>It&rsquo;s that last statement that I have trouble with, not because it is unreasonable &ndash; it isn&rsquo;t &ndash;, but because we&rsquo;re still only conjecturing that feature-to-word is &lsquo;the routine route&rsquo;. I think it is highly likely, but I&rsquo;m still waiting for the compelling experimental results. The word superiority effect only shows that we&rsquo;re really good at word recognition, not how we do it.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 23:21:12"
			itemprop="datePublished"
		>
			2013-06-27 23:21:12
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Caps are so much more legible than lowercase.  I can&rsquo;t see how this improves things in any way other than aesthetically.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-27 23:38:44"
			itemprop="datePublished"
		>
			2013-06-27 23:38:44
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Deus&#43;Lux">
            Deus Lux
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I have not been following this thread to the letter (pun intended) and I cannot be certain if my computer is displaying the presentation as it was intended, most importantly the timing, but based on a quick review Reicher (1969) is not be an accurate replication, if that indeed is what you were going for.</p>
<p>That aside, I think what is potentially interesting is the possibility of <a href="http://en.wikipedia.org/wiki/Inhibition_of_return">inhibition of return (IOR)</a> confounding  the results due to the varying proximities between correct and incorrect response letters. Of course, this is more than a simple detection task, but some food for thought:</p>
<p>In your slides, the correct response is always closer to its mate than the incorrect. Thus, at a certain SOA, the correct response gets an unfair penalty as it is masked by IOR because you are forced to return your attention to a previously attend location. Most importantly, one that is closer to the correct than incorrect response.</p>
<p>I would hypothesize that making a correct response will become easier when as the distance between the correct and incorrect letters increases in the initially presented series of letters, and that this will happen regardless of the type of letters presented (random/pseudo/word). See below:</p>
<p><img src="/images/old-images/loremip_5676.jpg" alt=""></p>
<p>Therefore, in order to avoid a <em>potential</em> conflict of IOR, and in order to draw statistically valid conclusions, the distance between correct and incorrect response letters in the initially presented series of letters will need to be counterbalanced both with and between conditions of random letter-string, pseudo-word, and word.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 01:08:00"
			itemprop="datePublished"
		>
			2013-06-28 01:08:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>[Chris Dean] “<em>if that indeed is what <strong>you</strong> were going for</em>”</p>
<p>Chris, it’s not my demo. 
I found it as a result of a google search. It’s one of a series of demos on this page:
<a href="http://www.psiexp.ss.uci.edu/research/teachingP140C/">http://www.psiexp.ss.uci.edu/research/teachingP140C/</a>
<a href="http://www.psiexp.ss.uci.edu/research/">http://www.psiexp.ss.uci.edu/research/</a></p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 02:00:53"
			itemprop="datePublished"
		>
			2013-06-28 02:00:53
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">When you say “paradigm in use” what paradigm are you referring to? Your own model, or an example of a classic WSE?</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 02:04:52"
			itemprop="datePublished"
		>
			2013-06-28 02:04:52
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I was referring to the Reicher-Wheeler Paradigm as it was used by Reicher [1969], Wheeler [1970], and others testing the effect, in other words representative of the classic WSE test.</p>
<p>Adding later: I know that Reicher [1969], Wheeler [1970] and Marilyn Jager Adams [1979] used quadrigrams and Purcell and Stanovich [1982]  use trigrams, but I don&rsquo;t remember about the others. Probably with more letters the effect is more dramatic.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 02:27:56"
			itemprop="datePublished"
		>
			2013-06-28 02:27:56
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Not the same as Reicher (1969) by a long shot. And upon further reflection, my IOR hypothesis needs a bit of refinement. But that won’t happen today.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 03:23:54"
			itemprop="datePublished"
		>
			2013-06-28 03:23:54
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John: &ldquo;The word superiority effect only shows that we&rsquo;re really good at word recognition, not how we do it.&rdquo;</p>
<p>We already knew we were good at word recognition before Reicher and Wheeler; what they added was that at short times we seem to be better at reading words than letters.</p>
<p>It is the impact of the time factor and masking which shows us something important is going on in the stages of brain processing that favors words. It doesn&rsquo;t say how, but the differential impact of the time factor on word and letter recognition is a key fact that any adequate theory of reading has to explain.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 03:32:05"
			itemprop="datePublished"
		>
			2013-06-28 03:32:05
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p><em>&hellip;what they added was that at short times we seem to be better at reading words than letters.</em></p>
<p>But that still doesn&rsquo;t tell us anything about <em>how</em> we read the words. Consciously identifying individual letters in the context of strings of letters is not a task we have reason to perform on a daily basis, whereas recognising words is. So I&rsquo;m not surprised that when tested we do better at the latter.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 03:33:49"
			itemprop="datePublished"
		>
			2013-06-28 03:33:49
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>[John Hudson] “<em>I&rsquo;m still waiting for the compelling experimental results<em>”</p>
<p>That’s Ferlinghetti-ish of you!</p>
<p>I think this will have to come from two places: crowding tests and ideal observer analysis or neural network modeling.</p>
<p>I think in familiar words there is a release from crowding because the relational filtering is done at a more elemental level, the level of role units, and hence requires smaller isolation fields than if relational filtering using explicitly labeled letters is required. Explicit labeling at the letter level requires larger isolation fields, because isolation fields are in direct proportion to the size of the item that must be detected and combined or integrated. So if the task mast be done with explicit labeling at the letter level, benchmark-level performance might require larger isolation fields. Tests will have to be done in foveal vision with fixation time constraints imposed by masking, as in the 2013 tests reported at the 2013 annual meeting I mentioned above.</p>
<p>Ideal observer analysis using multiple hidden layers, real receptive field constraints and documented isolation field constraints should be able to gauge the performance of a string kernel model using role-units versus that of a string kernel or other model using letters. If there is a good fit of the one or the other model&rsquo;s performance to behavioural results we might become biased toward the better fitting model. To find out more about Ideal Observer Analysis, consult: <a href="http://en.wikipedia.org/wiki/Ideal_observer_analysis">http://en.wikipedia.org/wiki/Ideal_observer_analysis</a></p>
<p>There is also new “circumstantial evidence” by members of the “bubbles” research team [Xavier Morin-Duchesne1, Daniel FIset, Martin Arguin and Frédéric Gosselin], that suggests [quoting their abstract] “that the visual system bypasses single letter identification on the road to word recognition.” See the abstract of their 2012 poster presentation here: <a href="http://ww.w.journalofvision.org/content/12/9/532.short">http://ww.w.journalofvision.org/content/12/9/532.short</a>. There was a followup poster presentation to this at the 2013 Vision Sciences Society Annual Meeting last month. See the abstract at this download link: <a href="http://www.visionsciences.org/programs/VSS_2013_Abstracts.pdf">http://www.visionsciences.org/programs/VSS_2013_Abstracts.pdf</a> [2.4MB], page 309.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 03:37:50"
			itemprop="datePublished"
		>
			2013-06-28 03:37:50
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John: &ldquo;Consciously identifying individual letters in the context of strings of letters is not a task we have reason to perform on a daily basis.&rdquo;</p>
<p>Ah, but according to the current received view it is something we perform constantly, because identifying individual letters first is a necessity to reading words. The apparent refutation of this view by the Reicher and Wheeler tests is what interactive activation theories attempt to account for, and remove the contradiction.</p>
<p>You have to look at the state of the debate to see why the Word Superiority Test became such a key piece of data: it is an apparent refutation.</p>
<p>Let also here address Kevin&rsquo;s very interesting questions. My model leads me to suspect that there will not be a &lsquo;familiar kernal superiority&rsquo; effect at very short SOAs, but that there <em>may</em> be at longer SOAs. At short SOAs, only the whole word will get into working memory, and I think that accounts for the normal WSE. At longer SOAs you can even get a WSE out of mixed case, and there I think interactive activation is indeed at work—it is just a later, more time consuming process, which is only started if whole word ID fails.</p>
<p>If Peter is right about the string kernels operating early on, I still am not sure whether the flanking role units are chopped off when the whole word relational structure is completely coded and assembled. If these flanking role units are filtered out at my final stage, then they won&rsquo;t appear in working memory and later processing. If they aren&rsquo;t filtered out, then they may appear. So the implication of my model is: no &lsquo;kernel superiority effect&rsquo; at short SOAs, possibly at longer ones.</p>
<p>I would add that generally, as you see Peter looks to crowding effects for a critical test of his theory. I suspect that the time factor may also prove crucial. If the time for the brain to cycle messages repeatedly, as required by interactive activation, is too long, that would refute that as an explanation for the normal WSE, and would be a strong argument for some theory like Peter&rsquo;s. With new imaging techniques, it should be possible to get enough information to make such calculations, I think. In addition there are new experimental techniques which can detect what is processed when, and these may prove revealing also.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 04:00:19"
			itemprop="datePublished"
		>
			2013-06-28 04:00:19
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Bill, note that I wrote &lsquo;<em>consciously</em> identifying individual letters&rsquo;, which is what the test methodology that reveals the word superiority effect requires. When we are reading, we&rsquo;re not consciously performing any recognition operations &ndash; either of letter or of words &ndash;, which is precisely why we find it so difficult to determine how we read.</p>
<p><em>You have to look at the state of the debate to see why the Word Superiority Test became such a key piece of data: it is an apparent refutation.</em></p>
<p>This is what I am saying, though: it doesn&rsquo;t appear to me as a refutation, because the task that the test uses only shows that we recognise words more quickly than we consciously identify individual letters in particular positions in strings. That doesn&rsquo;t refute any particular model of how we recognise the words, because none of those models involve us <em>consciously</em> identifying individual letters in particular positions such that we could name the letter.</p>
<p>What the test shows is that we can recognise words in a very, very short period of time, and having identified the words we can then figure out what the letters were. And when we&rsquo;re shown non-words we can&rsquo;t do either. But, according to Peter&rsquo;s reporting, when we&rsquo;re shown orthographically permissible pseudo-words, there is also a superiority effect, albeit not as great as for real words. That suggests to me that part of what enables us to recognise strings of letters and <em>hold them in our minds</em> &ndash; such that we can figure out what letters are involved&ndash; is a very rapid orthographic processing.</p>
<p>Now, it seems to me that your explanation for the pseudo-word superiority effect must be something like this: &lsquo;We perceive relational pattern of letter features across a whole word, and from that we recognise the word, except in this case it isn&rsquo;t a word so we can&rsquo;t, so we instead perform orthographic coding and guess at the letters&rsquo;. That seems a really long process to go through, involving a failure of a primary word recognition mechanism &ndash; your &lsquo;routine route&rsquo; &ndash; before we even get to the mechanism that enables us to complete the task. It seems to me more likely that the orthographic processing kicks in right away, in which case the superiority effect for real words over pseudo-words is simply one of familiarity, i.e. a matter of vocabulary.</p>
<p>And I think you shouldn&rsquo;t be too quick to discount familiarity, because if we were as good at recognising relational patterns of letter features across a whole word as you say, then it wouldn&rsquo;t much matter whether the words were real, pseudo or non in terms of our processing. It is precisely because the real words are familiar that they have an advantage: we recognise them as something that we <em>know,</em> and that means we know lots of things about them (what they mean, how they are spelled, what letters occur in which positions in the word), and we can hold them in our heads because they&rsquo;re already there. That means we don&rsquo;t need to be trying to recall the visual image of the thing we just saw for a split second while trying to carry out the conscious task of naming a particular letter in a particular position. In the case of a pseudo-word, the advantage also seems to me one of familiarity: the string exhibits patterns that we know, and that we are able to conventionally map to phonetic sequences, and that too means that we can handle the string as something other than a visual memory. In the case of a non-word, all we have is the visual memory of something that we didn&rsquo;t know and that didn&rsquo;t correspond to any known patterns and that we saw for a split second, so it&rsquo;s no surprise that there is a Non-Word Inferiority Effect.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 05:28:44"
			itemprop="datePublished"
		>
			2013-06-28 05:28:44
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>I reworked parts of the ppt file. 
I made pronounceable pseudowords, words in lowercase, and nonsense trigrams.
I can&rsquo;t figure out where the timings are set, but we can probably adjust them too.</p>
<p>Download the new ppt file from here:
<a href="http://www.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt">www.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt</a>
File size is 137 KB</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 13:01:12"
			itemprop="datePublished"
		>
			2013-06-28 13:01:12
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, I think it&rsquo;s not a long process, because &lsquo;quantizing&rsquo; or isolating letter features, and then having a process to code how they are connected must take place anyway to identify separate letters. We don&rsquo;t know it&rsquo;s a separate letter until analysis separates it out from the word as a whole—and the whole word is more easily separated, earlier, because of word spacing.</p>
<p>Peter&rsquo;s point that the letters are normally too close for efficient individual letter recognition—there are crowding effects—is germane here. It would be interesting to see if wide tracking would help some readers with dyslexia, for example. It may be that they have problems creating the filters to decode closely packed letters.</p>
<p>So, the same massively parallel process that enables us to identify letters enables us to identify the whole word pattern <em>at the same point in processing</em>. That&rsquo;s why it&rsquo;s more efficient to ID the whole word structural pattern if we have it recorded in our memory and easily accessible. Decoding the letters as abstract entities would be a next step beyond the assembly of the structural code of all the letters in the word, but the shortest next step to a word id can be an immediate connection to meaning and sound. For we have—if Peter&rsquo;s theory is right—a whole word structural code in our memory banks, to check for matches. That&rsquo;s actually the key question: do we have a memory record of a code of the whole word relational structural pattern? If we do have that memory, a straight path to it is quickest.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 13:57:29"
			itemprop="datePublished"
		>
			2013-06-28 13:57:29
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>William,</p>
<p>I would suggest that the whole word determined the relational structure pattern. Some months back, I was on my patio, observing the suburban wildlife when, immediately, they all disappeared, birds and squirrels alike. I asked myself, “What’s up with that?” And, then, I looked up and saw a hawk in the sky. Apparently, both the birds and squirrels had an image of a hawk as a clear and present danger hard-wired into their brains. There was no time for reflection on the meaning of that outline.</p>
<p>Given the speed and fluidity at which both conversation and reading flow, it is sensible to assume that whole words get hard-wired into our white matter, as we grow and learn, and as our brain grows, which it normally does until approximately age thirty.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 15:08:41"
			itemprop="datePublished"
		>
			2013-06-28 15:08:41
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:oldnick">
            oldnick
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>[John Hudson to William Berkson] “<em>Now, it seems to me that your explanation for the pseudo-word superiority effect must be something like this: &lsquo;We perceive relational pattern of letter features across a whole word, and from that we recognise the word, except in this case it isn&rsquo;t a word so we can&rsquo;t, so we instead perform orthographic coding and guess at the letters&rsquo;. That seems a really long process to go through, involving a failure of a primary word recognition mechanism &ndash; your &lsquo;routine route&rsquo; &ndash; before we even get to the mechanism that enables us to complete the task.</em>”</p>
<p>Below is a link to a pdf which shows the communication between what I’ve been calling the “role-unit level string kernel deck” and the “intrinsic integration interface.” It is based on my knowledge of neural physiology, specifically how neural activity is governed by the handling of “action potentials” at established synaptic links between the branching components of “axons” and the branching components of “dendritic arbours.” Axons are the outgoing or broadcasting parts of neurons, and dendrite trees are at the receiving and amplification end.</p>
<p>In the pdf I compare activity in both decks for a canonical word and for a word with a single set of transposed letters. You can draw your own conclusions.</p>
<p>Here is a link to the pdf:
<a href="http://www.enneson.com/public_downloads/typophile/two_decks_pe_for_typophilers.pdf">www.enneson.com/public_downloads/typophile/two_decks_pe_for_typophilers.pdf</a>
641.3 KB
The pdf has three pages.</p>
<p>Earlier I wrote that there would be a lot of string-kernel overlap in pseudowords. I think if your were to plot this out you would see that this is the case. The problem is, there won’t be just one single full-word unit in the intrinsic integration interface which gets fully or partially activated. A single full-word unit is one that has a meaning and a pronunciation vector over it. But this is the same in the Interactive Activation account. Pseudowords don’t fully activate any one item in the orthographic lexicon, which constitutes the word level in the Interactive Activation scheme. Pseudowords just activate some more than others. One question then, in a string kernel scheme like mine is, does the visual system use established intrinsic integration codes for syllables or bigrams, if such exist? Syllables or bigrams are structures with just pronunciation vectors over them, not meaning vectors.</p>
<p>I agree that in the pseudowords a fast phonetic disambiguation comes into play and this is what we explicitly hold in working memory. But the Interactive Activation account in its most primitive form says there is feedback to the letter level and what we hold in implicit working memory is a sequence of strongly activated letter codes, not the explicit memory of the word, it’s pronunciation, and a latent knowledge of the spelling. So there is a disconnect here, with what we see through introspection.</p>
<p>It’s possible that already at my string kernel level a fast phonetic disambiguation starts to happen, especially in phonetically complex orthographies. This is because the string kernels contain flanking elements outside the nucleus which can conceivably indicate which phonetic variant applies. But I haven’t worked this out.</p>
<p>More likely intrinsic integration codes for bigrams, syllables and morphemes are used and strung together during the stimulus presentation phase. This might mean that with less time there would be a decreased superiority effect for longer and more difficult pseudowords.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 20:05:01"
			itemprop="datePublished"
		>
			2013-06-28 20:05:01
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Peter, with the addition of the ideas in your last couple of paragraphs, I think we&rsquo;re pretty much on the same page, although I can&rsquo;t claim to thoroughly understand the string kernel concept.</p>
<blockquote>I agree that in the pseudowords a fast phonetic disambiguation comes into play and this is what we explicitly hold in working memory.</blockquote>
<p>But if you&rsquo;re saying that this only comes into play in for the pseudowords, that means that we&rsquo;re first determining whether the word is a real word or a pseudoword (or, perhaps better terms, a known word vs an unknown word), and only then applying a fast phonetic disambiguation. This seems to me unlikely, especially since phonetic disambiguation will likely have been part of how we learn to read. My guess is that it is always happening while we&rsquo;re reading, but is augmented as we become experienced readers by pattern recognition processes that help us resolve known words even faster.</p>
<blockquote>But the Interactive Activation account in its most primitive form says there is feedback to the letter level and what we hold in implicit working memory is a sequence of strongly activated letter codes, not the explicit memory of the word, it’s pronunciation, and a latent knowledge of the spelling.</blockquote>
<p>This seems to be the nub of things, and for what I don&rsquo;t think I&rsquo;ve seen experimental data to sway me one way or the other.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 20:26:36"
			itemprop="datePublished"
		>
			2013-06-28 20:26:36
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Imagine two people being set a series of tasks to complete, each using a different method. Whichever finishes a task first, they both move on to the next task. The boss doesn&rsquo;t care which one completes the task first, only that the tasks are completed. Sometimes, the tasks favour one method over the other, and sometimes vice versa, but both people always set out to complete each task, because this takes less time than analysing each task first and trying to figure out which method will be faster.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 20:33:26"
			itemprop="datePublished"
		>
			2013-06-28 20:33:26
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">It seems to me that the whole word feature pattern retrieval method would be favoured for more common and shorter words, while the orthographic phonetic disambiguation method would be favoured for less common and longer words. But this means that there will be words of middling frequency and familiarity, and of middling length, for which it would be difficult to predict which method would be fastest and most accurate. Parallel processing of both methods would seem to me to be faster than iterative processing of first one and then the other.</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 20:39:53"
			itemprop="datePublished"
		>
			2013-06-28 20:39:53
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:John&#43;Hudson">
            John Hudson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>John, in a known word the link to a pronunciation is immediate with the matrix-internal resonance inside the intrinsic integration interface. At least, that’s the hypothesis. Think of this as an intrinsic integration neuron linking directly with an unambiguous articulatory action script in another area of the brain. In the demo, after a few “training” session, I found myself adopting the strategy of going directly to a pronunciation from the initial visual recogniton, and holding that in working memory. I found myself doing this for both words and pseudo-words, and I saw that I was able to do it successfully with pseudowords.</p>
<p>From testing myself on the pseudowords in the revised demo, I suspect that the link to a pronunciation is slightly less immediate, especially in complex pseudowords, but I have a hunch the link to a pronunciation in pseudowords leverages the same intrinsic-integration system because the pseudowords present themselves to me perceptually as internally cohesive word-like things. Even with the short exposure time used in the demo. But if it is indeed less immediate, the link to an articulatory action script would probably be less successful with less exposure time at the front end of the test.</p>
<p>Like you I suspect there is some background process of addressing the articulatory system already at an early stage of processing. As I intimated before, I think this can take hold as early as the string kernel coding stage. It might also be that this works along with the partial but significant activation in the intrinsic integration interface to do or a introduce convolution to an existing articulatory script at the position or positions where the letter that makes the word a psuedoword appears. Or it could be that some interfacilitation  or pooling of activity between competing partly activated units within the intrinsic integration interface occurs.</p>
<p>That pseudowords present themselves to me perceptually at a very short exposure time as ordered, internally cohesive word-like things indicates to me that the intrinsic integration interface area <em>is</em> activated. Remember that for me the intrinsic-integration interface hypothesis accounts for the fact that we see words as unitary, ordered, pronounceable, sense-invested object-like things with an internal cohesion typical of true gestalts. The consonant strings look cluttered, chaotic and crowded to me. In my opinion an orthographic coding hypothesis in which the constituent items are extrinsically and associatively linked in a distributed network, but not intrinsically integrated at a neural code level can’t account for that. I know introspection and attention to the ordinary everyday lived experience of a thing are considered unreliable, but they have to be a factor in hypothesis building and model selection in the absence of decisive tests. It is well-known that pseudowords produce activity in the visual word-form area almost as much as words do. Consonant strings don’t.</p>
<p>I&rsquo;m not a hugh fan of horse-race models.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 22:20:12"
			itemprop="datePublished"
		>
			2013-06-28 22:20:12
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Peter, when you test yourself, what are your dependent variables, and how are you analyzing your data? Is there some way you are removing yourself from the <a href="https://en.wikipedia.org/wiki/Experimenter's_bias">experimenter’s bias</a> that I am not aware of?</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-28 23:29:00"
			itemprop="datePublished"
		>
			2013-06-28 23:29:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:Chris&#43;Dean">
            Chris Dean
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Chris, I don&rsquo;t pretend I am carrying out a valid test. I found going through the demo a valuable exercise and encountered things I didn’t expect, coming from an extended period of deep immersion in the literature on the effect.</p>
<p>If I had a bias, it was toward thinking that perceptual processing was being stopped before a conscious identification of the word, and toward assuming that I wouldn‘t try to develop a strategy or strategies that optimized my chances of making the correct post-mask decision, least of all, ones that would involve holding an auditory image of the word in my head.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-29 02:06:17"
			itemprop="datePublished"
		>
			2013-06-29 02:06:17
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>&ldquo;Race horse models.&rdquo; I&rsquo;ve been pushing this idea, and Peter has always been reluctant to embrace it. I still think that a lot of different processes go on simultaneously, and when one &lsquo;wins&rsquo; by identifying a word, then the process stops, and the eye jumps to another set of words.</p>
<p>When it&rsquo;s a familiar word, and the type and physical conditions are readable, I think the whole word structural pattern wins.</p>
<p>I do think that the coding of all the structural features of the letters in word happens simultaneously, and once coded they are then both separated into whole words and individuated as letters. If the whole word pattern matches a remembered code &lsquo;resonance&rsquo; with that memory stored in a neuron or many neurons wins the race. If not the individuated letters are used for phonemes and orthography. But that is a step beyond, another &lsquo;deck&rsquo; in the process.</p>
<p>If I&rsquo;m right any pseudoword superiority effect should take longer SOA than the word superiority effect, at least to reach the same level of accuracy. I don&rsquo;t know if there are data on this.</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-29 04:16:58"
			itemprop="datePublished"
		>
			2013-06-29 04:16:58
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:William&#43;Berkson">
            William Berkson
	    </a>
			</span></span>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><p>Apparently the timing in the word superiority effects demos I provided above are set in the &ldquo;transitions&rdquo; panel in PowerPoint 2010. The version I have is 2008. If there is a reader of this post that has 2010, can he or she check the settings?</p>
<p>Download a version of the ppt file from here:
<a href="http://www.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt">http://www.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt</a>
File size is 137 KB</p>
</div>
		
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2013-06-30 13:26:16"
			itemprop="datePublished"
		>
			2013-06-30 13:26:16
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">

			<span class="p-author h-card" itemprop="name">
            <a href="/?q=author:enne_son">
            enne_son
	    </a>
			</span></span>
	</div>
	
</div>
 
  <p class="text-end"><a class="text-secondary" href="https://github.com/typophile/typophile.github.io/edit/main/content/node/103822.md">Suggest an edit</a></p>

</article>

    </main>
    
  </body>
</html>
