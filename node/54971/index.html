<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Letter combination frequencies | Typophile Archive</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.91.2" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>

    <meta property="og:title" content="Letter combination frequencies" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://typophile.github.io/node/54971/" /><meta property="article:section" content="node" />
<meta property="article:published_time" content="2009-02-18T22:13:00+00:00" />
<meta property="article:modified_time" content="2009-02-18T22:13:00+00:00" />

<meta itemprop="name" content="Letter combination frequencies">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2009-02-18T22:13:00+00:00" />
<meta itemprop="dateModified" content="2009-02-18T22:13:00+00:00" />
<meta itemprop="wordCount" content="0">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Letter combination frequencies"/>
<meta name="twitter:description" content=""/>

  </head>

  <body class="ma0 avenir bg-near-white">

      <header>
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container">
      <a class="navbar-brand" href="/">Typophile Archive</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
    </div>
  </nav>
</header>
 
    <main class="pb7" role="main">
      
<article class="container">
  <header class="post-header py-3">
    <h1 class="post-title p-name" itemprop="name headline">Letter combination frequencies</h1>
    <p class="text-muted">
              
      <time class="f6 mv4 dib tracked" datetime="2009-02-18T22:13:00Z">February 18, 2009</time>
      
        
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">nina</span></span>
      

    </p>
  </header>
  <div class="post-content e-content" itemprop="articleBody">How would one go about finding out about relative frequencies of two-letter combinations in different languages* – ideally even discerning between uppercase and lowercase letters? I'm essentially looking for something like a list that would say, an "r" is (I'm making this up for the sake of illustration) 13% likely to be followed by an "e" in English, but only 2% likely to be followed by a "k". 
I'm aware there will not be a Final Verdict on this to be found, but there must be some approximative data to work with?
* I'm most interested in German and English for the moment.

A quick-ish web search has brought up charts with relative <em>single letter</em> frequencies in different languages, as well as <em>word</em> frequency lists, but not a lot about pairs of letters. Wikipedia has a list of <a href="http://en.wikipedia.org/wiki/Bigram">Bigram Frequencies in the English language</a>, but this is single-case – and also only lists the <em>most common</em> combinations, while I'm looking for something more in the vein of a "pairing" frequency table.

Any good, reasonably reliable references/literature you guys can recommend? 
I'm done looking at wonky charts that don't say what data they're based on…
Of course, please re-direct me if this has been discussed before. I couldn't find anything, but then the search has been hiccuping.</div>

   <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Here's another one counting single letters: http://www.typotheque.com/type_utilities/lettermeter
Not sure about pairs though, but someone who knows their way around Python should be able to modify this in a snap.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-18 22:19:01"
			itemprop="datePublished"
		>
			2009-02-18 22:19:01
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Frode Bo Helland</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Actually, judging from the illustration it appears to count pairs as well. Not sure.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-18 22:19:54"
			itemprop="datePublished"
		>
			2009-02-18 22:19:54
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Frode Bo Helland</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Frode, thanks. I've found a number of such applets written in different programming languages, but they're always based on a pretty narrow scope of the text you feed into them. I'd be more interested in some scientific analyses based on, I don't know, the quantified vocabulary of a given language?

BTW, it just occurred to me that since this sort of research would likely be undertaken by linguists and/or cryptographers, there might not be much data that distinguishes between UC and lc?</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-18 22:23:07"
			itemprop="datePublished"
		>
			2009-02-18 22:23:07
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>nina</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">> I’d be more interested in some scientific analyses based on, I don’t know, the quantified vocabulary of a given language?

The obvious idea is to feed a vocabulary into one of those "applets" you saw. Though you probably want to feed a <em>book</em> rather than a <em>vocabulary</em>. That's because latter would have exactly one entry for "the", but former will have it aplenty thus severely affecting the counts for "th" and "he" ... so it really depends on what exactly you want to measure.

A program that builds a histogram of letter combination probabilities (even if it's case-aware) is really a 5 minutes of work for any decent programmer in virtually any language. Find one, feed one cold beer, collect one "lettermeter" program.
</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-18 23:02:09"
			itemprop="datePublished"
		>
			2009-02-18 23:02:09
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>apankrat</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><em>"Though you probably want to feed a book rather than a vocabulary."</em>

Ah, I wasn't talking about dictionaries, but about the collective, uh, "vocabulary" in approximately the sense that Wikipedia says "A person's vocabulary is the set of words they are familiar with in a language" – only I meant it in a non-subjective way, so a better word would likely be <a href="http://en.wikipedia.org/wiki/Lexis_(linguistics)">lexis</a> ("the total bank of words and phrases of a particular language, the artifact of which is known as a lexicon"). Sorry about the confusing wording.
Note I did say "quantified", to account for frequencies of words.

I suspect that defining and extracting these masses of data is trickier than analysing it, and must have been done before by researchers who knew what they were doing. </div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-18 23:19:33"
			itemprop="datePublished"
		>
			2009-02-18 23:19:33
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>nina</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">From a programmers perspective, it's not that difficult as long as all the texts that are being supplied have the same encoding and as long as such things as line-breaks can be ignored. Remember that ck becomes k-k in German, so just discarding the line-break will not work. On the other hand, you might want to know how often a certain letter is followed or preceded by a linebreak, i.e. a hyphen. Different spaces are also rather difficult if you are interested in characteres at the beginning and the end of words. A rough estimate, however, should be not to difficult to obtain in C or even TeX. As for the sources, I'd try to find large chunks of text on the internet. (The quantified lexis of books might be more important than the overall quantified lexis of the language, as I assume the purpose is related to typography.) Ghostview can also extract plain text from PDFs. -- cs</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-19 01:05:19"
			itemprop="datePublished"
		>
			2009-02-19 01:05:19
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>cschroeppel</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">By "quantified vocabulary" (which obviously is impossible to define exactly – apart from being worded awkwardly) I guess I meant a sampling large and broad enough that the results gleaned from evaluating it would be substantial enough to work from on a somewhat general level. That means the data basis must be *much* broader than a single text you can fill into those convenient boxes in typical online tools… plus selected/compiled in a non-arbitrary way.

The Brown corpus appears to provide such a basis to work from: http://en.wikipedia.org/wiki/Brown_Corpus

See the related discussion on this thread:
http://typophile.com/node/54942</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-23 21:36:41"
			itemprop="datePublished"
		>
			2009-02-23 21:36:41
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>nina</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Large amounts of text can be found in the Gutenburg project, free for use.

I'm assuming that you are using this to single out kerning pairs? You might want to do a search for "most common kerning pairs" as I know I've seen something like that somewhere. </div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-23 22:27:48"
			itemprop="datePublished"
		>
			2009-02-23 22:27:48
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Dan Gayle</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Oh, right. But I think it's not the amount of text only; it's the way it's selected and collated. I guess I've spent too much time studying that collating "something, just a lot of text" myself would not feel like a hack job (and not enough time to actually do it myself, satisfactorily). Maybe I put too much trust in research, but I do feel this needs a basis that's thought through in a way.
Like, what's in the Gutenberg project? It won't have daily newspapers. Words used are highly context specific, which is one reason why I think the Brown corpus is (or was?) doing a pretty good job of collating different language usage contexts.

<em>"I’m assuming that you are using this to single out kerning pairs?"</em>
Um, not yet. It was something that crossed my mind when I started thinking about spacing more seriously, which is also premature for me, but highly interesting. (Like wanting to know how often on average an "r" would be followed by a "round glyph" vs. a straight.)</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-23 22:58:09"
			itemprop="datePublished"
		>
			2009-02-23 22:58:09
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>nina</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><cite> > (Like wanting to know how often on average an “r” would be followed by a “round glyph” vs. a straight.)</cite>

Your dedication is admirable. But I have to ask: Would it really matter how often an "r" is followed by round vs. straight? Won't you want both combinations to look acceptable and well-spaced?

There was a time when disk space and processing time were at more of a premium and there was a point of diminishing returns for adding kerning pairs for unlikely (or practically non-existent) combinations. But things have changed quite a bit in the past several years. Also, with class kerning capabilities in OpenType, these frequency issues are much less important considerations, it seems to me.

-- Kent.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-24 13:41:59"
			itemprop="datePublished"
		>
			2009-02-24 13:41:59
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>kentlew</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">For a more extensive and up-to-date word list than the Brown Corpus, take a look at the Corpus of American English. Here's a description from [[http://en.wikipedia.org/wiki/Corpus_of_American_English|Wikipedia]]:

<em>The freely-searchable 385+ million word Corpus of Contemporary American English (COCA) is the largest corpus of American English currently available, and the only publicly-available corpus of American English to contain a wide array of texts from a number of genres....

The corpus is composed of more than 385 million words in more than 150,000 texts, including 20 million words each year from 1990-2008. For each year (and therefore overall, as well), the corpus is evenly divided between the five genres of spoken, fiction, popular magazines, newspapers, and academic journals. The texts come from a variety of sources:

Spoken: (79 million words) Transcripts of unscripted conversation from nearly 150 different TV and radio programs. 

Fiction: (75 million words) Short stories and plays from literary magazines, children’s magazines, popular magazines, first chapters of first edition books 1990-present, and movie scripts. 

Popular Magazines: (81 million words) Nearly 100 different magazines, with a good mix (overall, and by year) between specific domains (news, health, home and gardening, women, financial, religion, sports, etc). 

Newspapers: (76 million words) Ten newspapers from across the US, with a good mix between different sections of the newspapers, such as local news, opinion, sports, financial, etc. 

Academic Journals: (76 million words) Nearly 100 different peer-reviewed journals. These were selected to cover the entire range of the Library of Congress classification system.</em>

For your purposes you probably wouldn't want the spoken words, but they can easily be filitered out.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~~~~~~~~~~~~~~~~~
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>When going from </em>A to Z,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>I often end up </em>At Oz.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-24 21:07:07"
			itemprop="datePublished"
		>
			2009-02-24 21:07:07
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>AtoZ</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Kent,
<em>"Your dedication is admirable. But I have to ask: Would it really matter how often an “r” is followed by round vs. straight? Won’t you want both combinations to look acceptable and well-spaced?"</em>
Indeed I'm probably off on kind of a longish-term learning tangent here, rather than aiming for an actual, quick, tangible result that I'm then going to put to use in my font. 
This curiosity about the frequency of letter combinations (especially regarding the "r" example mentioned) also wasn't triggered by thinking about kerning, but rather the lettershapes themselves. In different fonts, the "r" – not due to spacing/kerning, but judging from the shape of the arm/beak – seems to go especially well with subsequent round shapes; or it has particular problems with an "a" following it; or it may look too close to an "m" when followed by an "n", etc.; that made me wonder which combinations would be most important to consider in an "r" (& I'm assuming there must be similar considerations considering other glyphs as well).
Of course I agree the goal has to be to make it look decent at the very least next to as close to "anything" as possible – but I'm wondering if there is one especially frequent pattern of combination that would take precedence due to its frequency.

AtoZ: Thank you! That's awesome. I was wondering if the Brown corpus isn't somewhat out of date by now.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-02-24 23:32:55"
			itemprop="datePublished"
		>
			2009-02-24 23:32:55
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>nina</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I know it's been a few years, but if you're still looking for good resources on this, try: 

http://urtd.net/projects/cod/

Someone suggested this to me in another thread. Not only does it include base pairs, but it also includes accented pairs.

Tim</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2014-09-08 14:19:14"
			itemprop="datePublished"
		>
			2014-09-08 14:19:14
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>timotheus</span
			></span
		>
	</div>
	
</div>
 
</article>

    </main>
    
  </body>
</html>
