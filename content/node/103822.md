---
author:
  name: Chris Dean
  picture: 111971
body: "<a href=\"http://www.cnn.com/2013/06/13/us/navy-all-caps/index.html?hpt=hp_t2\">United
  States navy ditches ALL CAPS message format</a>. (From CNN, 2013, 06, 13)\r\n\r\nScore
  one for <a href=\"http://en.wikipedia.org/wiki/Bouma\">Bouma</a>."
comments:
- author:
    name: oldnick
    picture: 109434
  body: "So\u2026internet \u201Cetiquette\u201D trumps over a century of tradition.
    Marshall McLuhan was right\u2026"
  created: '2013-06-13 13:13:15'
- author:
    name: Theunis de Jong
    picture: 114717
  body: "\"we have torpedos ready stop surrender your vessel stop so please stop stop
    lol stop\"\r\n\r\nBut does morse code does not support lowercase. How are they
    going to do that thing with the flashing lights, then? Use Unicode?\r\n\r\n\"All
    your (pizza emoji) are belong to us stop\""
  created: '2013-06-13 13:30:00'
- author:
    name: Chris Dean
    picture: 111971
  body: "\u201C<em>In it, the Navy said it is ditching its in-house Defense Message
    System in favor of e-mail. One with a very apt acronym: NICE (Navy Interface for
    Command Email)</em>.\u201D\r\n\r\nBeautiful irony. Capital letters for an acronym
    to describe a system which no longer uses ALL CAPS settings."
  created: '2013-06-13 14:09:00'
- author:
    name: russellm
    picture: 111614
  body: "<cite>... a century of tradition. </cite>\r\n\r\nFirst it was the rum rations.
    Then it was Morse Code... Now this."
  created: '2013-06-13 15:53:58'
- author:
    name: JamesM
  body: My uncle died in WWII when his ship was torpedoed and sank, so anything to
    improve naval communications is fine with me. But with that said, given that orders
    are usually read very carefully, I'm not sure how big a difference this will make,
    but hopefully it'll help.
  created: '2013-06-13 17:02:01'
- author:
    name: Thomas Phinney
    picture: 128358
  body: "Chris, do you actually believe in this bouma nonsense? There is an awful
    lot of research in this area. The reasons that lowercase is more legible than
    uppercase have nothing to do with word shape and everything to do with making
    each letter more distinctive from others.\r\n\r\nThat's not to say there is no
    interplay between letters, or that we don't need to recognize words. But we don't
    do it by the shape of the word as a whole."
  created: '2013-06-13 20:38:34'
- author:
    name: Chris Dean
    picture: 111971
  body: Casual joke.
  created: '2013-06-13 21:49:04'
- author:
    name: hrant
    picture: 110403
  body: "A bouma isn't always a whole word; sometimes it's a single letter (and maybe
    even less often than I suspect).\r\n\r\nThe \"nonsense\" is thinking that the
    brain would simply ignore the information-rich clustering of letters (and often
    their silhouettes).\r\n\r\nhhp\r\n"
  created: '2013-06-13 22:19:46'
- author:
    name: quadibloc
    picture: 118515
  body: "I was amused that the article noted that back in the 1850s, teletypewriters
    with upper-case only character sets and three-bank keyboards were in use by the
    Navy.\r\n\r\nIn fact, 5-level code was still very much alive for TELEX messages
    even in the 1960s (and, in fact, I've heard that it isn't quite dead even yet)."
  created: '2013-06-14 02:37:51'
- author:
    name: oldnick
    picture: 109434
  body: "John,\r\n\r\nDitto for Weather Service and AP wires in the 1980s, sent to
    television stations..."
  created: '2013-06-14 13:44:20'
- author:
    name: russellm
    picture: 111614
  body: 'our work purchasing system works in all caps.  I write my requisitions like
    this: "I would like to purchase 100 stop signs please. Thank you very much and
    have a good day." and they come out as, :" HEY YOU! I NEED 100 STOP SIGNS! NOW!!!'
  created: '2013-06-14 14:50:24'
- author:
    name: enne_son
    picture: 109487
  body: "[Thomas] <em>\u2026 do you actually believe in this bouma nonsense?</em>\r\n\r\nThomas,
    for the record, the term Bouma Shape was introduced by Insup Taylor and M. Martin
    Taylor in their 1983 book <em>The Psychology of Reading</em>. From the information
    below, you will see that the construct includes a whole lot more than \u201Cthe
    shape of the word as a whole.\u201D The term is a result of Taylor and Taylor\u2019s
    efforts to factor \u201Cinterior features\u201D into the \u201Cshape definition\u201D
    of words.\r\n\r\nTaylor & Taylor list \u201Cthe seven groups of mutually confusable
    lowercase letters found by Bouma (1971)\u201D. They mean the S1, S2, S3, S4, A1,
    A2 & D grouping of lowercase letters Bouma introduced in \u201CVisual Recognition
    of Isolated Lower-case Letters\u201D, which they number from 1 through 7, as follows
    [the descriptions are Bouma\u2019s; the groupings are a result of confusion frequencies
    culled from recognition tests]:\r\n1 = a s z x : [HB\u2019s S1] inner parts and
    rectangular envelope\r\n2 = e o c : [HB\u2019s S2] almost round envelope\r\n3
    = r v w : [HB\u2019s S4] oblique outer parts\r\n4 = n m u : [HB\u2019s S3] rectangular
    envelope with well-expressed vertical outer parts\r\n5 = d h k b : [HB\u2019s
    A1] ascending extensions protruding from a well-expressed body\r\n6 = t i l f
    : [HB\u2019s A2] slenderness\r\n7 = g j p q y : [HB\u2019s D] no description\r\n1,
    2, 3, & 4 are categorized by Bouma and the Taylors as short (S); 5 & 6 as tall
    (A); 7 as projecting (D)\r\n\r\nTaylor and Taylor: \u201CThe \u201CBouma shape\u201D
    of a word can be defined by listing the group numbers of its letters: \u201Cat\u201D
    has the shape 16 (short-filled, tall-thin), and \u201Cdog\u201D is 527 (tall-fat,
    short-round, projecting).\r\n\r\nTaylor and Taylor found that by their Bouma Shapes
    \u2014 defined in this way \u2014 something like 87% of the words in the English
    language corpus they used unique. If I'm not mistaken, Taylor and Taylor used
    this to elucidate the contribution made to reading by parafoveal pre-processing
    and give an account of skipping.\r\n"
  created: '2013-06-16 14:00:33'
- author:
    name: hrant
    picture: 110403
  body: "The term \"Bouma shape\" has also been used by P. Saenger. I just simplified
    it (being a big believer in two-syllable labels).\r\n\r\nBTW, I would contend
    that:\r\n- It's not about bouma uniqueness, but the degrees of bouma confusability.\r\n-
    The silhouettes are much more significant than the interiors, so that grouping
    is over-simple; the 1/2/3/4 and 5/6 groupings need to be on a joined and on a
    lower level.\r\n- It's all processing; no pre-processing.\r\n- I think \"skipping\"
    might be misleading, since no content is skipped (in \"real\" reading).\r\n\r\nhhp\r\n"
  created: '2013-06-16 15:02:35'
- author:
    name: enne_son
    picture: 109487
  body: "The term Bouma shape as used by Paul Saenger in <em>Space Between Words:
    The Origins of Silent Reading</em> is clearly taken from Taylor and Taylor. But
    here is how Saenger defines the term in his glossary : \u201CThe shape of a word
    when written in upper- or lower case letters and delimited by space, as defined
    by the Dutch psychologist Herman Bouma.\u201D \r\n\r\nSaenger\u2019s argument
    in the book is. \u201CWhile the paleographer\u2019s principal focus has been on
    the classification of individual letter forms, the student of the history of reading
    in the medieval West is primarily concerned with the evolution of word shape,
    and letter forms are important only to the degree that they play a role in determining
    that shape. Thus the adoption of the miniscule, that is, lower case letters, as
    a book script is significant for the historian of reading insofar as it contributed,
    in conjunction with word separation, to giving each word a distinct image.\u201D\r\n\r\nI
    originally glommed on to the \u201Ceach word a distinct image\u201D idea because
    of the content the Taylors gave to it, and because of Gerrit Noordzij\u2019s account
    of the \u2018consolidation\u2019 of the word [Chapter 6 of <em>The stroke: theory
    of writing</em>]. Now I believe that reading starts with a low-resolution indexing
    of word items in parafoveal preview and proceeds to a feature-analytically based
    processing routine that occurs at and upon fixation. The low resolution indexing
    provides informative \u201Censemble statistics\u201D that go beyond the envelope
    shape of the word or the simple pattern of ascending, descending and neutral characters,
    and provides a reference frame for the feature-analytic processing. The feature-analytic
    processing yields information about words at the level of their role-units (prototypical
    structures, like stems and counters): the identity of the role units, their relative
    positions, local combinations (in letters), and their across-the-word distributions.\r\n\r\nSo
    where the the Taylors\u2019s notions might apply most directly, as far as I can
    see, is in relation to the kind of information the \u201Censemble statistics\u201D
    contain. There is currently a growing body of research in this area.\r\n\r\nBut
    I just wanted to provide a little push-back on Thomas\u2019s \u201Cbouma nonsense\u201D
    comment."
  created: '2013-06-16 16:53:25'
- author:
    name: William Berkson
    picture: 110306
  body: "By the way, Thomas, I am not seeing that upper case letters are less differentiated
    from one another in terms of shape. Are H and N less different than h and n? Are
    B and D less different than b and d? What study are you alluding to? I thought
    the usual explanation for slower reading of all caps, for the letter ID and look-up
    view, is that lower case is more familiar\u2014which I also don't buy as an explanation,
    but that is a different story.  "
  created: '2013-06-16 18:11:19'
- author:
    name: William Berkson
    picture: 110306
  body: "Thomas, these matters are by no means settled. The \"word superiority effect\"\u2014letters
    within words are more identifiable in a short time than letters within non-words\u2014prima
    facie supports the idea of whole word pattern. Word pattern does not necessarily
    mean the oversimplified version of word envelope, as Peter says. Peter's idea
    is that for skilled readers and familiar words, the visual pattern of the whole
    word, like a Chinese Character, is read, rather than by going the route of first
    identifying letters and then doing a look-up\u2014which we can also do. (And yes,
    I know there are root characters in Chinese.)   \r\n\r\nSupposedly the word superiority
    effect is made compatible with the letter identification and look-up view by 'interactive
    activation', but that this operates routinely for skilled readers has not been
    actually tested, only simulated in computer models. It is a model that can simulate
    the process, but whether it actually works that way in the brain is not known.
    Peter and I are quite doubtful that it operates routinely as claimed, because
    we think it would take too much time in the brain for the rapidity of skilled,
    normal reading. A more more one-way link between visual pattern and sound and
    semantics (rather than cycling up and down) would be quicker.  \r\n\r\nMy understanding
    is that a difference between Peter's and Hrant's views is that Hrant thinks Boumas
    operate mainly in the parafovea, whereas Peter thinks this is an important but
    limited influence, and the main impact is on letters seen in the fovea. "
  created: '2013-06-16 21:21:38'
- author:
    name: quadibloc
    picture: 118515
  body: Although h and n in one way, and b and d in another, are good examples, a
    more typical case would be to compare a and t to A and T. The presence of ascenders
    and descenders does make both words and letters easier to differentiate; all-caps
    text is clearly significantly less readable than lower-case text. Many studies
    have confirmed that, although it should be obvious for almost any typeface.
  created: '2013-06-17 03:59:40'
- author:
    name: William Berkson
    picture: 110306
  body: 'I agree that descenders and ascenders help. My comment was to rebut the explanation
    that Thomas reports: that they differentiate letters more. As far as topology
    the caps don''t seem any less differentiated. The eye may pick out ascenders more
    easily, but then you are getting to the added effects of letters being visually
    within words, which the letter ID and look-up theory deny. '
  created: '2013-06-17 16:42:38'
- author:
    name: Thomas Phinney
    picture: 128358
  body: "Letter differentiation and even the parallel processing of letters is taking
    place within the context of overall size, as denoted by the other neighboring
    letters. I won't suggest for a moment that such does not matter. Without that
    there would be no meaning of x-height, ascender and descender. Topology alone
    isn't everything, clearly.\r\n\r\nI had always heard tell of \"bouma\" as the
    \"shape of the word\" in terms of its outer silhouette. If the bouma includes
    the topology of indivdual letters, then... well, it seems kind of like a two-party
    political system where both parties are moving toward the middle. Instead of a
    clear dichotomy between two theories, it seems more like a spectrum. I will be
    curious to hear what folks think the practical differences are between a theory
    driven by bouma shape (that includes full topology of individual letters) and
    one driven by individual letters that allows for interactions and effects of neighboring
    letters. Really, \"bouma shape\" doesn't feel so top-down in this version....\r\n\r\nCheers,\r\n\r\nT"
  created: '2013-06-19 21:22:23'
- author:
    name: Delete
  body: Rather than rely on theoretical arguments, there is experimental data from
    traffic signs that mixed case signs are better than all caps for comprehension.
  created: '2013-06-19 21:34:24'
- author:
    name: John Hudson
    picture: 110397
  body: "<em>there is experimental data from traffic signs</em>\r\n\r\nYes, but all
    this data confirms is that mixed case text is better for traffic signs. The data
    doesn't indicate <em>why</em> mixed case is better, which is what the theoretical
    arguments are about."
  created: '2013-06-20 00:14:36'
- author:
    name: John Hudson
    picture: 110397
  body: "Thomas: <em> I will be curious to hear what folks think the practical differences
    are between a theory driven by bouma shape (that includes full topology of individual
    letters) and one driven by individual letters that allows for interactions and
    effects of neighboring letters.</em>\r\n\r\nI think part of the attraction of
    Peter's proposed feature role model is that it suggests ways in which we might
    bypass the problems of interactions and effects of neighbouring letters, specifically
    the problem of crowding, which demonstrably makes individual letter recognition
    difficult. If we don't need to recognise individual letters in order to make a
    first pass at word recognition, then that would explain why crowding doesn't seem
    to be a major impediment to rapid and accurate reading. If, instead of recognising
    individual letters <em>as such</em> we are taking in information from multiple
    letters in the foveal fixation (I'm with Peter on this, not Hrant) and recognising
    patterns that resolve to letter sequences, then the closeness and visual interaction
    of those letters that inhibit individual letter recognition actually become useful.
    This is how I understand boumas: a perceptual unit of recognition.\r\n\r\nThe
    latter point is important, I think, because it avoids having to commit to defining
    bouma as any specific graphical phenomenon -- such as the 'word shape' -- and,
    indeed, indicates why such definitions are unhelpful. A bouma is a thing in the
    perception, not a thing on the page. This also means, of course, that it is liable
    to individual variation, i.e. you and I might perceive different boumas when reading
    the same piece of typography."
  created: '2013-06-20 00:31:44'
- author:
    name: Thomas Phinney
    picture: 128358
  body: 'Thanks to all who elaborated on the topic. '
  created: '2013-06-20 20:45:23'
- author:
    name: quadibloc
    picture: 118515
  body: 'Incidentally, on the subject of how old 5-level code is: just today, I came
    across an ad for a 3M Whisper Writer 1000, a teletypewriter that used a thermal
    printer, in a 1983 issue of <em>Datamation</em>.'
  created: '2013-06-21 01:47:32'
- author:
    name: Chris Dean
    picture: 111971
  body: "@IsleofGough  \u201C<em>\u2026there is experimental data from traffic signs
    that mixed case signs are better than all caps for comprehension</em>.\u201D\r\n\r\nReference?"
  created: '2013-06-21 03:42:00'
- author:
    name: russellm
    picture: 111614
  body: my eyes.
  created: '2013-06-21 04:49:47'
- author:
    name: russellm
    picture: 111614
  body: "if you are looking at a sign from such a distance that you can distinguish
    the general shape of the words but not the letters, you will be able to understand
    the message - especially on a traffic or regulatory sign where there is a limited
    number of possible messages with less difficulty if the message is set in mixed
    case than if it is set in all caps. This is hardly even worth discussing. \r\n\r\n
    \ No \r\n  Trespassing\r\n\r\non an 18\" square sign is easier to read than\r\n\r\n
    \ NO\r\n  TRESPASSING \r\n\r\nfrom 100 meters. Try it. \r\n\r\n\r\n"
  created: '2013-06-21 05:04:45'
- author:
    name: John Hudson
    picture: 110397
  body: Russell, IoG referred to 'experimental data', not anecdotal evidence, so Chris
    is quite justified in asking for a reference.
  created: '2013-06-21 06:30:15'
- author:
    name: enne_son
    picture: 109487
  body: "[Thomas Phinney] <em>I will be curious to hear what folks think the practical
    differences are between a theory driven by bouma shape (that includes full topology
    of individual letters) and one driven by individual letters that allows for interactions
    and effects of neighboring letters. </em>\r\n\r\nThomas, for me it\u2019s mainly
    a matter of <em>compatibility</em> with the express and defining attunements of
    professional type-involved practitioners. And it\u2019s a matter of providing
    perceptual-processing <em>touchstones</em>  for typography and type design.\r\n\r\nIf
    a feature-analytic processing of the S1 to S4 / A1 & A2 / D shapes (see above)
    in <em> bounded maps</em> of visual information is seen as a foundational dynamic
    at the \u201Cfront end\u201D of reading, factors like distinctive cue-value, clear
    delineation, proper salience, relative location and some kind of cohesive equilibrium
    at the elemental level of shape primitives (letter details) are easily seen to
    be matters of intrinsic and densely interacting importance. A rhythmic spacing,
    a consistent contrast-styling scheme and strategic construction here, mindful
    of these factors, produce a gestalt integrity at the level of the whole word.
    So this provides a natural fit with the express attunements of experienced type
    designers and typographers to matters of spacing or fit, consistent contrast styling,
    and strategic construction.\r\n\r\nThe feature analytic processing that leverages
    bouma-shape particulars isn\u2019t the whole story in identifying words though.
    Feature-analytic processing is generally thought to lead to a kind of parallel
    letter recognition as a next step which then underpins the orthographic processing
    necessary to get to words. In current models the feature-analytic processing is
    often underspecified, but your sense of moving toward the middle is apt. The two
    processing routines are compatible: they can be seen as different phases or sub-routines
    of a single hierarchical process. \r\n\r\nI\u2019m exploring the equally compatible
    \u2014 and neuro-mechanically plausible \u2014 idea that in the normal reading
    of extended texts by skilled readers feature-analytic processing yields \u2014
    as a result of \u201Cunsupervised\u201D perceptual learning \u2014 a more elemental
    decomposition than the decomposition into individual letters. My more elemental
    decomposition is a decomposition into what I\u2019ve been calling role units.
    This then underpins a higher level parsing of the word into what can be called
    role-unit string kernels.* By leveraging the overlap in these string kernels in
    familiar words the visual word-form resolution system gets to words and a word
    superiority effect.\r\n\r\nI\u2019m generally hesitant to use the thread-space
    of a topic with another focus to summarize my perspective, but I feel it might
    be necessary to show that what\u2019s involved in addressing your follow-up Thomas
    is not just a moving to the middle.\r\n\r\nIn the context of this thread, and
    its bouma-shape by-way, I suspect that the lower case construction is a construction
    that more effectively leverages for the purposes of individuating gestalt-level
    units, the dimensionality of the western alphabet\u2019s cartesian feature-space
    \u2014 with it\u2019s more diversified or informationally rich implementation
    of the compact base-line to x-height zone and more strategic use of the ascender
    and descender zones.\r\n_________________\r\n\r\n* string kernels in this scheme
    are units with the structure x+(y+z)+a (where x and y and z and a are role-units,
    and the \u201C+\u201D sign inside the bracket indicates a local or contiguous
    combination implemented by a letter junction or a common edge, and the \u201C+\u201D
    sign outside the brackets indicates an \u201Copen\u201D or non-contiguous combination
    constrained by immediate adjacency. \r\n\r\n[img:sites/default/files/old-images/string_kernels_72ppi_5970.jpg]"
  created: '2013-06-21 12:55:13'
- author:
    name: russellm
    picture: 111614
  body: "No doubt he is, but I did suggest trying it :o)\r\n\r\n"
  created: '2013-06-21 15:40:35'
- author:
    name: William Berkson
    picture: 110306
  body: "[[http://clearviewhwy.com/ResearchAndDesign/legibilityStudies.php|Here are
    the experimental tests on Clearview Highway]], which found that U & lc was more
    legible\u2014readable quickly at a greater distance\u2014within a given sized
    sign, than an all caps font. This was just one comparison, though. "
  created: '2013-06-21 15:56:41'
- author:
    name: Kevin Larson
    picture: 109739
  body: "Peter, would you predict that a lowercase string kernel nucleus is easier
    to recognize in the presence of a string kernel than on its own: in your example,
    the <em>a</em> is easier to recognize between <em>h</em> and <em>t</em> than a
    lowercase <em>a</em> on its own? Is this also true when the letters aren\u2019t
    part of a word: would it be easier to recognize the lowercase <em>a</em> when
    it is between <em>j</em> and <em>t</em> than a lowercase <em>a</em> on its own?
    Does this effect go away for uppercase letters, or is it just diminished?"
  created: '2013-06-21 17:17:55'
- author:
    name: dezcom
    picture: 109959
  body: I see a written word as an interaction of shapes as well as agreed upon letter
    forms.  Some type faces do a better job of integrating both.
  created: '2013-06-21 19:28:07'
- author:
    name: enne_son
    picture: 109487
  body: "Kevin, good questions. I\u2019m thinking it through \u2014 jat is a pronounceable
    non-word and might partially activate two string kernels, and a letter is a string-kernel
    too, but with a lower dimensionality, so it gets complicated. I'll respond more
    fully later, but it sounds like something worth testing. \r\n\r\nYou might be
    interested to know that one of Jonathan Grainger\u2019s former students Thomas
    Hannagan adapted the notion of string kernels to build on Grainger\u2019s \u201Copen-bigram\u201D
    model of how the brain encodes orthographic information during reading. My scheme
    uses a somewhat different application of the string-kernel concept than Thomas
    Hannagan\_introduced by transposing it from the level of letters to the role-unit
    level. My application and extension of the idea can probably account better for
    the speed and\_automaticity of lexical decision, and the speed and automaticity
    of \u201Cvisual word-form resolution\u201D in the \u201Cimmersive\u201D reading
    of skilled readers (which are probably related), while Thomas\u2019s can probably
    account better for transposition effects such\_as those that occur in that \u201CCambridge\u201D
    scrambled letter text. These predictions might be able to be quantified, implemented
    in a model and lead to a test. \r\n\r\nThomas\u2019s string kernel paper \u2014
    [2012] titled \u201CProtein analysis meets visual word recognition: a case for
    String kernels in the brain\u201D \u2014 is listed here: \r\nhttp://www.neuraldiaries.com/ThomasHannagan/publications.html"
  created: '2013-06-22 03:22:40'
- author:
    name: John Hudson
    picture: 110397
  body: 'Point of information: jat is a word, referring to a member of a Punjabi peasant
    caste.'
  created: '2013-06-22 07:06:14'
- author:
    name: Chris Dean
    picture: 111971
  body: "@enne_son: Following Larson\u2019s remarks, you may also wish to consider
    the difference between what happens with <a href=\"http://en.wikipedia.org/wiki/Pseudoword\">pseudowords</a>,
    <a href=\"http://www.talktalk.co.uk/reference/encyclopaedia/hutchinson/m0038925.html\">letter
    strings</a>, and random letter combinations."
  created: '2013-06-22 13:17:56'
- author:
    name: enne_son
    picture: 109487
  body: "Chris, Kevin: in the scheme I'm proposing string kernels are \u201Cunits\u201D
    in a single \u201Chidden\u201D layer of a 3 to 4 deck hierarchical network. In
    its default operational mode the network doesn\u2019t do \u201Cexplicit labelling\u201D
    \ at the letter level. So I\u2019m not sure I can make direct predictions that
    presume explicit labeling of the elements in the string kernel nucleus when the
    string kernel is presented in isolation \u2014 that is, outside of the context
    of the whole word or the full orthographic sequence, and abstracted from the full
    hierarchical network.\r\n\r\nMy first impulse was, however, to say that, as a
    consequence of the scheme, the >a< probably is easier to recognize between >h<
    and >t< than when it is between >j< and >t<. Even with John Hudson\u2019s caveat,
    because for most jat is unfamiliar. The other cases seem less straightforward.
    But now I'm not so sure there would be an effect.\r\n\r\nIn my scheme the detection
    of a string kernel by a string kernel detection neuron builds on activity at an
    earlier level, which involves what I've been calling role-unit level \u201Cquantization,\u201D
    local combination detection, and across-the-word pair-wise distribution mapping,
    which occur simultaneously and in concert with each other at a single level. So
    another way of asking the question is: is local combination detection easier when
    it is in the context of a word than when it operates without flankers, even if
    the flankers are part of an actual word.\r\n\r\nThat\u2019s as far as I\u2019ve
    gotten so far with an answer."
  created: '2013-06-24 03:56:03'
- author:
    name: Albert Jan Pool
    picture: 111860
  body: "After I read that the story about the US Navy \u2018banning ALL CAPS\u2019
    on the CNN site, I commented that the next thing I\u2019d see happen is the mixed
    writing of names on credit cards. Someone commented that this would be \u2018impossible\u2019
    because of legal restrictions. Unfortunately I couldn\u2019t comment on that anymore,
    because either Disquss or CNN prevents me from tracking and commenting that discussion
    (is this what Snowden is talking about? :\u2013). Being halfway able to track
    what I wrote on Typophile, my question for today is wether anyone here knows about
    such (US?) legislation?\r\n\r\nBtw, in Germany, the mixed writing of names on
    bank cards does not seem to be a problem. On some of my german EC bank cards,
    my name is written mixed. Also, the new DIN 1450 on legibility states that text
    has to be written mixed. All caps is to be used for emphasis only. At some point
    DIN 1450 might collide with some US credit card legislation \u2026 "
  created: '2013-06-24 12:59:28'
- author:
    name: John Hudson
    picture: 110397
  body: "Peter, the Hannagan and Grainger paper seems to associate string kernels
    strongly with the open bigram coding model. Is this also a factor of your proposal?\r\n\r\nIn
    a recent paper, Kinoshita and Norris present experimental results that question
    the open bigram model. \r\nhttp://www.sciencedirect.com/science/article/pii/S0749596X13000223\r\n\r\nThoughts?"
  created: '2013-06-24 20:08:46'
- author:
    name: John Hudson
    picture: 110397
  body: "BTW, have you read the Norris and Kinoshita 'noisy channel' paper?\r\n\r\n'Reading
    through a noisy channel: why there's nothing special about the perception of orthography.'\r\nhttp://psycnet.apa.org/?&fa=main.doiLanding&doi=10.1037/a0028450\r\n\r\nI've
    only seen the abstract, but it sounds lively and contentious:\r\n<blockquote>The
    goal of research on how letter identity and order are perceived during reading
    is often characterized as one of \u201Ccracking the orthographic code.\u201D Here,
    we suggest that there is no orthographic code to crack: Words are perceived and
    represented as sequences of letters, just as in a dictionary. Indeed, words are
    perceived and represented in exactly the same way as other visual objects. The
    phenomena that have been taken as evidence for specialized orthographic representations
    can be explained by assuming that perception involves recovering information that
    has passed through a noisy channel: the early stages of visual perception. The
    noisy channel introduces uncertainty into letter identity, letter order, and even
    whether letters are present or absent. We develop a computational model based
    on this simple principle and show that it can accurately simulate lexical decision
    data from the lexicon projects in English, French, and Dutch, along with masked
    priming data that have been taken as evidence for specialized orthographic representations.</blockquote>"
  created: '2013-06-25 00:19:53'
- author:
    name: enne_son
    picture: 109487
  body: "John, my default scheme doesn't have a bigram-coding \u201Cdeck.\u201D I
    suspect Thomas Hannagan will find a way to adapt his string kernels idea to accommodate
    the new data presented by Kinoshita and Norris. Whether what emerges will still
    be able to bear the name \"open-bigram\" model, I don\u2019t know.\r\n\r\nThe
    assumption underlying most current computational models of orthographic processing
    is that \u201Cthe game of visual word recognition is played in the orthographic
    court of constituent letter recovery.\u201D This is a quotation from Ram Frost
    in a 2011 <em>Behavioral and Brain Sciences</em> target paper that tries to plot
    the path towards a \u201Cuniversal\u201D model of reading. Thomas Hannagan applies
    the string kernel idea to orthography. I apply it to role-units."
  created: '2013-06-25 18:46:17'
- author:
    name: enne_son
    picture: 109487
  body: "John, I've been following the Norris and Kinoshita work for several years
    and have read the paper you referenced. The contention that there is no orthographic
    code to crack is a bit of a red herring, because, in the stimulus sampling approach
    used by Norris and his associates, a representation of letter order is assumed.
    On their own account \u201Cthe Bayesian Reader (Norris, 2006) assumes that word
    recognition involves a representation of input string consisting of sequentially
    ordered letter identities.\u201D These form the \u201Cpriors\u201D in a process
    that decides if the pattern of letters in the stimulus can be fitted to the coded
    representation. The newer work proposes \u201Can extension to the Bayesian Reader
    that incorporates letter position noise during sampling from perceptual input.\u201D\r\n\r\nMy
    scheme doesn\u2019t do everyday recognition on the basis of a representation of
    letter order, but on the basis of a relational filtering (see Bill\u2019s post
    just above) at the level of role units, that detects string kernels before getting
    to words.\r\n\r\nIn 2012 Jeffrey S. Bowers and Colin J. Davis wrote a paper highly
    critical of the Bayesian approach with the provocative title \u201CBayesian Just-So
    Stories in Psychology and Neuroscience.\u201D \r\nSee the downloadable pdf at
    the top of the list here:\r\nhttp://scholar.google.ca/scholar?cluster=5694260091742085801&hl=en&as_sdt=0,5\r\nGriffiths,
    Chater, Norris and Pouget replied to this here:\r\nhttp://www.yangzhiping.com/files/pubs/415.pdf\r\nBowers
    and Davis replied to the reply here:\r\nhttp://scholar.google.ca/scholar?cluster=17829249214662681864&hl=en&as_sdt=0,5&as_ylo=2012\r\n\r\nSo
    it gets complicated and contentuous, and we might have to bring in the US Navy
    to settle accounts using lower case so we can read it comfortably.\r\n"
  created: '2013-06-25 18:55:58'
- author:
    name: William Berkson
    picture: 110306
  body: "Kevin, John, I've been batting back and forth with Peter some ideas about
    a brain processing model that would give a more detailed causal account of how
    the brain would work to identify words in the manner Peter has hypothesized. This
    model building is with an eye to identifying a crucial experiment to test the
    usual 'slot processing' of letters against Peter's whole word model. \r\n\r\nFirst,
    Peter warns me that the phrase \"word specific visual pattern\" is often used
    in the literature to refer to all aspects of the word\u2014the type style, weight,
    the slant, etc. So above I should have more accurately said that Peter's theory
    claims that immersive reading of skilled readers involves identifying words by
    the <em>relational structure</em> of letter features across the whole word. The
    key features he thinks are 'role units' such as ascenders, bowls, stems. \r\n\r\nThe
    model I've developed does give a prediction in answer to your insightful questions,
    Kevin. I call the model \"recursive  relationship filtering,\" using \"matrix
    resonance.\"\r\n\r\nBefore I describe it, let me give some background that will
    make it more clear where I am coming from. My impression\u2014and I only know
    the literature second hand in discussing with Peter\u2014is that most of the modeling
    is too heavily influenced by the Anglo-American tradition of associationism, beginning
    with Locke, Berkeley, and Hume. The idea is that the whole story of learning is
    variations on the theme of repeated association of events. Kant thought that instead
    we have an inborn framework that we use to interpret perceptual information, and
    without that framework we are \"blind.\" So association plays a role, but inner
    frameworks are equally important. \r\n\r\nA key development beyond Kant came with
    Darwin's survival of the fittest. Psychologists and philosophers took Darwin as
    a model for learning, and said that Kant's idea of inner frameworks was correct,
    but that we actively adapt and change them, using trial and error to better and
    more accurately capture meaningful information from the welter of perceptual data
    that comes into our brains. This was the idea of K\xFClpe in Germany, and is followers
    in the Gestalt and W\xFCrtzburg schools of psychology, and by the American pragmatist
    Charles Saunders Peirce. My teacher Popper was influenced by teachers from the
    W\xFCrtzburg school, as I explained in my book with John Wettersten, <em>Learning
    from Error</em> (in the German edition <em>Lehrnen aus dem Irrtum</em>).\r\n\r\nThe
    relevance of this long-standing dispute to reading models is that I don't assume
    that skilled readers simply use the same process as those learning to read, but
    just do it faster. \u2014The processing patterns of learning and skilled readers
    are not isomorphic. Instead, I assume that in learning to read fluently, the brain
    sets up, by trial and error, a framework and pattern of connections that will
    very quickly and efficiently decode the structural pattern of words. \r\n\r\nThis
    is related to Elkhonon Goldberg's theory (See <em>The Executive Brain</em>) that
    one side of the brain deals more with novelty, and the other with learned, more
    efficient processing structures. In effect, as we learn, a much more efficient
    structure is set up on one side of the brain only. This idea is corroborated,
    Peter tells me, by the discovery that skilled readers have more 'lateralization'\u2014processing
    only in one side\u2014than learners, who use both sides more. \r\n\r\nI'll post
    this prelim now, and then explain the model later today.        "
  created: '2013-06-25 23:53:36'
- author:
    name: William Berkson
    picture: 110306
  body: "So here's the processing model of 'relational filtering' to identify words,
    using Peter's concepts. \r\n\r\nFirst, it is a decoding model, encoding structural
    features, and then matching them to words. I don't know whether it operates at
    early visual processing levels, or only after the visual pattern is already processed
    to the point that we would recognize any script that we can't decipher. Second,
    this basic model doesn't assume that Peter is right about reading whole word structural
    patterns, though I think he is. \r\n\r\nThe motto is Michelangelo's \"the more
    the marble wastes, the more the statue grows.\" Its central feature is a filtering
    model, rather than a pure building-up model. It starts with a huge amount of data,
    and ends up with a single word through relationship filtering which arrives at
    a brief code.\r\n\r\nEach filter layer I will describe in terms of functionality,
    as I don't know exactly how this is executed by one or masses of neurons. \r\n\r\n1.
    The bottom layer or deck is a layer in which we throw a framework of an inner
    matrix (inborn or learned) over the incoming data. \r\n\r\n2. All the information
    on the bottom layer is sent to layer two, which establishes a 'set' as to what
    kind of pattern it is: whether the pattern is, for example: 1. a face, 2. a three-dimensional
    object, or 3. writing to be decoded. If the criteria in this second layer in the
    'face' area aren't met, the signal is stopped, and not sent on from this area.
    Since it is a word the 'word' filter will detect a match, and all of the information
    will be passed on to the third level.\r\n\r\n3. On the third level, letter features
    are coded. The cells in the matrix in each small area are sent forward to the
    fourth level, in which a group of say four vertical cells is coded to represent
    location. \r\n\r\n4. The information on the third level is mapped to arrays of
    role unit detectors on the fourth level; each detector in an array can detect
    one of the possible combinations of the four states of the cells. All detectors
    receive the information of all four cells, but only where there is a match to
    the code of a particular role unit is the information sent on to the next level.
    All the alternative ID's for a particular role unit are passed on to the next
    level preserving information on what is adjacent. \r\n\r\n5. The fifth level puts
    together the role units in Peter's 'string kernels', as described above, and passes
    them directly to 'word codes', which code words as an assembly of string kernels.
    \r\n\r\n6. On the sixth and final level are all words represented in whole word
    structural relationship codes, codes of adjacent string kernels. Each complete
    word code assembled on level 5 is 'painted' or sent across the entire vocabulary,
    or at least common vocabulary, and whatever remembered word code matches that
    input, responds and links to a meaning and sound. The word is detected consciously
    at this point. \r\n\r\nNow this sixth level is the final decoding level only for
    skilled readers reading familiar words. If it is an unfamiliar name, for example,
    or a confusion typo, the information will be sent on to an orthographic level,
    where letters are identified and look-ups done on higher levels again. \r\n\r\nIn
    other words, reading by visual pattern, without any orthographic processing, is
    the preferred route, as it is quickest. But we can also do quite quickly\u2014but
    a bit longer\u2014the orthographic route. All of these routes have been built
    up in the learning process, and unfamiliar names etc. regularly draw on these
    skills. The idea is that processing by whole word structural pattern is the last
    learned, but the first used. \r\n\r\nWhen the words are well done visually, and
    the memory of the code is there, this is experienced as instant reading of words:
    we don't see letters, we experience meaningful words. In something like the scrambled
    letter tests, we are conscious of some time taken to stare at a word to decipher
    it. \r\n\r\nThe main advantage of of model like this is that it accounts for why
    we read AlTeRnAtiNg CaSe more slowly, which the interactive activation model just
    puts down to lack of familiarity. But this explanation is not plausible because
    the interactive activation says we first identify separate letters as abstract
    entities, and we are in fact extremely familiar with both lower case and caps.\r\n
    \       \r\nNext, I will post answering Kevin's questions with this model, and
    discussing all caps verse lower case. Sorry for the lack of an illustrative diagram.
    I don't have time to do one now. \r\n  \r\n  \r\n\r\n"
  created: '2013-06-26 00:00:17'
- author:
    name: John Hudson
    picture: 110397
  body: I'm sympathetic to both the matrix and role unit ideas but, other than a presumption
    of efficiency, is there a reason to favour a model that builds up from these to
    whole word structure recognition, rather than considering that they might contribute
    to letter recognition and hence to orthographic processing?
  created: '2013-06-26 00:27:21'
- author:
    name: William Berkson
    picture: 110306
  body: "I think it's only efficiency, in terms of being quicker. But this is a big
    deal in brain processing, I think. Matthew Luckiesh compared reading speed to
    blood pressure. The body will do almost anything to keep up blood pressure, because
    if it is too low you can pass out and die. Similarly, the brain wants to perceive
    meaning, and perceive it now. It is a matter of survival, which carries over to
    reading speed. \r\n\r\nThe brain is massively connected\u2014more synapses than
    stars in the milky way!\u2014and does parallel processing, but it is not that
    quick in cycling through a reflective process. (Reflection undoubtedly involves
    interaction of levels as envisaged in interactive activation.) Compare the speed
    of recognizing a face and adding 436 + 784. Also I don't see how going to a look-up
    of abstract letter identities isn't an additional step and time. You already have
    the letter features, and relationships in order to identify letters. If you just
    squeeze the ID from that, without going to abstract letter identities, you are
    leaving out a step, it seems to me, so that it is quicker. That's why I think
    we learn the whole word structural pattern\u2014unconsciously, just as we learn
    grammar in our mother tongue. \r\n\r\nWhether we need to identify the flanking
    role units in the manner Peter suggests is less clear to me. We might have identified
    the structural pattern of the letters, and of the relational pattern of the role
    units in whole word in another way, maybe. But I don't see why a look-up of orthography
    of abstract letter units is needed routinely. \r\n\r\nAn important point to note
    is that the word superiority effect is a time-based effect. It is only when you
    flash words for very short times followed by a mask that you get it. It you look
    longer at words and non-words, it goes away. That to me is an indication of the
    fact that the race to word ID is won by whole word relational pattern, before
    it goes to orthography. \r\n\r\nThe race is 'won' when you get a meaningful word
    into working memory, as that is key to consciousness. To me you get the word superiority
    effect because word ID by the whole word structural pattern gets into working
    memory before the orthographic processing route. We are also very good at the
    orthographic route, but it isn't as fast. \r\n\r\n\r\n\r\n"
  created: '2013-06-26 00:58:36'
- author:
    name: John Hudson
    picture: 110397
  body: "I'm not convinced by the efficiency argument, per se, because evolution favours
    good-enough solutions over optimal solutions. Your comments about the word superiority
    effect gave me pause, but that is, after all, a <em>letter recognition test</em>.
    What I have posited is that narrow matrix alignment of role units perceived as
    boumas aid letter recognition in a crowded context, and the so-called 'word superiority
    effect' is exactly what one would anticipate in that model, because real words
    are made up of role units arranged in ways that produce familiar bouma shapes,
    and non-words are not.\r\n\r\nI'm really looking forward to your experiment design
    ideas!"
  created: '2013-06-26 02:30:59'
- author:
    name: quadibloc
    picture: 118515
  body: "It sounds as though Norris and Kinoshita are presenting an interesting idea:
    that people read as a naive person would assume, by recognizing the letters one
    by one, but the fact that they do so under adverse conditions, and with the aid
    (or otherwise) of a visual pre-processing system developed for other purposes
    makes it appear that 'bouma' and word shape play a direct role in reading.\r\n\r\nAfter
    all, it's only occasionally in reading that one has to guess which letter is in
    any given position. A lot of people even move their lips when they read. Those
    who read more quickly are more likely to be recognizing a word at a time, but
    even they consciously think of themselves as reading individual letters."
  created: '2013-06-26 02:31:11'
- author:
    name: enne_son
    picture: 109487
  body: "John asked: is there a reason to favour a model that builds up from role-units
    to whole word structure recognition, rather than considering that role units might
    contribute to letter recognition and hence to orthographic processing?\r\n\r\nI
    think it can be argued that in the part of the visual cortex where letters are
    recognized, the neurons\u2019 preferred receptive field is larger than the single
    letter, and includes parts of adjacent letters in optimally spaced text typographically
    speaking. In a relational filtering environment, this mismatch makes the local
    combination detection required to get to letters subject to crowding, unless the
    role-units that are adjacent to the string-kernel nucleus, are part of an already
    learned and synaptically supported code-item. \r\n\r\nAccording to Denis Pelli
    and others crowding is negligible in foveal vision. However, in a poster presentation
    at this year\u2019s Vision Sciences Society Annual Meeting last month, Mara Lev,
    Oren Yehezkel, and Uri Polat found that when foveal processing of letter targets
    is interrupted by backward masking, the spatial crowding is revealed and that
    a release from crowding in the fovea is achieved by allowing an increase in reaction
    time.\r\n\r\nSo there is a psychophysical reason to favour a model that goes from
    role-units to string kernels instead of letters. Getting to independent letters
    would require squelching of elements in the area surrounding the string kernel
    nucleus."
  created: '2013-06-26 02:54:26'
- author:
    name: William Berkson
    picture: 110306
  body: "John, following the above model, the word superiority effect comes from the
    word getting into working memory before the letters are identified as such. We
    don't identify the middle letter or whatever but deduce it from knowing the word.
    So it's not really 'letter recognition' in my model and Peter's theory, it's word
    recognition and letter deduction. \r\n\r\nAbout what evolution favors, I think
    being able to read 'meaning' in scenes and situations, very rapidly, is highly
    rewarded both for survival and reproduction. Language in particular is essential
    to social interaction, which is the big thing that gives us our advantage. Also
    it is important to winning and keeping a mate, so good language skills give a
    competitive advantage, pushing beyond 'good enough.'\r\n\r\nThe guy who can understand
    ladies very well is way ahead of the game. Of course I've never met one who could
    :) "
  created: '2013-06-26 02:56:43'
- author:
    name: William Berkson
    picture: 110306
  body: 'Very interesting analysis, Peter. '
  created: '2013-06-26 03:00:48'
- author:
    name: John Hudson
    picture: 110397
  body: "Remind me, does the word superiority effect apply only to real words, or
    also in any degree to word-like sequences, i.e. non-words that correspond to orthographic
    patterns of the given language.\r\n\r\nOr, put the question another way, is the
    word superiority over non-words the same as the word superiority over pseudo-words?
    Is there a superiority of pseudo-words over non-words?"
  created: '2013-06-26 04:13:28'
- author:
    name: enne_son
    picture: 109487
  body: "John, it\u2019s the strongest when real words are used, but superiority effects
    also occur for pseudo-words that follow the orthographic patterns that occur in
    real words. Consonant strings perform the worst."
  created: '2013-06-26 04:26:33'
- author:
    name: John Hudson
    picture: 110397
  body: Thanks, Peter. Superiority effects for pseudo-words suggest something other
    than -- or in addition to -- 'knowing the word' is at work.
  created: '2013-06-26 05:28:01'
- author:
    name: quadibloc
    picture: 118515
  body: "@William Berkson:\r\nThe human brain has high requirements for oxygen, calories,
    and protein. Thus, how can we explain the extent to which it is developed, considering
    that only long after the brain was developed did intelligence bring a big evolutionary
    payoff, by letting humans invent things like the bow and arrow - never mind rifles
    and shotguns and antibiotics?\r\n\r\nIn nature, some animals have structures that
    are developed out of all proportion to their survival needs. The antlers of the
    Irish Elk. The tail of the peacock. And these were due to sexual selection.\r\n\r\nThus,
    it is very likely that sexual selection played a significant role in the development
    of human intelligence."
  created: '2013-06-26 13:42:58'
- author:
    name: William Berkson
    picture: 110306
  body: "John, one of the key weaknesses with the research on the word superiority
    effect that Peter has pointed me to is that the time between the onset of the
    stimulus and the mask, the so-called SOA (Stimulus Onset Asychrony), has not been
    well studied and controlled. They tended to just adjust it until they got results.
    But how long an SOA gets what results isn't clear. \r\n\r\nWhat the masking seems
    to do is cut off the processing\u2014by feeding in a new image\u2014and the exact
    time interval matters, and could be very revealing of what is going on. \r\n\r\nSo
    to evaluate the relationship between word and pseudo word and non word effects,
    you'd need to be more systematic about controlling the SOAs.  \r\n\r\nMy suspicion
    is that when the whole word relational pattern fails, the signals with information
    about the role units are immediately fed forward for orthographic and sound of
    syllables processing. Some of that must be accessible to memory immediate after
    the mask. But Peter's note that the superiority of pseudo words is less; so this
    indicates some kind of hierarchy in time on the processing, with in my view the
    whole words pattern first. "
  created: '2013-06-26 15:33:19'
- author:
    name: enne_son
    picture: 109487
  body: "John, by definition a pronounceable pseudo-word contains several identifiable
    string kernels.\r\n\r\nSuperiority effects are about the facilitating and /or
    inhibiting effect of context. Interactive Activation schemes see the facilitation
    and inhibition as a consequence of the two-way communication between decks and
    as requiring a series of feed-forward and feed-backward <em>epochs</em> to sort
    themselves out.\r\n\r\nIn a relational filtering environment using role-units,
    string kernels and an intrinsic integration interface, local combination detection
    is already inhibited in non-words and facilitated in familiar words by the time
    we get to the string-kernel level in the initial feed-forward sweep. It is inhibited
    in non-words by the presence of unfamiliar flanking role units, and facilitated
    in words and pronounceable pseudo-words by familiar flanking role-units.\r\n\r\nLocal
    combination detection is foundational to letter recognition, but the post-mask
    letter recognition called for in the Richer-Wheeler versions of the tests \u2014
    tests that are biased toward encouraging rapid automatic word-form resolution
    at the front end \u2014 requires explicit labeling at the post-mask end. It can
    be argued then, that the post-mask letter identification benefits from the early
    low-level facilitation of local combination detection in the string-kernel context."
  created: '2013-06-26 15:39:28'
- author:
    name: enne_son
    picture: 109487
  body: Bill, I don't think longer SOAs were needed to get pseudoword superiority
    effects. I think they were needed to get word superiority effects with alternating
    case stimuli and words with increased spacing between the letters.
  created: '2013-06-26 16:21:25'
- author:
    name: William Berkson
    picture: 110306
  body: 'Peter, ok, thanks. Maybe the pronunciation of syllables in the language is
    coded at the same level as whole words? That might explain it. Then the pronunciation
    clues would be fed forward to the next processing level if the whole word structural
    pattern doesn''t ID a specific meaningful word. But otherwise stopped. '
  created: '2013-06-26 17:31:06'
- author:
    name: enne_son
    picture: 109487
  body: "As a result of viewing the Word Superiority Demo <a href=\"http://www.psiexp.ss.uci.edu/research/teachingP140C/demos/demo_wordsuperiorityeffect.ppt\">here</a>,
    I think I need to extend my scheme and revise the analysis I gave in the last
    sentence of the last paragraph of my 26 Jun 2013 \u2014 7:39am post.\r\n\r\nBut
    I'll wait until readers still following this topic have had a chance to view the
    demo. Go through it several times, then try some introspection."
  created: '2013-06-27 16:30:01'
- author:
    name: Chris Dean
    picture: 111971
  body: "Best to use informative hyperlinks instead of \u201Chere.\u201D Especially
    when they initiate downloads.\r\n\r\nHow about \u201Cdownload a Word Superiority
    demonstration (ppt, 159k).\u201D"
  created: '2013-06-27 20:09:09'
- author:
    name: John Hudson
    picture: 110397
  body: "Wow. I don't think I was expecting the word superiority effect to be quite
    so extreme. For me, it was the difference between 100% success with words and
    100% failure with non-words. The non-words were all orthographically impossible,
    though, and I'd love to see a demo that included pseudo-words.\r\n\r\nI'm still
    not sure what the word superiority effect indicates, though, in terms of how we
    read. It demonstrates that we proceed very quickly to word recognition, and that
    our brains are telling us what word we're looking at (if it is a word) more quickly
    than we consciously identify individual letters in specific <em>unannounced</em>
    positions. But does that actually tell us anything about <em>how</em> we recognise
    the word, or only that consciously identifying letters in unannounced positions
    is something we are not practiced at doing?"
  created: '2013-06-27 20:16:21'
- author:
    name: enne_son
    picture: 109487
  body: "Chris, I\u2019ll bear your suggestion in mind.\r\n\r\nChris or Kevin, does
    the demo give an accurate impression of the paradigm in use? \r\n\r\nI don\u2019t
    know what exposure times and SOAs were used in the demo. I do know that in the
    early stages of the history of testing with the Reicher-Wheeler task, trigrams
    and quadrigrams were often used."
  created: '2013-06-27 20:59:05'
- author:
    name: William Berkson
    picture: 110306
  body: "John, I think the subjective experience supports the basic idea that Peter
    has been urging for a long time, and that I have tried to model a bit above. Skilled
    readers can recognize familiar words by the relational pattern of letter features
    across the whole word. We can also do orthographic look-ups, probably in some
    manner that the interactive activation folks have modeled. But these are not the
    quickest or the routine route for skilled readers. \r\n\r\nWe recognize the word
    as a whole, and then if we are asked about letters, we can draw on our memory
    of correct spelling. But just as we can read words we can't spell correctly\u2014if
    you're like me anyway!\u2014we routinely read words without having to process
    spelling at all. \r\n\r\nSorry for the delay on further posting on Kevin's questions.
    I'll do that soon.   "
  created: '2013-06-27 22:11:27'
- author:
    name: Nick Shinn
    picture: 110193
  body: "Peter, in the demo what are the letters above and below XXXXXX for?\r\n\r\n**\r\n\r\nAs
    with John, IMO the word superiority effect reveals little about the reading process,
    other than the fact you can\u2019t recognize something that shouldn\u2019t exist.\r\n\r\n[img:sites/default/files/old-images/fp_06_5701.jpg]"
  created: '2013-06-27 22:44:02'
- author:
    name: enne_son
    picture: 109487
  body: "Nick, you have to run the demo in the slide show mode, clicking your way
    through. The XXXXXXs are a masking device that is intended to arrest the further
    coding of the visual information after the stimulus disappears. When the arrangement
    of XXXXXXs appear with a letter above and below one item in the series, you have
    to decide which one you saw in the stumulus word or non-word. \r\n\r\nYou can
    check up on your the accuracy of your selection by toggling back using the backward
    / forward / up / down keys on your keyboard."
  created: '2013-06-27 23:07:40'
- author:
    name: John Hudson
    picture: 110397
  body: "Bill: <em>Skilled readers can recognize familiar words by the relational
    pattern of letter features across the whole word. We can also do orthographic
    look-ups, probably in some manner that the interactive activation folks have modeled.
    But these are not the quickest or the routine route for skilled readers.</em>\r\n\r\nIt's
    that last statement that I have trouble with, not because it is unreasonable --
    it isn't --, but because we're still only conjecturing that feature-to-word is
    'the routine route'. I think it is highly likely, but I'm still waiting for the
    compelling experimental results. The word superiority effect only shows that we're
    really good at word recognition, not how we do it."
  created: '2013-06-27 23:21:12'
- author:
    name: Deus Lux
    picture: 126714
  body: Caps are so much more legible than lowercase.  I can't see how this improves
    things in any way other than aesthetically.
  created: '2013-06-27 23:38:44'
- author:
    name: Chris Dean
    picture: 111971
  body: "I have not been following this thread to the letter (pun intended) and I
    cannot be certain if my computer is displaying the presentation as it was intended,
    most importantly the timing, but based on a quick review Reicher (1969) is not
    be an accurate replication, if that indeed is what you were going for.\r\n\r\nThat
    aside, I think what is potentially interesting is the possibility of <a href=\"http://en.wikipedia.org/wiki/Inhibition_of_return\">inhibition
    of return (IOR)</a> confounding  the results due to the varying proximities between
    correct and incorrect response letters. Of course, this is more than a simple
    detection task, but some food for thought:\r\n\r\nIn your slides, the correct
    response is always closer to its mate than the incorrect. Thus, at a certain SOA,
    the correct response gets an unfair penalty as it is masked by IOR because you
    are forced to return your attention to a previously attend location. Most importantly,
    one that is closer to the correct than incorrect response. \r\n\r\nI would hypothesize
    that making a correct response will become easier when as the distance between
    the correct and incorrect letters increases in the initially presented series
    of letters, and that this will happen regardless of the type of letters presented
    (random/pseudo/word). See below:\r\n\r\n[img:sites/default/files/old-images/loremip_5676.jpg]\r\n\r\nTherefore,
    in order to avoid a <em>potential</em> conflict of IOR, and in order to draw statistically
    valid conclusions, the distance between correct and incorrect response letters
    in the initially presented series of letters will need to be counterbalanced both
    with and between conditions of random letter-string, pseudo-word, and word."
  created: '2013-06-28 01:08:00'
- author:
    name: enne_son
    picture: 109487
  body: "[Chris Dean] \u201C<em>if that indeed is what <strong>you</strong> were going
    for</em>\u201D\r\n\r\nChris, it\u2019s not my demo. \r\nI found it as a result
    of a google search. It\u2019s one of a series of demos on this page:\r\nhttp://www.psiexp.ss.uci.edu/research/teachingP140C/\r\nhttp://www.psiexp.ss.uci.edu/research/"
  created: '2013-06-28 02:00:53'
- author:
    name: Chris Dean
    picture: 111971
  body: "When you say \u201Cparadigm in use\u201D what paradigm are you referring
    to? Your own model, or an example of a classic WSE?"
  created: '2013-06-28 02:04:52'
- author:
    name: enne_son
    picture: 109487
  body: "I was referring to the Reicher-Wheeler Paradigm as it was used by Reicher
    [1969], Wheeler [1970], and others testing the effect, in other words representative
    of the classic WSE test. \r\n\r\nAdding later: I know that Reicher [1969], Wheeler
    [1970] and Marilyn Jager Adams [1979] used quadrigrams and Purcell and Stanovich
    [1982]  use trigrams, but I don't remember about the others. Probably with more
    letters the effect is more dramatic."
  created: '2013-06-28 02:27:56'
- author:
    name: Chris Dean
    picture: 111971
  body: "Not the same as Reicher (1969) by a long shot. And upon further reflection,
    my IOR hypothesis needs a bit of refinement. But that won\u2019t happen today."
  created: '2013-06-28 03:23:54'
- author:
    name: William Berkson
    picture: 110306
  body: "John: \"The word superiority effect only shows that we're really good at
    word recognition, not how we do it.\"\r\n\r\nWe already knew we were good at word
    recognition before Reicher and Wheeler; what they added was that at short times
    we seem to be better at reading words than letters.    \r\n\r\nIt is the impact
    of the time factor and masking which shows us something important is going on
    in the stages of brain processing that favors words. It doesn't say how, but the
    differential impact of the time factor on word and letter recognition is a key
    fact that any adequate theory of reading has to explain. "
  created: '2013-06-28 03:32:05'
- author:
    name: John Hudson
    picture: 110397
  body: "<em>...what they added was that at short times we seem to be better at reading
    words than letters.</em>\r\n\r\nBut that still doesn't tell us anything about
    <em>how</em> we read the words. Consciously identifying individual letters in
    the context of strings of letters is not a task we have reason to perform on a
    daily basis, whereas recognising words is. So I'm not surprised that when tested
    we do better at the latter."
  created: '2013-06-28 03:33:49'
- author:
    name: enne_son
    picture: 109487
  body: "[John Hudson] \u201C<em>I'm still waiting for the compelling experimental
    results<em>\u201D\r\n\r\nThat\u2019s Ferlinghetti-ish of you!\r\n\r\nI think this
    will have to come from two places: crowding tests and ideal observer analysis
    or neural network modeling.\r\n\r\nI think in familiar words there is a release
    from crowding because the relational filtering is done at a more elemental level,
    the level of role units, and hence requires smaller isolation fields than if relational
    filtering using explicitly labeled letters is required. Explicit labeling at the
    letter level requires larger isolation fields, because isolation fields are in
    direct proportion to the size of the item that must be detected and combined or
    integrated. So if the task mast be done with explicit labeling at the letter level,
    benchmark-level performance might require larger isolation fields. Tests will
    have to be done in foveal vision with fixation time constraints imposed by masking,
    as in the 2013 tests reported at the 2013 annual meeting I mentioned above.\r\n\r\nIdeal
    observer analysis using multiple hidden layers, real receptive field constraints
    and documented isolation field constraints should be able to gauge the performance
    of a string kernel model using role-units versus that of a string kernel or other
    model using letters. If there is a good fit of the one or the other model's performance
    to behavioural results we might become biased toward the better fitting model.
    To find out more about Ideal Observer Analysis, consult: http://en.wikipedia.org/wiki/Ideal_observer_analysis\r\n\r\nThere
    is also new \u201Ccircumstantial evidence\u201D by members of the \u201Cbubbles\u201D
    research team [Xavier Morin-Duchesne1, Daniel FIset, Martin Arguin and Fr\xE9d\xE9ric
    Gosselin], that suggests [quoting their abstract] \u201Cthat the visual system
    bypasses single letter identification on the road to word recognition.\u201D See
    the abstract of their 2012 poster presentation here: http://ww.w.journalofvision.org/content/12/9/532.short.
    There was a followup poster presentation to this at the 2013 Vision Sciences Society
    Annual Meeting last month. See the abstract at this download link: http://www.visionsciences.org/programs/VSS_2013_Abstracts.pdf
    [2.4MB], page 309."
  created: '2013-06-28 03:37:50'
- author:
    name: William Berkson
    picture: 110306
  body: "John: \"Consciously identifying individual letters in the context of strings
    of letters is not a task we have reason to perform on a daily basis.\"\r\n\r\nAh,
    but according to the current received view it is something we perform constantly,
    because identifying individual letters first is a necessity to reading words.
    The apparent refutation of this view by the Reicher and Wheeler tests is what
    interactive activation theories attempt to account for, and remove the contradiction.\r\n\r\nYou
    have to look at the state of the debate to see why the Word Superiority Test became
    such a key piece of data: it is an apparent refutation. \r\n\r\nLet also here
    address Kevin's very interesting questions. My model leads me to suspect that
    there will not be a 'familiar kernal superiority' effect at very short SOAs, but
    that there *may* be at longer SOAs. At short SOAs, only the whole word will get
    into working memory, and I think that accounts for the normal WSE. At longer SOAs
    you can even get a WSE out of mixed case, and there I think interactive activation
    is indeed at work\u2014it is just a later, more time consuming process, which
    is only started if whole word ID fails. \r\n\r\nIf Peter is right about the string
    kernels operating early on, I still am not sure whether the flanking role units
    are chopped off when the whole word relational structure is completely coded and
    assembled. If these flanking role units are filtered out at my final stage, then
    they won't appear in working memory and later processing. If they aren't filtered
    out, then they may appear. So the implication of my model is: no 'kernel superiority
    effect' at short SOAs, possibly at longer ones. \r\n\r\nI would add that generally,
    as you see Peter looks to crowding effects for a critical test of his theory.
    I suspect that the time factor may also prove crucial. If the time for the brain
    to cycle messages repeatedly, as required by interactive activation, is too long,
    that would refute that as an explanation for the normal WSE, and would be a strong
    argument for some theory like Peter's. With new imaging techniques, it should
    be possible to get enough information to make such calculations, I think. In addition
    there are new experimental techniques which can detect what is processed when,
    and these may prove revealing also. \r\n \r\n\r\n     \r\n\r\n\r\n"
  created: '2013-06-28 04:00:19'
- author:
    name: John Hudson
    picture: 110397
  body: "Bill, note that I wrote '<em>consciously</em> identifying individual letters',
    which is what the test methodology that reveals the word superiority effect requires.
    When we are reading, we're not consciously performing any recognition operations
    -- either of letter or of words --, which is precisely why we find it so difficult
    to determine how we read.\r\n\r\n<em>You have to look at the state of the debate
    to see why the Word Superiority Test became such a key piece of data: it is an
    apparent refutation.</em>\r\n\r\nThis is what I am saying, though: it doesn't
    appear to me as a refutation, because the task that the test uses only shows that
    we recognise words more quickly than we consciously identify individual letters
    in particular positions in strings. That doesn't refute any particular model of
    how we recognise the words, because none of those models involve us <em>consciously</em>
    identifying individual letters in particular positions such that we could name
    the letter.\r\n\r\nWhat the test shows is that we can recognise words in a very,
    very short period of time, and having identified the words we can then figure
    out what the letters were. And when we're shown non-words we can't do either.
    But, according to Peter's reporting, when we're shown orthographically permissible
    pseudo-words, there is also a superiority effect, albeit not as great as for real
    words. That suggests to me that part of what enables us to recognise strings of
    letters and <em>hold them in our minds</em> -- such that we can figure out what
    letters are involved-- is a very rapid orthographic processing.\r\n\r\nNow, it
    seems to me that your explanation for the pseudo-word superiority effect must
    be something like this: 'We perceive relational pattern of letter features across
    a whole word, and from that we recognise the word, except in this case it isn't
    a word so we can't, so we instead perform orthographic coding and guess at the
    letters'. That seems a really long process to go through, involving a failure
    of a primary word recognition mechanism -- your 'routine route' -- before we even
    get to the mechanism that enables us to complete the task. It seems to me more
    likely that the orthographic processing kicks in right away, in which case the
    superiority effect for real words over pseudo-words is simply one of familiarity,
    i.e. a matter of vocabulary.\r\n\r\nAnd I think you shouldn't be too quick to
    discount familiarity, because if we were as good at recognising relational patterns
    of letter features across a whole word as you say, then it wouldn't much matter
    whether the words were real, pseudo or non in terms of our processing. It is precisely
    because the real words are familiar that they have an advantage: we recognise
    them as something that we <em>know,</em> and that means we know lots of things
    about them (what they mean, how they are spelled, what letters occur in which
    positions in the word), and we can hold them in our heads because they're already
    there. That means we don't need to be trying to recall the visual image of the
    thing we just saw for a split second while trying to carry out the conscious task
    of naming a particular letter in a particular position. In the case of a pseudo-word,
    the advantage also seems to me one of familiarity: the string exhibits patterns
    that we know, and that we are able to conventionally map to phonetic sequences,
    and that too means that we can handle the string as something other than a visual
    memory. In the case of a non-word, all we have is the visual memory of something
    that we didn't know and that didn't correspond to any known patterns and that
    we saw for a split second, so it's no surprise that there is a Non-Word Inferiority
    Effect."
  created: '2013-06-28 05:28:44'
- author:
    name: enne_son
    picture: 109487
  body: "I reworked parts of the ppt file. \r\nI made pronounceable pseudowords, words
    in lowercase, and nonsense trigrams.\r\nI can't figure out where the timings are
    set, but we can probably adjust them too.\r\n\r\nDownload the new ppt file from
    here:\r\nwww.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt\r\nFile
    size is 137 KB"
  created: '2013-06-28 13:01:12'
- author:
    name: William Berkson
    picture: 110306
  body: "John, I think it's not a long process, because 'quantizing' or isolating
    letter features, and then having a process to code how they are connected must
    take place anyway to identify separate letters. We don't know it's a separate
    letter until analysis separates it out from the word as a whole\u2014and the whole
    word is more easily separated, earlier, because of word spacing. \r\n\r\nPeter's
    point that the letters are normally too close for efficient individual letter
    recognition\u2014there are crowding effects\u2014is germane here. It would be
    interesting to see if wide tracking would help some readers with dyslexia, for
    example. It may be that they have problems creating the filters to decode closely
    packed letters.  \r\n\r\nSo, the same massively parallel process that enables
    us to identify letters enables us to identify the whole word pattern *at the same
    point in processing*. That's why it's more efficient to ID the whole word structural
    pattern if we have it recorded in our memory and easily accessible. Decoding the
    letters as abstract entities would be a next step beyond the assembly of the structural
    code of all the letters in the word, but the shortest next step to a word id can
    be an immediate connection to meaning and sound. For we have\u2014if Peter's theory
    is right\u2014a whole word structural code in our memory banks, to check for matches.
    That's actually the key question: do we have a memory record of a code of the
    whole word relational structural pattern? If we do have that memory, a straight
    path to it is quickest.  \r\n\r\n\r\n\r\n\r\n"
  created: '2013-06-28 13:57:29'
- author:
    name: oldnick
    picture: 109434
  body: "William,\r\n\r\nI would suggest that the whole word determined the relational
    structure pattern. Some months back, I was on my patio, observing the suburban
    wildlife when, immediately, they all disappeared, birds and squirrels alike. I
    asked myself, \u201CWhat\u2019s up with that?\u201D And, then, I looked up and
    saw a hawk in the sky. Apparently, both the birds and squirrels had an image of
    a hawk as a clear and present danger hard-wired into their brains. There was no
    time for reflection on the meaning of that outline.\r\n\r\nGiven the speed and
    fluidity at which both conversation and reading flow, it is sensible to assume
    that whole words get hard-wired into our white matter, as we grow and learn, and
    as our brain grows, which it normally does until approximately age thirty."
  created: '2013-06-28 15:08:41'
- author:
    name: enne_son
    picture: 109487
  body: "[John Hudson to William Berkson] \u201C<em>Now, it seems to me that your
    explanation for the pseudo-word superiority effect must be something like this:
    'We perceive relational pattern of letter features across a whole word, and from
    that we recognise the word, except in this case it isn't a word so we can't, so
    we instead perform orthographic coding and guess at the letters'. That seems a
    really long process to go through, involving a failure of a primary word recognition
    mechanism -- your 'routine route' -- before we even get to the mechanism that
    enables us to complete the task.</em>\u201D\r\n\r\nBelow is a link to a pdf which
    shows the communication between what I\u2019ve been calling the \u201Crole-unit
    level string kernel deck\u201D and the \u201Cintrinsic integration interface.\u201D
    It is based on my knowledge of neural physiology, specifically how neural activity
    is governed by the handling of \u201Caction potentials\u201D at established synaptic
    links between the branching components of \u201Caxons\u201D and the branching
    components of \u201Cdendritic arbours.\u201D Axons are the outgoing or broadcasting
    parts of neurons, and dendrite trees are at the receiving and amplification end.\r\n\r\nIn
    the pdf I compare activity in both decks for a canonical word and for a word with
    a single set of transposed letters. You can draw your own conclusions. \r\n\r\nHere
    is a link to the pdf:\r\nwww.enneson.com/public_downloads/typophile/two_decks_pe_for_typophilers.pdf\r\n641.3
    KB\r\nThe pdf has three pages.\r\n\r\nEarlier I wrote that there would be a lot
    of string-kernel overlap in pseudowords. I think if your were to plot this out
    you would see that this is the case. The problem is, there won\u2019t be just
    one single full-word unit in the intrinsic integration interface which gets fully
    or partially activated. A single full-word unit is one that has a meaning and
    a pronunciation vector over it. But this is the same in the Interactive Activation
    account. Pseudowords don\u2019t fully activate any one item in the orthographic
    lexicon, which constitutes the word level in the Interactive Activation scheme.
    Pseudowords just activate some more than others. One question then, in a string
    kernel scheme like mine is, does the visual system use established intrinsic integration
    codes for syllables or bigrams, if such exist? Syllables or bigrams are structures
    with just pronunciation vectors over them, not meaning vectors.\r\n\r\nI agree
    that in the pseudowords a fast phonetic disambiguation comes into play and this
    is what we explicitly hold in working memory. But the Interactive Activation account
    in its most primitive form says there is feedback to the letter level and what
    we hold in implicit working memory is a sequence of strongly activated letter
    codes, not the explicit memory of the word, it\u2019s pronunciation, and a latent
    knowledge of the spelling. So there is a disconnect here, with what we see through
    introspection.\r\n\r\nIt\u2019s possible that already at my string kernel level
    a fast phonetic disambiguation starts to happen, especially in phonetically complex
    orthographies. This is because the string kernels contain flanking elements outside
    the nucleus which can conceivably indicate which phonetic variant applies. But
    I haven\u2019t worked this out. \r\n\r\nMore likely intrinsic integration codes
    for bigrams, syllables and morphemes are used and strung together during the stimulus
    presentation phase. This might mean that with less time there would be a decreased
    superiority effect for longer and more difficult pseudowords.\r\n\r\n\r\n\r\n"
  created: '2013-06-28 20:05:01'
- author:
    name: John Hudson
    picture: 110397
  body: "Peter, with the addition of the ideas in your last couple of paragraphs,
    I think we're pretty much on the same page, although I can't claim to thoroughly
    understand the string kernel concept.\r\n\r\n<blockquote>I agree that in the pseudowords
    a fast phonetic disambiguation comes into play and this is what we explicitly
    hold in working memory.</blockquote>\r\n\r\nBut if you're saying that this only
    comes into play in for the pseudowords, that means that we're first determining
    whether the word is a real word or a pseudoword (or, perhaps better terms, a known
    word vs an unknown word), and only then applying a fast phonetic disambiguation.
    This seems to me unlikely, especially since phonetic disambiguation will likely
    have been part of how we learn to read. My guess is that it is always happening
    while we're reading, but is augmented as we become experienced readers by pattern
    recognition processes that help us resolve known words even faster.\r\n\r\n<blockquote>But
    the Interactive Activation account in its most primitive form says there is feedback
    to the letter level and what we hold in implicit working memory is a sequence
    of strongly activated letter codes, not the explicit memory of the word, it\u2019s
    pronunciation, and a latent knowledge of the spelling.</blockquote>\r\n\r\nThis
    seems to be the nub of things, and for what I don't think I've seen experimental
    data to sway me one way or the other."
  created: '2013-06-28 20:26:36'
- author:
    name: John Hudson
    picture: 110397
  body: Imagine two people being set a series of tasks to complete, each using a different
    method. Whichever finishes a task first, they both move on to the next task. The
    boss doesn't care which one completes the task first, only that the tasks are
    completed. Sometimes, the tasks favour one method over the other, and sometimes
    vice versa, but both people always set out to complete each task, because this
    takes less time than analysing each task first and trying to figure out which
    method will be faster.
  created: '2013-06-28 20:33:26'
- author:
    name: John Hudson
    picture: 110397
  body: It seems to me that the whole word feature pattern retrieval method would
    be favoured for more common and shorter words, while the orthographic phonetic
    disambiguation method would be favoured for less common and longer words. But
    this means that there will be words of middling frequency and familiarity, and
    of middling length, for which it would be difficult to predict which method would
    be fastest and most accurate. Parallel processing of both methods would seem to
    me to be faster than iterative processing of first one and then the other.
  created: '2013-06-28 20:39:53'
- author:
    name: enne_son
    picture: 109487
  body: "John, in a known word the link to a pronunciation is immediate with the matrix-internal
    resonance inside the intrinsic integration interface. At least, that\u2019s the
    hypothesis. Think of this as an intrinsic integration neuron linking directly
    with an unambiguous articulatory action script in another area of the brain. In
    the demo, after a few \u201Ctraining\u201D session, I found myself adopting the
    strategy of going directly to a pronunciation from the initial visual recogniton,
    and holding that in working memory. I found myself doing this for both words and
    pseudo-words, and I saw that I was able to do it successfully with pseudowords.\r\n\r\nFrom
    testing myself on the pseudowords in the revised demo, I suspect that the link
    to a pronunciation is slightly less immediate, especially in complex pseudowords,
    but I have a hunch the link to a pronunciation in pseudowords leverages the same
    intrinsic-integration system because the pseudowords present themselves to me
    perceptually as internally cohesive word-like things. Even with the short exposure
    time used in the demo. But if it is indeed less immediate, the link to an articulatory
    action script would probably be less successful with less exposure time at the
    front end of the test.\r\n\r\nLike you I suspect there is some background process
    of addressing the articulatory system already at an early stage of processing.
    As I intimated before, I think this can take hold as early as the string kernel
    coding stage. It might also be that this works along with the partial but significant
    activation in the intrinsic integration interface to do or a introduce convolution
    to an existing articulatory script at the position or positions where the letter
    that makes the word a psuedoword appears. Or it could be that some interfacilitation
    \ or pooling of activity between competing partly activated units within the intrinsic
    integration interface occurs.\r\n\r\nThat pseudowords present themselves to me
    perceptually at a very short exposure time as ordered, internally cohesive word-like
    things indicates to me that the intrinsic integration interface area <em>is</em>
    activated. Remember that for me the intrinsic-integration interface hypothesis
    accounts for the fact that we see words as unitary, ordered, pronounceable, sense-invested
    object-like things with an internal cohesion typical of true gestalts. The consonant
    strings look cluttered, chaotic and crowded to me. In my opinion an orthographic
    coding hypothesis in which the constituent items are extrinsically and associatively
    linked in a distributed network, but not intrinsically integrated at a neural
    code level can\u2019t account for that. I know introspection and attention to
    the ordinary everyday lived experience of a thing are considered unreliable, but
    they have to be a factor in hypothesis building and model selection in the absence
    of decisive tests. It is well-known that pseudowords produce activity in the visual
    word-form area almost as much as words do. Consonant strings don\u2019t. \r\n\r\nI'm
    not a hugh fan of horse-race models.\r\n\r\n"
  created: '2013-06-28 22:20:12'
- author:
    name: Chris Dean
    picture: 111971
  body: "Peter, when you test yourself, what are your dependent variables, and how
    are you analyzing your data? Is there some way you are removing yourself from
    the <a href=\"https://en.wikipedia.org/wiki/Experimenter's_bias\">experimenter\u2019s
    bias</a> that I am not aware of?"
  created: '2013-06-28 23:29:00'
- author:
    name: enne_son
    picture: 109487
  body: "Chris, I don't pretend I am carrying out a valid test. I found going through
    the demo a valuable exercise and encountered things I didn\u2019t expect, coming
    from an extended period of deep immersion in the literature on the effect. \r\n\r\nIf
    I had a bias, it was toward thinking that perceptual processing was being stopped
    before a conscious identification of the word, and toward assuming that I wouldn\u2018t
    try to develop a strategy or strategies that optimized my chances of making the
    correct post-mask decision, least of all, ones that would involve holding an auditory
    image of the word in my head.\r\n\r\n"
  created: '2013-06-29 02:06:17'
- author:
    name: William Berkson
    picture: 110306
  body: "\"Race horse models.\" I've been pushing this idea, and Peter has always
    been reluctant to embrace it. I still think that a lot of different processes
    go on simultaneously, and when one 'wins' by identifying a word, then the process
    stops, and the eye jumps to another set of words. \r\n\r\nWhen it's a familiar
    word, and the type and physical conditions are readable, I think the whole word
    structural pattern wins. \r\n\r\nI do think that the coding of all the structural
    features of the letters in word happens simultaneously, and once coded they are
    then both separated into whole words and individuated as letters. If the whole
    word pattern matches a remembered code 'resonance' with that memory stored in
    a neuron or many neurons wins the race. If not the individuated letters are used
    for phonemes and orthography. But that is a step beyond, another 'deck' in the
    process. \r\n\r\nIf I'm right any pseudoword superiority effect should take longer
    SOA than the word superiority effect, at least to reach the same level of accuracy.
    I don't know if there are data on this.   \r\n\r\n"
  created: '2013-06-29 04:16:58'
- author:
    name: enne_son
    picture: 109487
  body: "Apparently the timing in the word superiority effects demos I provided above
    are set in the \"transitions\" panel in PowerPoint 2010. The version I have is
    2008. If there is a reader of this post that has 2010, can he or she check the
    settings?\r\n\r\nDownload a version of the ppt file from here:\r\nhttp://www.enneson.com/public_downloads/typophile/demo_wordsuperiorityeffect_pe_varients.ppt\r\nFile
    size is 137 KB"
  created: '2013-06-30 13:26:16'
date: '2013-06-13 12:57:26'
node_type: forum
title: United States navy ditches ALL CAPS message format

---
