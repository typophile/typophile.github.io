---
author:
  name: Tim Ahrens
  picture: 109692
body: When I do Cmd-I on a a font in Apple Font Book, it shows me a list of languages
  supported by the font. How is it generated? I assume there is some internal database?
  Is there any way to extract it?
comments:
- author:
    name: Synthview
    picture: 121472
  body: "Hello,\r\ngood question! I\u2019m interested into an answer too.\r\nsurely
    an internal algorithm.\r\nBut I have the impression the language listed are less
    than the effective ones. Doing it by hand following iso-8859-\xD7, I\u2019ve listed
    much more languages than FontBook list. \r\n"
  created: '2011-01-23 15:58:36'
- author:
    name: clauses
    picture: 112142
  body: Perhaps you could get a similar db from Georg Seifert. Alternatively you could
    compile one from Michael Everson's PDF over European languages. I also remember
    an Excel-file with a lot of languages and their glyphs in a matrix. I think Miguel
    Sousa did it or just posted it in an old Adobe blog; I can't seem to find it now.
  created: '2011-01-23 17:50:53'
- author:
    name: Santiago Orozco
    picture: 117620
  body: it should be an algorithm
  created: '2011-01-23 19:39:09'
- author:
    name: Theunis de Jong
    picture: 114717
  body: "<cite>.. it should be an algorithm</cite>\r\n\r\nIt <em>is</em> an algorithm.
    It could be sort a database of all possible languages, each with a set of glyphs
    this language needs, and all this algorithm needs to do is to check for each language\r\n\r\n\u2200(char)
    \u2208 Language\r\n\r\n-- of course it depends on the database how many different
    \"languages\" (dialects? idolects?) it contains, and how strict the check is (as
    perhaps not all <em>possible</em> characters are <em>required</em> for each language).
    And, of course, \"language\" itself is a fluent concept."
  created: '2011-01-23 22:36:24'
- author:
    name: Igor Freiberger
    picture: 115255
  body: "I thought this kind of information came from the codepages supported, what
    is defined in the font and interpreted by the OS algorithm.\r\n\r\nIf this is
    verified on the fly by MacOS taking into account the actual glyphs of the font,
    its database would be extremely useful. I'm searching for months to get all Latin-script
    languages mapped and it's an almost insane job.\r\n\r\nBut if this is an algorithm,
    how it evalues the support given through combining diacritics? A proper support
    would need the base glyph, the combining diacritic and the ccmp declaration (and
    also a mark definition for most stacking diacritics). Does MacOs verify all this?\r\n\r\nIn
    Windows I believe the OS simply takes language support from font codepages."
  created: '2011-01-24 02:14:27'
- author:
    name: blank
  body: "If you generate a font containing only Latin characters but enable the Arabic
    and Hebrew codepages Fontbook will report that the font supports Arabic and Hebrew.
    It\u2019s definitely not one of Apple\u2019s better applications."
  created: '2011-01-24 03:32:09'
- author:
    name: Santiago Orozco
    picture: 117620
  body: of course, first you should map every glyph for all the languages, then implement
    a tree search with recursion
  created: '2011-01-24 03:51:22'
- author:
    name: Tim Ahrens
    picture: 109692
  body: "Thanks for your thoughts, everyone.\r\n\r\nI was interested in the database
    more than in the lookup implementation (which is surely very simple, so no fuss
    necessary). So, is it not possible to get hold of the database? I looked into
    the package contents of Font Book.app but couldn't find it there. Maybe it is
    stored somewhere else in the system?"
  created: '2011-01-24 13:17:07'
- author:
    name: Khaled Hosny
    picture: 113033
  body: I usually use [[http://www.freedesktop.org/wiki/Software/fontconfig|fontconfig]]'s
    [[http://cgit.freedesktop.org/fontconfig/tree/fc-lang|orthography database]],
    either using their command line utilities (like fc-query) or parsing the database
    on my own. Pretty good coverage, even for languages that can be written in different
    scripts.
  created: '2011-01-24 13:25:50'
- author:
    name: twardoch
    picture: 110427
  body: "Tim, \r\n\r\nmy assumption is that the data comes from the ICU library which
    is bundled with Mac OS X. \r\n\r\nA."
  created: '2011-01-24 14:44:53'
- author:
    name: Igor Freiberger
    picture: 115255
  body: "Tim, as it <em>seems</em> FontBook does not evaluates the actual font contents
    but just the codepage declaration (which is arbitrary and may be wrong), the database
    may be simply a table with languages supported by each codepage. If this is what
    you need, a good startpoint is [[http://en.wikipedia.org/wiki/ISO/IEC_8859|here]].\r\n\r\nIf
    you need more information about languages and alphabets, this [[http://typophile.com/node/74756|thread]]
    has some good tips, including a link to Excel table referred above. There are
    also [[http://www.omniglot.com/writing|Omniglot]] and [[http://www.languagegeek.com/|LanguageGeek]]
    sites. For Latin script, a good general table is published in [[http://en.wikipedia.org/wiki/Latin-derived_alphabet|Wikipedia]]."
  created: '2011-01-24 14:55:05'
- author:
    name: Synthview
    picture: 121472
  body: http://www.eki.ee/letter/
  created: '2011-01-25 22:40:11'
- author:
    name: "Andreas St\xF6tzner"
    picture: 112329
  body: "For <cite>Andron Mega</cite> FontBook reports a number of 57 languages supported.
    My own counting resulted in about 280 languages.\r\n\r\nI based my record on composing
    samples utilizing text strings from the [[http://www.unicode.org/udhr/|UDHR project]],
    which I found very useful to perform the task. (see full listing [[http://www.signographie.de/cms/front_content.php?idart=280|here]].)"
  created: '2011-01-26 20:45:10'
- author:
    name: Si_Daniels
    picture: 110446
  body: 'Has anyone thought to ask Apple? '
  created: '2011-01-27 05:00:05'
- author:
    name: clauses
    picture: 112142
  body: "<cite>Has anyone thought to ask Apple?</cite>\r\n\r\nPointless as they don't
    answer."
  created: '2011-01-27 13:27:16'
- author:
    name: clauses
    picture: 112142
  body: Andreas I would take those lists with a big spoonful of salt. Just checking
    the Danish list [[http://www.unicode.org/udhr/d/udhr_dan.charcount]], and I can
    see Q, X, W, Z missing. A better list for Danish is the [[http://www.evertype.com/alphabets/danish.pdf]]
    from Michael Everson's [[http://www.evertype.com/alphabets/|Alphabets of Europe]],
    but that list is extremely inclusive, hence the characters in brackets are for
    loan-words, translitterations &c.
  created: '2011-01-27 13:35:00'
- author:
    name: Si_Daniels
    picture: 110446
  body: ">Pointless as they don't answer.\n\n:-) Well here's the answer Apple sent
    me...\n\nWe compare the cmap to the ICU exemplar strings for each language.\nWe
    just pick up the latest copy of the open source ICU database for each system release.\n\n\uF0A7
    CLDR Survey Tool http://unicode.org/cldr/apps/survey (jump to other items pop-up/characters)
    \ \n\uF0A7 e.g. http://unicode.org/cldr/apps/survey?_=be&x=characters\n\uF0A7
    Unicode Set definition http://icu.sourceforge.net/userguide/unicodeSet.html\n\n\n"
  created: '2011-01-27 19:58:57'
- author:
    name: "Andreas St\xF6tzner"
    picture: 112329
  body: "Clause, I didn\u2019t mean the character listings you have linked to (they\u2019re
    new to me, and look rather alien). Yes, there is something missing \u2013 not
    useful. The Everson language records (which I know and appreciate, of course)
    are, as you say, over-inclusive \u2013 which I find less helpful as well.\r\nI
    was referring to the bits of actual text (The Declaration of Human Rights) at
    the UDHR project site. This gives me some real-world proof for the scope of my
    fonts, yet still not entirely reliable since not every piece of text contains
    neccessarily all characters belonging to a language."
  created: '2011-01-27 21:32:54'
- author:
    name: Igor Freiberger
    picture: 115255
  body: "Simon, thanks for the information. CLDR is a great tool, although a bit confuse.
    I was not aware about it and surely will take advantage of this tip.\r\n\r\nAnyway,
    use of these data may explain the problems with FontBook.\r\n\r\nFirst: the list
    of languages covered by CLDR is still limited. For example, under Z it lists just
    Zulu, so any support or reference about Z\xE1paro, Zapotec, Zarma, Zazaki, Zhuang
    or Zuni are missed by evaluations based on CLDR.\r\n\r\nSecond: CLDR classifies
    characters as \"approved\" and \"proposed\". If FontBook (or any other tool) evalues
    the language support based only in \"approved\" (the safer way), it may miss some
    essential characters. And if it includes \"proposed\" (aiming a more complete
    analysis) it would return characters actually not needed. For example: it's known
    Latin script for Azerbaijani uses Schwa, but this character is not approved in
    CLDR until now. Thus, evaluation for Azerbaijani support based on CLDR would be
    inconsistent.\r\n\r\nI'm not saying CLDR is a bad tool \u2013 far from that, it's
    really useful. But it is under development and automated evaluations based on
    its data (as FontBook does) need to be understood as partial."
  created: '2011-01-27 22:02:44'
- author:
    name: dberlow
  body: "> I'm not saying CLDR is a bad tool \u2013 far from that, it's really useful...development
    and automated evaluations based on its data (as FontBook does) need to be understood
    as partial.\r\n\r\nSo, if I might ask, is there anything better than fontbook
    at doing this? And how much of the \"partial\" is up to the user regardless? Vs
    the other part of \"partial\" that's lacking time  completeness in language/nation
    mapping? And etc? Thanks."
  created: '2011-01-28 20:33:37'
- author:
    name: Igor Freiberger
    picture: 115255
  body: "<cite>Is there anything better than fontbook at doing this?</cite>\r\n\r\nALAIK,
    not. While I was searching for languages and its alphabets, I did surveys in various
    places \u2013Unicode charts, SIL, Ethnologue, LanguageGeek, Omniglot, Wikipedia,
    eki.ee, Evertype, Signographie, dozens of NGO related to minorities and also linguistic
    departments in several universities. I did ask for a place where these informations
    are compiled, but no one know such a site or book.\r\n\r\nCLDR is a nice addition
    to the list, but still not the one-does-all resource."
  created: '2011-01-28 21:35:11'
- author:
    name: riccard0
    picture: 117627
  body: 'And if someone could be able/willing to create/compile such a resource, which
    characteristics/tools would be needed? '
  created: '2011-01-29 09:28:24'
- author:
    name: JanekZ
    picture: 110966
  body: "Such a .txt could be very useful\r\n[img:sites/default/files/old-images/viet_4802.png]\r\nI
    changed the TAHOMA to my (under construction http://typophile.com/node/73413 )
    font, and\r\n[img:sites/default/files/old-images/VDviet_4480.png]\r\neverything
    is clear, no question marks!\r\n\r\nDisclaimer: These pictures are distributed
    on an AS IS basis, without warranty."
  created: '2011-01-29 11:50:40'
- author:
    name: John Hudson
    picture: 110397
  body: Suddenly, it's all very <a href="http://tiro.com/Articles/sylfaen_article.pdf">1997</a>.
    [PDF]
  created: '2011-01-30 03:21:21'
- author:
    name: Igor Freiberger
    picture: 115255
  body: "<cite>And if someone could be able/willing to create/compile such a resource,
    which characteristics/tools would be needed?</cite>\r\n\r\nRiccardo, I think the
    best is to build a relational database. This could be used in queries through
    the web or in some automatic tool. As any relational db can have its data exported
    in several ways, the information would feed since Python macros to standalone
    applications.\r\n\r\nThis db structure would include a table for languages, a
    table for scripts, a table for glyphs, a table for countries and tables with associations
    between all these. The separation in various tables is needed to handle the n-to-n
    relationships (glyphs\u2013alphabets\u2013languages\u2013scripts\u2013countries).
    Glyph table needs a field to insert the glyph image.\r\n\r\nAbout tools, I think
    any db manager could do this \u2013like phpMyAdmin for MySQL or even MS Access."
  created: '2011-01-30 06:01:30'
- author:
    name: k.l.
    picture: 110875
  body: "Instead of setting up one more of the same, why not help Unicode improve
    CLDR \u2013 <a href=\"http://unicode-inc.blogspot.com/2011/02/unicode-consortium-announces-opening-of.html\"
    target=\"_blank\">now that they are asking</a>?\r\n\r\nMy impression is that what
    is missing is a reliable set of characters per language. And I like the idea of
    having e.g. CLDR as a single source for a variety of locale information. CLDR's
    approach to classify character sets is an advantage and indicates that they are
    honest enough to point out \"not sure\". Applications can chose either approved
    or proposed sets and ideally would tell users about their choice."
  created: '2011-02-08 23:34:03'
- author:
    name: Tim Ahrens
    picture: 109692
  body: "<cite>...reliable set of characters per language</cite>\r\n\r\nThe first
    question is: what are these databases used for, what is the objective?\r\n\r\nIn
    the case of FontBook it is obviously to determine the set of supported languages
    for a given font.\r\n\r\nAs a font maker, it would be the other way round: Which
    characters do I have to provide in order to support a certain language? It all
    boils down to minimizing the number of fail cases where a document requires a
    character that is not in the font. Or, to achieve a certain coverage (say, 99.95%)
    while minimizing the costs. What are the costs? Design effort is a cost, which
    is not the same for each character \u2013 some accented characters can be generated
    with two clicks, whereas other rarely used characters need to be designed from
    scratch. What if the rationale is to minimize the file size, e.g. for web fonts?
    Then the cost is in kB, not work. But we have a similar scenario: some characters
    cost more, others less (accented characters have very low data volume).\r\n\r\nSo,
    we want to cover as many cases as possible with our font. In that sense, I think
    a linguistic approach does not help much. I believe trying to find out which characters
    are \u201Crequired\u201D is the wrong paradigm and 100% coverage is impossible
    to achieve. Also, \u201Ccases\u201D is very difficult to define in the first place.
    Do we distinguish between geographic locations as well? Are we talking about the
    web? Print? The past, present or future? How about foreign words or place names?
    We can find \u201Cco\xF6peration\u201D in English texts, and \u201CCaf\xE9\u201D
    with a French accent is standard in German. On the other hand, in German, the
    \xBBangular\xAB quotes, are very rarely used on websites although they are the
    classic form in novels. An \u201Cofficial\u201D character set is very difficult
    to determine for most languages so in my opinion, a real-world basis (scanning
    large amounts of text) would make more sense than an academic one. Another difficult
    question is, what constitutes a language \u2013 only letters and letter-like symbols,
    also punctuation? Are figures part of a language? For example, In most middle-east
    countries they use \u201Creal\u201D Arabic figures whereas in Northern Africa
    the \u201CLatin Arabic\u201D numerals we use seem to be preferred. How about mathematical
    punctuation, is that still language-specific, how much of it is \u201Crequired\u201D?
    I think this whole thing should not be restricted to letters. What does it help
    to know which letters I need to satisfy the needs of most of my users, but not
    the other characters.\r\n\r\nWhat we really need is the frequency of occurrence
    for each character in each language, found by a real-world survey. Then we can
    set a threshold and determine our set of required characters. Or, we can assign
    a nominal cost to each character, and then work out a tradeoff."
  created: '2011-02-09 09:38:46'
- author:
    name: gaultney
    picture: 112764
  body: "<em>I did ask for a place where these informations are compiled, but no one
    know such a site or book.</em>\r\n\r\nWe're working to change that. [[http://www.scriptsource.org/|ScriptSource]]
    is a site dedicated to gathering info on scripts and writing systems. It supports
    'tabular' data - lists of scripts, languages, characters, etc. - and the associations
    between them as well as text+graphics information. It's built on a database designed
    specifically for this type of linguistic data, and pulls in data from Unicode,
    the Ethnologue, various international standards (15924, 639-3) and CLDR. Text
    descriptions, articles, graphics, bibliographic links and even software can be
    connected to this skeleton of data.\r\n\r\nWe're working closely with the CLDR
    committee, and working on strategies to dramatically expand CLDR data for minority
    languages. CLDR already contains a huge volume of data in a useful but rather
    complex system that supports important concepts such as inheritance, but as a
    result can be tricky to navigate. We hope that ScriptSource can present some of
    that data in an easier to read format, and serve as a place to collect documentation
    and data toward further CLDR submissions. We see CLDR and international standards
    - not ScriptSource - as the long-term repositories of writing system data, but
    use our site to pull them together and enhance them. \r\n\r\nScriptSource is currently
    in testing and being refined for a public release later in the spring. If you're
    interested in finding our more go to [[http://www.scriptsource.org|www.scriptsource.org]].
    If you want to look at what we have so far and give us feedback drop me a line
    and I can give you an invitation, especially if you think that you might want
    to contribute to the site."
  created: '2011-02-09 10:12:26'
- author:
    name: Tim Ahrens
    picture: 109692
  body: Victor, ScriptSource looks very promising! Looking forward to seeing the results
    of you efforts. Will it help me decide on the character set (not only letter-set)
    to put in my fonts?
  created: '2011-02-09 11:03:49'
- author:
    name: gaultney
    picture: 112764
  body: "I hope so. The main 'objects' are script, language, writing system (what
    you get when you connect a script and a language, and a bit like a CLDR 'locale')
    and character. So you will be able to see what characters are used for a particular
    writing system - the main characters as well as auxiliary ones that are used for
    loan words, etc. We've focused more on characters than glyphs, although you'll
    be able to post details of glyph variants, cultural preferences, style differences,
    etc.\r\n\r\nYou'll also be able to see what writing systems use a particular character,
    although the completeness of that answer depends on the completeness of the underlying
    data, and there is still a <em>lot</em> of that data that doesn't exist yet."
  created: '2011-02-09 17:12:37'
date: '2011-01-23 15:37:17'
title: How does Font Book know the supported languages of a font?

---
