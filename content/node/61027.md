---
author:
  name: agisaak
  picture: 115092
body: "Does anyone know of any resources which would provide information on which
  letter *pairs* frequently occur in different languages? It would be especially useful
  if this included information on diacritics.\r\n\r\nI'm currently dealing with some
  consonant-vowel ligatures, and want to figure out if there are diacritical combinations
  which can be safely omitted. I'd tried googling for various diacritical combinations,
  but the useful data ends up buried amid results drawn from a miscellany of legacy
  CJK encodings.\r\n\r\nAndr\xE9"
comments:
- author:
    name: TheOtherNick
    picture: 117624
  body: 'Chthonic, Django Reinhart, Jzanus, Ljubjana, llama...you get the idea: too
    many possibilities... '
  created: '2009-08-18 03:27:00'
- author:
    name: Michel Boyer
    picture: 112585
  body: One source of information I have used for similar purposes is <a href="http://wiki.services.openoffice.org/wiki/Dictionaries">Open
    Office Dictionaries</a> and other dictionaries for spelling checkers. For instance,
    if you click on the link for Canadian English (<a href="http://ftp.services.openoffice.org/pub/OpenOffice.org/contrib/dictionaries/en_CA.zip">zip
    file</a>) you get a folder containing a file with extension .dic with 62341 entries
    (including "derived" entries). Other dictionaries can be much larger. The .dic
    file is plain text. If you remove what follows the slash after each word, you
    get a file on which you can run programs to extract pairs, count them, etc. Of
    course, that gives no information on the frequency with which those pairs occur
    in actual texts but that gives information on possible pairs for the language
    you chose. Some dictionaries are utf-8 encoded, others are latin1 and so on. The
    encoding is given at the first line of a second file with extension .aff. Some
    programming ability is thus required.
  created: '2009-08-18 04:30:49'
- author:
    name: agisaak
    picture: 115092
  body: "Well, yes there will be lots of possibilities, but some pairs are still going
    to be cross-linguistically more common than others, and diacritics which are not
    commonly used may not occur adjacent to others -- for example, I *think* that
    if one had an <em> sa </em> ligature in a font, that it would be more important
    to also implement <em>s\xE1</em> than <em>\u0219\xE4 </em>. But I'm basing that
    on the fact that <em>\xE4</em> doesn't occur in Rumanian and AFAIK that's the
    only language which uses <em>\u0219</em>. Even within a language which contains
    a variety of diacritics, it's not necessarily the case that all of those diacritics
    will occur adjacent to one another, and while it's relatively easy to find information
    on which diacritics are used in which languages, I haven't found information on
    diacritic pairs..\r\n\r\nAndr\xE9"
  created: '2009-08-18 04:31:16'
- author:
    name: agisaak
    picture: 115092
  body: "Thanks Michael -- I'd tried using the Mac OS built-in dictionary for those
    languages I've installed, but it doesn't support wildcards (or if it does, the
    asterisk isn't used for this). Never thought, though, to try opening the actual
    file (a senior moment).\r\n\r\nAndr\xE9"
  created: '2009-08-18 04:36:11'
- author:
    name: Michel Boyer
    picture: 112585
  body: "I use terminal windows and unix utilities to find those files and process
    them. Maybe you can do better with Mac utilities, I don't know.  For dictionaries
    installed by Firefox,  I type the command \"cd $HOME/Library/Ap*ort/Firefox\"
    in a terminal window and then\r\n<code>\r\nfind . -name \"*.dic\"\r\n</code>\r\ngives
    me the list of those dictionaries. They can be copied in some temporary folder
    and batch processed.\r\n\r\nMichel"
  created: '2009-08-18 05:10:56'
- author:
    name: blank
  body: "I always wish there would be some linguistics textbook that covers this stuff.
    Maybe Steve Peters will chime in here with some help.\r\n\r\nIf you have time
    to figure out the syntax to sift through text file wordlists it\u2019s pretty
    easy to put this stuff together using Python or just Bash scripting (grep \"*\xF6\xF6*\"
    file.txt | wc -l). The <a href=\"http://www.openwall.com/wordlists/\">OpenWall
    wordlists disk</a> is worth it\u2019s low price if you don\u2019t need to analyze
    actual text. Ask around in the netsec world and I\u2019m sure even more dictionaries
    exist. Project Gutenberg and similar resources probably have real texts covering
    many of the languages you need to analyze. "
  created: '2009-08-18 05:15:43'
- author:
    name: Michel Boyer
    picture: 112585
  body: I have somewhere a python script that counts bigrams in a utf-8 encoded source.
    To get the list of words, I just use "awk 'BEGIN{FS="/"}{print $1}' *.dic". If
    that can be useful, I'll try to find the script. That's just a few lines of code,
    never more.
  created: '2009-08-18 05:26:14'
- author:
    name: agisaak
    picture: 115092
  body: "     James wrote: <em> I always wish there would be some linguistics textbook
    that covers this stuff.  </em>\r\n\r\nLinguistics texts generally aren't that
    concerned with orthography, so this isn't a likely source. You'll find lots of
    information on the pairings of various <em> sounds </em>, but any statistics presented
    will likely involve IPA rather than orthographic representations.\r\n\r\nAndr\xE9"
  created: '2009-08-18 06:31:58'
- author:
    name: gaultney
    picture: 112764
  body: "Frequency analysis is what you really need - a dictionary would not be enough.
    This would require some long texts in all the languages of interest. I don't know
    of a good general source for these, but someone must have compiled such.\r\n\r\nSome
    years ago Luc(as) de Groot (http://www.lucasfonts.com/) did some good work on
    compiling resources for kerning and building some tools for it. I think he called
    it Kernologica. He should be able to point you in some useful directions.\r\n\r\n"
  created: '2009-08-18 09:08:56'
- author:
    name: Michel Boyer
    picture: 112585
  body: "<cite>Frequency analysis is what you really need.</cite>\r\n\r\nMost obviously.
    To get frequencies (absolute or relative) of bigrams, all you need is a very basic
    script that can be run on some utf-8 encoded input. To get such a script (for
    alphabetic bigrams), you can just copy what is between the cut lines and paste
    it in a terminal window and you will get an executable file named <code>bigrams</code>
    in your current folder.\r\n<code>\r\n----\r\ncat &gt;bigrams &lt;&lt;'EOF'\r\n#!/usr/bin/python\r\n\r\n#
    M. Boyer 2009\r\n\r\nimport codecs, sys\r\ninfile=codecs.open(sys.argv&#91;1&#93;,\"r\",\"utf-8\")\r\ntext=infile.read();
    infile.close()\r\n\r\ntallies={};&nbsp; nbdata=0;&nbsp; prev=' '\r\ndef tallyq(c):\r\n&nbsp;
    return c.isalpha()\r\n\r\nfor char in text:\r\n&nbsp; if (tallyq(prev) and tallyq(char)):\r\n&nbsp;
    &nbsp; datum=prev+char # ; datum=datum.lower()\r\n&nbsp; &nbsp; nbdata=nbdata+1\r\n&nbsp;
    &nbsp; if datum in tallies:\r\n&nbsp; &nbsp; &nbsp; tallies&#91;datum&#93;=tallies&#91;datum&#93;+1\r\n&nbsp;
    &nbsp; else:\r\n&nbsp; &nbsp; &nbsp; tallies&#91;datum&#93;=1\r\n&nbsp; prev=char\r\n\r\nfor
    d in tallies:\r\n&nbsp; print('%s;%d;%.3f%%' %\r\n&nbsp;&nbsp; &nbsp; (d.encode('utf-8'),
    tallies&#91;d&#93;, 100.0*tallies&#91;d&#93;/nbdata))\r\nEOF\r\nchmod 755 bigrams\r\n----\r\n</code>\r\n\r\nThen
    you decide what you want to run it on. For instance, if you want to run it on
    Chekhov's text  \u0414\u0430\u043C\u0430 \u0441 \u0441\u043E\u0431\u0430\u0447\u043A\u043E\u0439
    (The lady with the little dog), you can type (or copy and paste) the line\r\n\r\n<code>&nbsp;&nbsp;\tlynx
    -dump http://lib.ru/LITRA/CHEHOW/d.txt  > dama.txt</code>\r\n\t\r\nand then run
    (maybe after removing some html references at the bottom)\r\n\r\n<code>&nbsp;&nbsp;\t./bigrams
    dama.txt  | sort</code>\r\n\t\r\nHere is a copy paste of part of the output\r\n<code>\r\n\u0442\u043E;372;1.927%\r\n\u0442\u043F;1;0.005%\r\n\u0442\u0440;85;0.440%\r\n\u0442\u0441;31;0.161%\r\n\u0442\u0443;26;0.135%\r\n\u0442\u0444;2;0.010%\r\n\u0442\u0445;1;0.005%\r\n\u0442\u0447;7;0.036%\r\n</code>\r\nThere
    were 372 occurrences of \u0442\u043E which reprensents 1.927% of all bigrams (after
    cleaning the text).\r\n\r\nWith the internet, there are now many sources of texts
    in all languages. There is also nothing to prevent you from running the script
    on a dictionary to know possible combinations; it seems you then don't need the
    frequencies but it may still be interesting to see what were the words containing
    bigrams with very low frequencies. A simple <code>grep</code> answers the question.\r\n\r\nMichel\r\n\r\n[added]
    I guess the mac does not come with lynx installed. I must have installed it myself.
    That example may be more for Linux than mac users. Sorry. "
  created: '2009-08-18 13:20:56'
- author:
    name: ebensorkin
    picture: 109987
  body: "Nice!\r\n"
  created: '2009-08-18 14:29:03'
- author:
    name: kaibernau
  body: "Ohai.\r\n\r\nThe LetterMeter from Peter Bilak and Just van Rossum can run
    a text for single letter and letter pair occurence. Then it is just a matter of
    feeding it with the texts you deem appropriate.\r\n\r\nSays the website:\r\n<cite>LetterMeter
    is a text analysis tool, used in the Type&Media classes (postgraduate course of
    type design) at the Royal Academy of Art in The Hague. LetterMeter is designed
    for comparing multilingual texts and measuring the frequency of particular glyphs.\r\n\r\n<cite>Because
    it is Unicode based, it will work with the majority of languages. The current
    version will recognize Latin, Greek and Cyrillic glyphs, and sort them according
    to their formal attributes. LetterMeter's results include statistics for the incidences
    of round/square/open/diagonal left and right sides of glyphs, ratios of vowels/consonants,
    and counts of glyphs with accents, ascenders and descenders, in any given text(s).\r\n\r\n<cite>LetterMeter
    was developed jointly by Peter Bilak and Just van Rossum, whom I would like to
    thank for the Python programming. Vera Evstafieva helped with the Cyrillic specifications,
    and Panos Haratzopoulos with the Greek.\r\n\r\n<cite>LetterMeter is created using
    Python. and works only on Mac OS X. Although it is available for free, it is copyrighted,
    and you may not redistribute it. All rights reserved, \xA9 2003, Peter Bilak,
    Just van Rossum.</cite>\r\n\r\n<a href=\"http://www.typotheque.com/type_utilities/lettermeter\">For
    TEH DOWNLOADS at Typotheque</a>"
  created: '2009-08-18 19:41:22'
- author:
    name: Michel Boyer
    picture: 112585
  body: "Here is another tool I made using the above code (I replaced semicolons by
    tabs, and added basic choices). It can be used from absolutely any computer (well...
    you tell me if it works on an iPhone). <a href=\"http://www.iro.umontreal.ca/~boyer/typophile/bigrams/\"><strong>Link</strong></a>.\r\n\r\nOn
    a PC, if you save the resulting statistics as a text file, you can then import
    it in Excel for further processing. On the mac, I have found no way to import
    utf-8 text into Excel. Hard to believe!\r\n\r\nMichel"
  created: '2009-08-19 20:59:45'
- author:
    name: qu1j0t3
    picture: 109648
  body: "Here are some results I got for English. http://groups.google.ca/group/comp.lang.postscript/msg/34c2bb049b42f668?hl=en\r\n\r\nI
    used to use a C program to count the most common digrams, then augment it against
    punctuation, to generate kerning pair lists for URW Kernus."
  created: '2009-08-21 01:33:13'
- author:
    name: Michel Boyer
    picture: 112585
  body: "I guess there are indeed good references for English.\r\n\r\nBefore continuing,
    let me say that <a href=\"http://en.wikipedia.org/wiki/Lynx_(web_browser)\"><strong>Lynx</strong></a>
    for Mac OS X can be downloaded from http://www.apple.com/downloads/macosx/unix_open_source/lynxtextwebbrowser.html.
    To use it at the command line, you add <code>/Applications</code> to your path.
    I assume this is done, and that <code>\"Terminal > Window Settings > Display\"</code>
    is set to <code>Unicode (UTF-8)</code>. What follows is then good for Linux and
    Mac users that are used to unix commands.\r\n\r\nNow, some digrams may cause more
    than kerning problems. For instance, in the Typophile thread <a href=\"http://typophile.com/node/40439\"><strong>
    f + umlauts</strong></a>, Florian Hardwig mentions that the diagrams <strong>f\xE4</strong>,
    <strong>f\xF6</strong>, <strong>f\xFC</strong> may cause a clash between the umlauts
    and the f. Those combinations occur in German. How often? Let's check. \r\n\r\nOn
    the <a href=\"http://www.gutenberg.org/catalog/\"><strong>Project Gutenberg Catalog</strong></a>,
    I find Kant's <a href=\"http://www.gutenberg.org/etext/6343\">Kritik der reinen
    Vernunft</a>. On that page I see no html version, and no utf-8 version. I see
    a plain text iso-8859-1 file and if I right click the \"main site\" link and paste
    it I get that the iso-8859-1 text has url\r\n\r\n\thttp://www.gutenberg.org/dirs/etext04/8ikc210.txt\r\n\t\r\nI
    will thus need to tell lynx to expect iso8859-1 text; I will save the result in
    kritik.txt as follows (on the command line):\r\n<code>\r\n\tlynx --dump -assume_charset=ISO8859-1
    http://www.gutenberg.org/dirs/etext04/8ikc210.txt > kritik.txt\r\n</code>\t\r\nThe
    resulting file  kritik.txt now contains the utf8 text (lynx did the reencoding).\r\n\r\nNow
    I look at the digrams in kritik.txt; I do not try to be efficient; the bigrams
    code above is not, and as long as I get my answer in reasonable time, that's fine
    with me. I'll just find all bigrams in the text and then <code>egrep</code> those
    containing f\xE4, f\xF6, f\xFC (I replaced semicolons by tabs in the <code>bigrams</code>
    code)\r\n<code>\r\n  ./bigrams kritik.txt | egrep \"f[\xE4\xF6\xFC]\"\r\n </code>
    \r\nand I get the output\r\n<code>\r\nf\xF6      27      0.003%\r\nf\xFC      697
    \    0.079%\r\nf\xE4      255     0.029%\r\n</code>\r\nwhich means that there
    is a total of 27+697+255 = 979 possible clashes in Kant's text. In my library,
    the book is 847 pages. On the average, that is more than one possible clash per
    page. A few simple an inefficient scripts, unix commands and pipes often give
    answers faster than sophisticated programs. \r\n\r\nMichel\r\n"
  created: '2009-08-21 15:22:19'
date: '2009-08-16 05:23:25'
node_type: forum
title: Resources on letter pair/diacritic pair frequency?

---
