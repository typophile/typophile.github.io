<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Resources on letter pair/diacritic pair frequency? | Typophile Archive</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.91.2" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>

    <meta property="og:title" content="Resources on letter pair/diacritic pair frequency?" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://typophile.github.io/node/61027/" /><meta property="article:section" content="node" />
<meta property="article:published_time" content="2009-08-16T05:23:25+00:00" />
<meta property="article:modified_time" content="2009-08-16T05:23:25+00:00" />

<meta itemprop="name" content="Resources on letter pair/diacritic pair frequency?">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2009-08-16T05:23:25+00:00" />
<meta itemprop="dateModified" content="2009-08-16T05:23:25+00:00" />
<meta itemprop="wordCount" content="0">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Resources on letter pair/diacritic pair frequency?"/>
<meta name="twitter:description" content=""/>

  </head>

  <body class="ma0 avenir bg-near-white">

      <header>
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container">
      <a class="navbar-brand" href="/">Typophile Archive</a>
      <button
        class="navbar-toggler"
        type="button"
        data-bs-toggle="collapse"
        data-bs-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
    </div>
  </nav>
</header>
 
    <main class="pb7" role="main">
      
<article class="container">
  <header class="post-header py-3">
    <h1 class="post-title p-name" itemprop="name headline">Resources on letter pair/diacritic pair frequency?</h1>
    <p class="text-muted">
              
      <time class="f6 mv4 dib tracked" datetime="2009-08-16T05:23:25Z">August 16, 2009</time>
      
        
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">agisaak</span></span>
      

    </p>
  </header>
  <div class="post-content e-content" itemprop="articleBody">Does anyone know of any resources which would provide information on which letter *pairs* frequently occur in different languages? It would be especially useful if this included information on diacritics.

I'm currently dealing with some consonant-vowel ligatures, and want to figure out if there are diacritical combinations which can be safely omitted. I'd tried googling for various diacritical combinations, but the useful data ends up buried amid results drawn from a miscellany of legacy CJK encodings.

André</div>

   <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Chthonic, Django Reinhart, Jzanus, Ljubjana, llama...you get the idea: too many possibilities... </div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 03:27:00"
			itemprop="datePublished"
		>
			2009-08-18 03:27:00
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>TheOtherNick</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">One source of information I have used for similar purposes is <a href="http://wiki.services.openoffice.org/wiki/Dictionaries">Open Office Dictionaries</a> and other dictionaries for spelling checkers. For instance, if you click on the link for Canadian English (<a href="http://ftp.services.openoffice.org/pub/OpenOffice.org/contrib/dictionaries/en_CA.zip">zip file</a>) you get a folder containing a file with extension .dic with 62341 entries (including "derived" entries). Other dictionaries can be much larger. The .dic file is plain text. If you remove what follows the slash after each word, you get a file on which you can run programs to extract pairs, count them, etc. Of course, that gives no information on the frequency with which those pairs occur in actual texts but that gives information on possible pairs for the language you chose. Some dictionaries are utf-8 encoded, others are latin1 and so on. The encoding is given at the first line of a second file with extension .aff. Some programming ability is thus required.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 04:30:49"
			itemprop="datePublished"
		>
			2009-08-18 04:30:49
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Well, yes there will be lots of possibilities, but some pairs are still going to be cross-linguistically more common than others, and diacritics which are not commonly used may not occur adjacent to others -- for example, I *think* that if one had an <em> sa </em> ligature in a font, that it would be more important to also implement <em>sá</em> than <em>șä </em>. But I'm basing that on the fact that <em>ä</em> doesn't occur in Rumanian and AFAIK that's the only language which uses <em>ș</em>. Even within a language which contains a variety of diacritics, it's not necessarily the case that all of those diacritics will occur adjacent to one another, and while it's relatively easy to find information on which diacritics are used in which languages, I haven't found information on diacritic pairs..

André</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 04:31:16"
			itemprop="datePublished"
		>
			2009-08-18 04:31:16
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>agisaak</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Thanks Michael -- I'd tried using the Mac OS built-in dictionary for those languages I've installed, but it doesn't support wildcards (or if it does, the asterisk isn't used for this). Never thought, though, to try opening the actual file (a senior moment).

André</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 04:36:11"
			itemprop="datePublished"
		>
			2009-08-18 04:36:11
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>agisaak</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I use terminal windows and unix utilities to find those files and process them. Maybe you can do better with Mac utilities, I don't know.  For dictionaries installed by Firefox,  I type the command "cd $HOME/Library/Ap*ort/Firefox" in a terminal window and then
<code>
find . -name "*.dic"
</code>
gives me the list of those dictionaries. They can be copied in some temporary folder and batch processed.

Michel</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 05:10:56"
			itemprop="datePublished"
		>
			2009-08-18 05:10:56
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I always wish there would be some linguistics textbook that covers this stuff. Maybe Steve Peters will chime in here with some help.

If you have time to figure out the syntax to sift through text file wordlists it’s pretty easy to put this stuff together using Python or just Bash scripting (grep "*öö*" file.txt | wc -l). The <a href="http://www.openwall.com/wordlists/">OpenWall wordlists disk</a> is worth it’s low price if you don’t need to analyze actual text. Ask around in the netsec world and I’m sure even more dictionaries exist. Project Gutenberg and similar resources probably have real texts covering many of the languages you need to analyze. </div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 05:15:43"
			itemprop="datePublished"
		>
			2009-08-18 05:15:43
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>blank</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I have somewhere a python script that counts bigrams in a utf-8 encoded source. To get the list of words, I just use "awk 'BEGIN{FS="/"}{print $1}' *.dic". If that can be useful, I'll try to find the script. That's just a few lines of code, never more.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 05:26:14"
			itemprop="datePublished"
		>
			2009-08-18 05:26:14
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">     James wrote: <em> I always wish there would be some linguistics textbook that covers this stuff.  </em>

Linguistics texts generally aren't that concerned with orthography, so this isn't a likely source. You'll find lots of information on the pairings of various <em> sounds </em>, but any statistics presented will likely involve IPA rather than orthographic representations.

André</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 06:31:58"
			itemprop="datePublished"
		>
			2009-08-18 06:31:58
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>agisaak</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Frequency analysis is what you really need - a dictionary would not be enough. This would require some long texts in all the languages of interest. I don't know of a good general source for these, but someone must have compiled such.

Some years ago Luc(as) de Groot (http://www.lucasfonts.com/) did some good work on compiling resources for kerning and building some tools for it. I think he called it Kernologica. He should be able to point you in some useful directions.

</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 09:08:56"
			itemprop="datePublished"
		>
			2009-08-18 09:08:56
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>gaultney</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody"><cite>Frequency analysis is what you really need.</cite>

Most obviously. To get frequencies (absolute or relative) of bigrams, all you need is a very basic script that can be run on some utf-8 encoded input. To get such a script (for alphabetic bigrams), you can just copy what is between the cut lines and paste it in a terminal window and you will get an executable file named <code>bigrams</code> in your current folder.
<code>
----
cat &gt;bigrams &lt;&lt;'EOF'
#!/usr/bin/python

# M. Boyer 2009

import codecs, sys
infile=codecs.open(sys.argv&#91;1&#93;,"r","utf-8")
text=infile.read(); infile.close()

tallies={};&nbsp; nbdata=0;&nbsp; prev=' '
def tallyq(c):
&nbsp; return c.isalpha()

for char in text:
&nbsp; if (tallyq(prev) and tallyq(char)):
&nbsp; &nbsp; datum=prev+char # ; datum=datum.lower()
&nbsp; &nbsp; nbdata=nbdata+1
&nbsp; &nbsp; if datum in tallies:
&nbsp; &nbsp; &nbsp; tallies&#91;datum&#93;=tallies&#91;datum&#93;+1
&nbsp; &nbsp; else:
&nbsp; &nbsp; &nbsp; tallies&#91;datum&#93;=1
&nbsp; prev=char

for d in tallies:
&nbsp; print('%s;%d;%.3f%%' %
&nbsp;&nbsp; &nbsp; (d.encode('utf-8'), tallies&#91;d&#93;, 100.0*tallies&#91;d&#93;/nbdata))
EOF
chmod 755 bigrams
----
</code>

Then you decide what you want to run it on. For instance, if you want to run it on Chekhov's text  Дама с собачкой (The lady with the little dog), you can type (or copy and paste) the line

<code>&nbsp;&nbsp;	lynx -dump http://lib.ru/LITRA/CHEHOW/d.txt  > dama.txt</code>
	
and then run (maybe after removing some html references at the bottom)

<code>&nbsp;&nbsp;	./bigrams dama.txt  | sort</code>
	
Here is a copy paste of part of the output
<code>
то;372;1.927%
тп;1;0.005%
тр;85;0.440%
тс;31;0.161%
ту;26;0.135%
тф;2;0.010%
тх;1;0.005%
тч;7;0.036%
</code>
There were 372 occurrences of то which reprensents 1.927% of all bigrams (after cleaning the text).

With the internet, there are now many sources of texts in all languages. There is also nothing to prevent you from running the script on a dictionary to know possible combinations; it seems you then don't need the frequencies but it may still be interesting to see what were the words containing bigrams with very low frequencies. A simple <code>grep</code> answers the question.

Michel

[added] I guess the mac does not come with lynx installed. I must have installed it myself. That example may be more for Linux than mac users. Sorry. </div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 13:20:56"
			itemprop="datePublished"
		>
			2009-08-18 13:20:56
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Nice!
</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 14:29:03"
			itemprop="datePublished"
		>
			2009-08-18 14:29:03
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>ebensorkin</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Ohai.

The LetterMeter from Peter Bilak and Just van Rossum can run a text for single letter and letter pair occurence. Then it is just a matter of feeding it with the texts you deem appropriate.

Says the website:
<cite>LetterMeter is a text analysis tool, used in the Type&Media classes (postgraduate course of type design) at the Royal Academy of Art in The Hague. LetterMeter is designed for comparing multilingual texts and measuring the frequency of particular glyphs.

<cite>Because it is Unicode based, it will work with the majority of languages. The current version will recognize Latin, Greek and Cyrillic glyphs, and sort them according to their formal attributes. LetterMeter's results include statistics for the incidences of round/square/open/diagonal left and right sides of glyphs, ratios of vowels/consonants, and counts of glyphs with accents, ascenders and descenders, in any given text(s).

<cite>LetterMeter was developed jointly by Peter Bilak and Just van Rossum, whom I would like to thank for the Python programming. Vera Evstafieva helped with the Cyrillic specifications, and Panos Haratzopoulos with the Greek.

<cite>LetterMeter is created using Python. and works only on Mac OS X. Although it is available for free, it is copyrighted, and you may not redistribute it. All rights reserved, © 2003, Peter Bilak, Just van Rossum.</cite>

<a href="http://www.typotheque.com/type_utilities/lettermeter">For TEH DOWNLOADS at Typotheque</a></div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-18 19:41:22"
			itemprop="datePublished"
		>
			2009-08-18 19:41:22
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>kaibernau</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Here is another tool I made using the above code (I replaced semicolons by tabs, and added basic choices). It can be used from absolutely any computer (well... you tell me if it works on an iPhone). <a href="http://www.iro.umontreal.ca/~boyer/typophile/bigrams/"><strong>Link</strong></a>.

On a PC, if you save the resulting statistics as a text file, you can then import it in Excel for further processing. On the mac, I have found no way to import utf-8 text into Excel. Hard to believe!

Michel</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-19 20:59:45"
			itemprop="datePublished"
		>
			2009-08-19 20:59:45
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">Here are some results I got for English. http://groups.google.ca/group/comp.lang.postscript/msg/34c2bb049b42f668?hl=en

I used to use a C program to count the most common digrams, then augment it against punctuation, to generate kerning pair lists for URW Kernus.</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-21 01:33:13"
			itemprop="datePublished"
		>
			2009-08-21 01:33:13
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>qu1j0t3</span
			></span
		>
	</div>
	
</div>
  <div class="comment card m-3">
	<div class="card-body">
		<div class="commentbody">I guess there are indeed good references for English.

Before continuing, let me say that <a href="http://en.wikipedia.org/wiki/Lynx_(web_browser)"><strong>Lynx</strong></a> for Mac OS X can be downloaded from http://www.apple.com/downloads/macosx/unix_open_source/lynxtextwebbrowser.html. To use it at the command line, you add <code>/Applications</code> to your path. I assume this is done, and that <code>"Terminal > Window Settings > Display"</code> is set to <code>Unicode (UTF-8)</code>. What follows is then good for Linux and Mac users that are used to unix commands.

Now, some digrams may cause more than kerning problems. For instance, in the Typophile thread <a href="http://typophile.com/node/40439"><strong> f + umlauts</strong></a>, Florian Hardwig mentions that the diagrams <strong>fä</strong>, <strong>fö</strong>, <strong>fü</strong> may cause a clash between the umlauts and the f. Those combinations occur in German. How often? Let's check. 

On the <a href="http://www.gutenberg.org/catalog/"><strong>Project Gutenberg Catalog</strong></a>, I find Kant's <a href="http://www.gutenberg.org/etext/6343">Kritik der reinen Vernunft</a>. On that page I see no html version, and no utf-8 version. I see a plain text iso-8859-1 file and if I right click the "main site" link and paste it I get that the iso-8859-1 text has url

	http://www.gutenberg.org/dirs/etext04/8ikc210.txt
	
I will thus need to tell lynx to expect iso8859-1 text; I will save the result in kritik.txt as follows (on the command line):
<code>
	lynx --dump -assume_charset=ISO8859-1 http://www.gutenberg.org/dirs/etext04/8ikc210.txt > kritik.txt
</code>	
The resulting file  kritik.txt now contains the utf8 text (lynx did the reencoding).

Now I look at the digrams in kritik.txt; I do not try to be efficient; the bigrams code above is not, and as long as I get my answer in reasonable time, that's fine with me. I'll just find all bigrams in the text and then <code>egrep</code> those containing fä, fö, fü (I replaced semicolons by tabs in the <code>bigrams</code> code)
<code>
  ./bigrams kritik.txt | egrep "f[äöü]"
 </code> 
and I get the output
<code>
fö      27      0.003%
fü      697     0.079%
fä      255     0.029%
</code>
which means that there is a total of 27+697+255 = 979 possible clashes in Kant's text. In my library, the book is 847 pages. On the average, that is more than one possible clash per page. A few simple an inefficient scripts, unix commands and pipes often give answers faster than sophisticated programs. 

Michel
</div>
	</div>
	<div class="card-footer text-muted post-meta text-end">
		<time
			class="dt-published"
			datetime="2009-08-21 15:22:19"
			itemprop="datePublished"
		>
			2009-08-21 15:22:19
		</time>
		•
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<span class="p-author h-card" itemprop="name"
				>Michel Boyer</span
			></span
		>
	</div>
	
</div>
 
</article>

    </main>
    
  </body>
</html>
